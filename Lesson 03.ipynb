{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Techniques for Improving the Effectiveness of RAG Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the cell below to load the video presentation that accompanies this notebook, and watch it before working through the materials in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video controls width=\"640\" height=\"360\">\n",
       "    <source src=\"https://d36m44n9vdbmda.cloudfront.net/assets/s-fx-20-v1/lesson-03.mp4\" type=\"video/mp4\">\n",
       "    Your browser does not support the video tag.\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "video_url = \"https://d36m44n9vdbmda.cloudfront.net/assets/s-fx-20-v1/lesson-03.mp4\"\n",
    "\n",
    "video_html = f\"\"\"\n",
    "<video controls width=\"640\" height=\"360\">\n",
    "    <source src=\"{video_url}\" type=\"video/mp4\">\n",
    "    Your browser does not support the video tag.\n",
    "</video>\n",
    "\"\"\"\n",
    "\n",
    "display(HTML(video_html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 03: Evaluating Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to Lesson 03! Now that we have set up our retriever and can get search results through API calls, we could present the results in many ways. We could hook into an existing tool that consumes APIs or build something custom using a framework designed for easy web apps like Streamlit. Even just dumping results into a spreadsheet and visualizing it can help evaluate search results. \n",
    "\n",
    "To validate our system, however, we need to record feedback! While we might start with qualitative feedback from manually observing the results of a few searches, we'll need to start recording the feedback data in order to make quantitative assessments of our RAG system's effectiveness and iterate on the design.\n",
    "\n",
    "Certainly, you'll often be able to tell with just a few starter searches if something is drastically wrong, so getting that kind of qualitative feedback first is valuable. Building effective RAG systems requires rapid iterative prototyping, so it's good to use a small test dataset for these trials--like our relatively tiny dataset of just a few hundred blog articles for this course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This notebook will focus on search, the LLM, and the judge UI and database.**\n",
    "\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "<img src=\"img/03_overview.png\" width=\"850\" alt=\"architecture diagram showing the search, judge UI/database, and LLM highlighted\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restart the Services"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make sure you're staring this lesson with all your services in the correct state, please restart them by running the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bringing containerized services down...\n",
      "Services down.\n",
      "Bringing containerized services back up...\n",
      "Services back up.\n"
     ]
    }
   ],
   "source": [
    "!./restart.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Judge UI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using FastAPI as a backend and Vue.js as a frontend, we've custom built a Judge UI for searching different asset types via semantic or keyword search. This is essentially just a bit of wiring that is making very simple REST calls to the router. Via SQLAlchemy, we've integrated FastAPI with a little backend SQLite database to store the feedback we generate by voting on results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Launching the Judge UI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You already launched the `judge` service in Lesson 00."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36mjudge-1  | \u001b[0m\u001b[38;5;39m2024-08-12 14:37:28,215 |     INFO | database | /judge/db/sql_app.db\u001b[0m\n",
      "\u001b[36mjudge-1  | \u001b[0m/usr/local/lib/python3.8/site-packages/pydantic/_internal/_config.py:321: UserWarning: Valid config keys have changed in V2:\n",
      "\u001b[36mjudge-1  | \u001b[0m* 'orm_mode' has been renamed to 'from_attributes'\n",
      "\u001b[36mjudge-1  | \u001b[0m  warnings.warn(message, UserWarning)\n",
      "\u001b[36mjudge-1  | \u001b[0m\u001b[38;5;39m2024-08-12 14:37:28,247 |     INFO | main | Instantiating FastAPI app\u001b[0m\n",
      "\u001b[36mjudge-1  | \u001b[0m\u001b[38;5;39m2024-08-12 14:37:28,247 |     INFO | main | Formatting uvicorn loggers before app startup\u001b[0m\n",
      "\u001b[36mjudge-1  | \u001b[0m\u001b[38;5;39m2024-08-12 14:37:28,247 |     INFO | main | Enabling CORS\u001b[0m\n",
      "\u001b[36mjudge-1  | \u001b[0m\u001b[38;5;39m2024-08-12 14:37:28,248 |     INFO | main | FastAPI Setup Complete\u001b[0m\n",
      "\u001b[36mjudge-1  | \u001b[0m\u001b[38;5;39m2024-08-12 14:37:28,282 |     INFO | uvicorn.error | Started server process [1]\u001b[0m\n",
      "\u001b[36mjudge-1  | \u001b[0m\u001b[38;5;39m2024-08-12 14:37:28,282 |     INFO | uvicorn.error | Waiting for application startup.\u001b[0m\n",
      "\u001b[36mjudge-1  | \u001b[0m\u001b[38;5;39m2024-08-12 14:37:28,282 |     INFO | main | Re-formatting uvicorn loggers on startup\u001b[0m\n",
      "\u001b[36mjudge-1  | \u001b[0m\u001b[38;5;39m2024-08-12 14:37:28,282 |     INFO | uvicorn.error | Application startup complete.\u001b[0m\n",
      "\u001b[36mjudge-1  | \u001b[0m\u001b[38;5;39m2024-08-12 14:37:28,282 |     INFO | uvicorn.error | Uvicorn running on http://0.0.0.0:5007 (Press CTRL+C to quit)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!docker-compose logs judge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Viewing the Judge UI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `judge` service is available on port 5007. Execute the following cell to generate a link to open it in a new browser tab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "var host = window.location.host;\n",
       "var url = 'http://'+host+':5007';\n",
       "element.innerHTML = '<a style=\"color:green;\" target=\"_blank\" href='+url+'>Click to open the judge UI.</a>';\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%js\n",
    "var host = window.location.host;\n",
    "var url = 'http://'+host+':5007';\n",
    "element.innerHTML = '<a style=\"color:green;\" target=\"_blank\" href='+url+'>Click to open the judge UI.</a>';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how it lets you choose what type of search, switch some of the basic parameters, and select which asset type you are searching."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the Judge UI, compare the results from searching through the summarization chunks vs. the standard chunking. Standard chunks are much more granular and better suited for very specific Q&A tasks. Summarized chunks are better for discovering content you may want to then dive in deeper and read. \n",
    "\n",
    "With standard chunking, the chunks are ordered by score, so sometimes you get multiple sections from the same document; in an app UI, you might organize those search results together so users still see multiple relevant documents in their results.\n",
    "\n",
    "**For the remainder of this notebook we're going to focus on our summarization method of chunking, so we can leave that asset type checked and uncheck the other one in the UI.**\n",
    "\n",
    "We're doing so because the summarization method produces fewer overall chunks, which makes evaluation faster for the purposes of this introductory notebook. However, the same principles would apply for standard chunking."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic and keyword search"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember how some searches are more suited to semantic search and some to keyword search? Imagine the use case where we want to search for a product name, and in particular a relatively new product that may not be in the training data of our embedding model.\n",
    "\n",
    "Search for `H200` using semantic search and note how some documents mismatch on H20.ai, for example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voting with the Judge UI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's vote. First let's evaluate how semantic search does on this query.\n",
    "\n",
    "1. Make sure we're in Semantic Search mode.\n",
    "2. Type `H200` in the search box.\n",
    "3. Make sure only \"TechBlog Posts Summaries\" is checked in Asset Types.\n",
    "4. Vote thumbs up or down on the 10 results. This populates our SQLite database.\n",
    "\n",
    "Now let's evaluate how keyword search does on this query.\n",
    "1. Switch to Keyword Search mode.\n",
    "2. Type `*H200` in the search box. **Include the asterisk so that we get wildcard matches on the closely-related GH200.**\n",
    "3. Make sure only \"TechBlog Posts Summaries\" is checked in Asset Types.\n",
    "4. Vote thumbs up or down on the 10 results. This populates our SQLite database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Votes into Pandas DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SQLite DBs are each a single file, which we can easily query with Python and load into a Pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/workspace/dli'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/workspace/dli/db/sql_app.db'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql_db_filepath = os.path.abspath(os.path.join(os.getcwd(), \"db\", \"sql_app.db\"))\n",
    "sql_db_filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id query                                        chunk_id search_type  \\\n",
      "0   1  H200  doc:techblogs:290276e3c86d4881a05f532d36d0a625    semantic   \n",
      "1   2  H200  doc:techblogs:916d08c7a5284951b0fe279422f7b05c    semantic   \n",
      "2   3  H200  doc:techblogs:7987820dd7094e04b9e59b03ab2884ba    semantic   \n",
      "3   4  H200  doc:techblogs:00b99f27c9e3491ba4104c745c4699c8    semantic   \n",
      "4   5  H200  doc:techblogs:268093563da547568e11e074de4a853c    semantic   \n",
      "5   6  h200  doc:techblogs:916d08c7a5284951b0fe279422f7b05c     keyword   \n",
      "6   7  h200  doc:techblogs:8cd6f1f976f84445814d723225b4aa58     keyword   \n",
      "7   8  h200  doc:techblogs:6612d9d0524b43399ea964c885c468f2     keyword   \n",
      "\n",
      "  keyword_search_field keyword_search_type asset_type   k  results_idx  \\\n",
      "0                 None                None  techblogs  10            0   \n",
      "1                 None                None  techblogs  10            1   \n",
      "2                 None                None  techblogs  10            2   \n",
      "3                 None                None  techblogs  10            3   \n",
      "4                 None                None  techblogs  10            4   \n",
      "5              content               union  techblogs  10            0   \n",
      "6              content               union  techblogs  10            1   \n",
      "7              content               union  techblogs  10            2   \n",
      "\n",
      "   n_results  vote_value     username                     created  \n",
      "0         10           1  dkylemiller  2024-08-12 14:37:28.229523  \n",
      "1         10           1  dkylemiller  2024-08-12 14:37:28.229523  \n",
      "2         10           0  dkylemiller  2024-08-12 14:37:28.229523  \n",
      "3         10           1  dkylemiller  2024-08-12 14:37:28.229523  \n",
      "4         10           1  dkylemiller  2024-08-12 14:37:28.229523  \n",
      "5         10           1  dkylemiller  2024-08-12 14:37:28.229523  \n",
      "6         10           0  dkylemiller  2024-08-12 14:37:28.229523  \n",
      "7         10           1  dkylemiller  2024-08-12 14:37:28.229523  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "\n",
    "def select_all_feedback() -> pd.DataFrame: \n",
    "    # Read sqlite query results into a pandas DataFrame\n",
    "    con = sqlite3.connect(sql_db_filepath)\n",
    "    df = pd.read_sql_query(\"SELECT * FROM feedback\", con)\n",
    "    con.close()\n",
    "    return df\n",
    "\n",
    "df = select_all_feedback()\n",
    "\n",
    "# Verify that result of SQL query is stored in the dataframe\n",
    "print(df.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 13)\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision and Recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in most data science evaluation tasks, we are interested in precision and recall. Remembering the definitions of each:\n",
    "\n",
    "- **Precision:** Total number of *relevant* documents retrieved / Total number of documents retrieved.\n",
    "- **Recall:** Total number of relevant documents *retrieved* / Total number of relevant documents *in the database*.\n",
    "\n",
    "In retrievers, those scores are typically calculated with the system set for some arbitrary number of results K (indicated as Precision@K, Recall@K).\n",
    "\n",
    "You may see variations on these like Mean Average Precision, F1 score (F1@K), or other rank-based metrics like Mean Reciprocal Rank (MRR) or Normalized Cumulative Discounted Gain (NCDG). We'll just focus on Precision and Recall in this course.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "<img src=\"img/precision-recall.png\" width=\"600\" alt=\"Precision Recall\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating Precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We voted on the top 5 results for these two different methods, so we can calculate Precision@5 just by doing a bit of `groupby` magic with pandas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 13)\n",
      "   id query                                        chunk_id search_type  \\\n",
      "0   1  h200  doc:techblogs:290276e3c86d4881a05f532d36d0a625    semantic   \n",
      "1   2  h200  doc:techblogs:916d08c7a5284951b0fe279422f7b05c    semantic   \n",
      "2   3  h200  doc:techblogs:7987820dd7094e04b9e59b03ab2884ba    semantic   \n",
      "3   4  h200  doc:techblogs:00b99f27c9e3491ba4104c745c4699c8    semantic   \n",
      "4   5  h200  doc:techblogs:268093563da547568e11e074de4a853c    semantic   \n",
      "\n",
      "  keyword_search_field keyword_search_type asset_type   k  results_idx  \\\n",
      "0                 None                None  techblogs  10            0   \n",
      "1                 None                None  techblogs  10            1   \n",
      "2                 None                None  techblogs  10            2   \n",
      "3                 None                None  techblogs  10            3   \n",
      "4                 None                None  techblogs  10            4   \n",
      "\n",
      "   n_results  vote_value     username                     created  \n",
      "0         10           1  dkylemiller  2024-08-12 14:37:28.229523  \n",
      "1         10           1  dkylemiller  2024-08-12 14:37:28.229523  \n",
      "2         10           0  dkylemiller  2024-08-12 14:37:28.229523  \n",
      "3         10           1  dkylemiller  2024-08-12 14:37:28.229523  \n",
      "4         10           1  dkylemiller  2024-08-12 14:37:28.229523  \n"
     ]
    }
   ],
   "source": [
    "# first filter to all the feedback we put in manually (a.k.a. human feedback)\n",
    "hf = df[df['username'] != 'llmjudge']\n",
    "\n",
    "# Next transform query column so that it strips wildcards and lowercases everything\n",
    "hf[\"query\"] = hf[\"query\"].str.replace(\"*\", \"\").str.lower()\n",
    "\n",
    "print(hf.shape)\n",
    "print(hf.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  query search_type asset_type  precision\n",
      "0  h200     keyword  techblogs   0.666667\n",
      "1  h200    semantic  techblogs   0.800000\n"
     ]
    }
   ],
   "source": [
    "result = hf.groupby([\"query\", \"search_type\", \"asset_type\"]).aggregate(precision=(\"vote_value\", \"mean\")).reset_index(drop=False)\n",
    "print(result)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So for our single \"H200\" query, keyword search of the summarization chunks had a precision of 1.0, and semantic search of the summarization chunks had a precision of 0.6.\n",
    "\n",
    "We would then repeat this for not just a single query, but multiple queries across our domain of interest, averaging our precision values across those queries. We would then repeat this process with other chunking strategies for consistency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating Recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall is a trickier metric to evaluate than precision because our denominator is the total number of relevant documents in the entire database--and finding relevant documents automatically is the point of the system we are trying to build!\n",
    "\n",
    "If we were to brute force this with human feedback through the UI, it would take a long time, especially as the database grows in size--a good reminder is to do initial evaluation on a subset of the dataset.\n",
    "\n",
    "We can make recall easier to calculate with workarounds. For example, we could try a number of different search types (both semantic and keyword) and chunking strategies with the same query, then assume (or test!) that those multiple strategies found most relevant documents.\n",
    "\n",
    "We've actually done that in our case on a small scale; ideally, we'd want to be more thorough, with additional strategies.\n",
    "\n",
    "First, we need to calculate the total number of unique document ids that are positives:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>totalpos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>h200</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  query  totalpos\n",
       "0  h200         5"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "totalpos = hf[hf[\"vote_value\"] == 1].groupby(\"query\").aggregate(totalpos=(\"chunk_id\", \"nunique\")).reset_index(drop=False)\n",
    "totalpos"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we join this to our `hf` dataframe so we can calculate recall:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf = pd.merge(hf, totalpos, on=[\"query\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>search_type</th>\n",
       "      <th>asset_type</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>totalpos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>h200</td>\n",
       "      <td>keyword</td>\n",
       "      <td>techblogs</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>h200</td>\n",
       "      <td>semantic</td>\n",
       "      <td>techblogs</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  query search_type asset_type  precision  recall  totalpos\n",
       "0  h200     keyword  techblogs   0.666667     0.4         5\n",
       "1  h200    semantic  techblogs   0.800000     0.8         5"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_grouped = hf.groupby([\"query\", \"search_type\", \"asset_type\"]).aggregate(precision=(\"vote_value\", \"mean\"), recall=(\"vote_value\", \"sum\"), totalpos=(\"totalpos\", \"first\")).reset_index(drop=False)\n",
    "hf_grouped[\"recall\"] = hf_grouped[\"recall\"] / hf_grouped[\"totalpos\"]\n",
    "hf_grouped"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And there we have our recall! Or at least a plausible proxy for it, much faster to calculate than going through every document in the database manually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM As A Judge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting human feedback is pretty important for evaluation. However, unless you have access to an automated way of continually collecting user preferences - as search engines track whether or not a user clicked on a search result - it will require a lot of dedicated person-hours to build up your database of feedback, especially as your database of document chunks scales.\n",
    "\n",
    "This leads to a natural follow-up question: can we ask a machine to evaluate whether a document is relevant or not? If so, we could solve the problem by just throwing compute at it.\n",
    "\n",
    "We are actually getting close to another important topic in information retrieval systems: **rerankers**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bi-encoders vs Cross-encoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we've been using a \"bi-encoder\" to embed our documents. It's called a bi-encoder because we are calculating the output score from two embeddings--in our case, the user's query and the document text they are searching through. We are calculating the output score as a similarity between the embedding vectors, like cosine similarity.\n",
    "\n",
    "Typical rerankers use cross-encoders instead of bi-encoders. They are lightweight language models (compared to something like GPT). They take in two pieces of text at once and encode them jointly, spitting out a similarity score directly instead of producing two vectors and then calculating an output score. \n",
    "\n",
    "These models are usually more accurate than bi-encoders+cosine similarity because they can take into account interactions between the pieces of text. However, they don't scale as well as bi-encoders, because cross-encoders need to directly score each query-result pair.\n",
    "\n",
    "If we had a really good cross-encoder model, we could ask that model to grade each query against each document and calculate precision and recall through that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternatives for Reranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, training a good cross-encoder is challenging, in part because the datasets we would most like to use (broad search engine results, which are essentially human evaluations of query-passage matches!) are often proprietary. The best cross-encoders on HuggingFace, for instance often use the [MS MARCO](https://microsoft.github.io/msmarco/) dataset--an excellent search engine dataset with a license that restricts commercial use. \n",
    "\n",
    "In the absence of a good, readily-available cross-encoder, we can turn to a general purpose LLM like GPT, Llama 2, Mistral, etc. These are orders of magnitude larger and slower than a simple cross-encoder, but should be more powerful than the cross-encoder at determining relevance and therefore able to perform evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Tech Blog Summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first need access to all the document summaries that we saved in Lesson 02."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# load the summaries from the json file\n",
    "with open(\"data/techblogs_summaries/saved.json\", \"r\") as f:\n",
    "    saved_summaries = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(saved_summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA AI Workbench, now in beta, aims to streamline how enterprise developers create, share, and scale AI and machine learning projects. It allows developers to create, collaborate, and migrate AI workloads on their GPU-enabled environment of choice. The beta release includes new features such as simplified setup and installation on Windows, Ubuntu, and macOS, expanded support for container runtimes and Git servers, and new base images for project creation. The article walks through a coding copilot reference example using AI Workbench to fine-tune a generative AI model on a GPU system. Key concepts include AI Workbench Projects, fine-tuning methods such as Quantized Low Rank Adaptation (QLoRA), and a walkthrough of a Mistral 7B fine-tuning project. AI Workbench helps simplify the process of developing and deploying AI models by providing an intuitive user experience, streamlined configuration, and automation for handling Git and container-based developer environments.\n"
     ]
    }
   ],
   "source": [
    "summary = saved_summaries['https://developer.nvidia.com/blog/create-share-and-scale-enterprise-ai-workflows-with-nvidia-ai-workbench-now-in-beta/'][0]['text']\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use LLM to Check Summary Relevance"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we send a request to Mixtral to determine if this text is relevant to the query \"H200\". Here we import a `ChatOpenAI` instance of our local NIM Mixtral 8x7B model configured and ready for use with LangChain from an [`llms` helper file](llms.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llms import llms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = llms.nim_mixtral_llm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional Remote LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optionally, instead of using our local model, you can also use either NVIDIA AI Foundation's Mixtral 8x7B model or OpenAI's gpt-3.5-turbo.\n",
    "\n",
    "For either of these 2 options you'll need an API key. For more details about NVIDIA AI Foundation and obtaining a free API key, see [the notebook *NVIDIA AI Foundation.ipynb*](./NVIDIA%20AI%20Foundation.ipynb).\n",
    "\n",
    "After obtaining an appropriate API key, uncomment the appropriate cell below, add your API key, and run the cell to set `llm` to the remote LLM you chose to work with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NVIDIA AI Foundation Mixtral 8x7B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from llms import set_api_key\n",
    "# set_api_key('NVIDIA_API_KEY', '<your_nvidia_api_key>')\n",
    "# llm = llms.nvai_mixtral_llm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OpenAI GPT-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from llms import set_api_key\n",
    "# set_api_key('OPENAI_API_KEY', '<your_openai_api_key>')\n",
    "# llm = llms.openai_gpt3_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "# Initialize a semaphore object with a limit of 3.\n",
    "limit = asyncio.Semaphore(3)\n",
    "\n",
    "async def async_generate(llm, msg):\n",
    "    resp = await llm.agenerate([msg])\n",
    "    return resp.generations[0][0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA AI Workbench, now in beta, aims to streamline how enterprise developers create, share, and scale AI and machine learning projects. It allows developers to create, collaborate, and migrate AI workloads on their GPU-enabled environment of choice. The beta release includes new features such as simplified setup and installation on Windows, Ubuntu, and macOS, expanded support for container runtimes and Git servers, and new base images for project creation. The article walks through a coding copilot reference example using AI Workbench to fine-tune a generative AI model on a GPU system. Key concepts include AI Workbench Projects, fine-tuning methods such as Quantized Low Rank Adaptation (QLoRA), and a walkthrough of a Mistral 7B fine-tuning project. AI Workbench helps simplify the process of developing and deploying AI models by providing an intuitive user experience, streamlined configuration, and automation for handling Git and container-based developer environments.'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful AI bot being used by NVIDIA to determine if a passage of text is relevant to a search query. \"\n",
    "            + \"All of the passages of text are in some way related to NVIDIA, so in order to be relevant it needs to be a strict match between the topic of the passage and the topic of the query. \"\n",
    "            + 'Format your output as a JSON object with a single boolean field \"relevant\". ',\n",
    "        ),\n",
    "        (\n",
    "            \"user\",\n",
    "            'Is the following passage strictly relevant to a search query for \"{query}\"?\\nPassage: {passage}',\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create, Share, and Scale Enterprise AI Workflows with NVIDIA AI Workbench, Now in Beta\n",
      "NVIDIA AI Workbench, now in beta, aims to streamline how enterprise developers create, share, and scale AI and machine learning projects. It allows developers to create, collaborate, and migrate AI workloads on their GPU-enabled environment of choice. The beta release includes new features such as simplified setup and installation on Windows, Ubuntu, and macOS, expanded support for container runtimes and Git servers, and new base images for project creation. The article walks through a coding copilot reference example using AI Workbench to fine-tune a generative AI model on a GPU system. Key concepts include AI Workbench Projects, fine-tuning methods such as Quantized Low Rank Adaptation (QLoRA), and a walkthrough of a Mistral 7B fine-tuning project. AI Workbench helps simplify the process of developing and deploying AI models by providing an intuitive user experience, streamlined configuration, and automation for handling Git and container-based developer environments.\n",
      "-----\n",
      "Modernizing the Data Center with Accelerated Networking\n",
      "Accelerated networking is crucial for modern data centers to optimize networking workloads, especially as AI and other complex workloads grow. By offloading demanding tasks to specialized hardware like CPUs, GPUs, DPUs, or SuperNICs, data centers can enhance performance, scalability, and efficiency. Techniques such as lossless networking, RDMA, adaptive routing, and in-network computing can help organizations unlock the full potential of modern applications, including AI. Implementing SuperNICs and DPUs can offload workloads from the host processor to accelerate communications and cope with the increasing need to move data. Network acceleration also enables high-speed, low-latency data transfers between servers, efficient workload distribution, and faster model training. Network abstraction, network optimization, end-to-end stack optimization, and in-network computing are essential for optimizing network performance and accommodating the unique traffic patterns of modern workloads. To build efficient, high-performance networks with acceleration, organizations can refer to whitepapers and ebooks like NVIDIA Spectrum-X Network Platform Architecture and Networking for the Era of AI.\n",
      "-----\n",
      "Emulating the Attention Mechanism in Transformer Models with a Fully Convolutional Network\n",
      "The article discusses the integration of convolution operations and self-attention mechanisms in computer vision tasks to enhance performance and efficiency. The Convolutional Self-Attention (CSA) module is proposed as a way to model both local and global feature relationships using only convolutions, resulting in improved hardware utilization and reduced deployment latency. Experimental results demonstrate competitive accuracy compared to contemporary transformer networks, with superior efficiency on optimized GPUs. The CSA module is compared against benchmark models like Swin Transformer, ConvNext, and Convolutional Vision Transformer, showing commendable accuracy and the fastest latency in comparisons. The strategic design of CSA allows for efficient computations that balance accuracy and latency well, making it compatible with TensorRT restricted mode for deployment in AV applications. The article concludes by highlighting the advantages of CSA in achieving all-to-all relational encoding with reduced computational load, making it a suitable model design for AV production and other computer vision tasks.\n",
      "-----\n",
      "Announcing NVIDIA Metropolis Microservices for Jetson for Rapid Edge AI Development\n",
      "NVIDIA has announced Metropolis microservices for Jetson, offering a suite of customizable building blocks for developing vision AI applications at the edge. These APIs and microservices aim to streamline the development and deployment process, reducing time from years to months. The platform includes over 15 microservices for video storage, AI perception, analytics, and more. Reference applications, such as AI-enabled network video recorder and zero-shot detection using generative AI, demonstrate the capabilities of these microservices. Developers have the flexibility to choose and integrate various services based on their product maturity. Partners like AAEON, Advantech, and others are incorporating these microservices into their offerings. Overall, NVIDIA Metropolis microservices for Jetson provide a powerful and efficient way to develop edge AI applications.\n",
      "-----\n",
      "Advancing Production AI with NVIDIA AI Enterprise\n",
      "The article discusses the challenges of deploying AI models in production and introduces NVIDIA AI Enterprise as a solution to accelerate the process. It highlights the complexity of the AI software stack and the importance of maintaining security in the face of increasing vulnerabilities. NVIDIA AI Enterprise offers three supported branches for different needs, with regular security updates and API stability. The platform also provides transparency through security advisories and exploitability information. Furthermore, software optimization over time leads to performance gains without hardware upgrades, reducing energy consumption and costs. Enterprise support is included with every subscription, providing organizations with access to NVIDIA experts for assistance and troubleshooting. Overall, NVIDIA AI Enterprise aims to simplify the process of deploying and maintaining production AI models, allowing organizations to focus on leveraging AI for valuable insights. Interested users can request a free 90-day evaluation license to experience the platform.\n",
      "-----\n",
      "One Giant Superchip for LLMs, Recommenders, and GNNs: Introducing NVIDIA GH200 NVL32\n",
      "AWS and NVIDIA have partnered to introduce the NVIDIA GH200 NVL32 superchip in DGX Cloud on AWS, offering a 32-GPU NVLink domain and 19.5 TB of memory. This technology significantly improves performance in tasks like GPT-3 training and LLM inference. The CPU-GPU memory interconnect is 7x faster than PCIe Gen 5, providing more memory for applications. The GH200 NVL32 is a scalable design for hyperscale data centers, supported by NVIDIA software and libraries for thousands of GPU applications. This superchip is ideal for tasks such as LLM training, recommender systems, and GNNs, providing up to 7.9x faster training performance for models with massive embedding tables. The GH200 NVL32 can also increase GNN training performance by up to 5.8x compared to previous models. This technology is a game-changer for cloud computing, offering improved performance and efficiency for a wide range of AI and computing applications.\n",
      "-----\n",
      "NVIDIA TensorRT-LLM Enhancements Deliver Massive Large Language Model Speedups on NVIDIA H200\n",
      "NVIDIA has made significant enhancements to its TensorRT-LLM software, resulting in massive speedups for large language models (LLMs) on the NVIDIA H200 GPU. These improvements include optimizations for both compute throughput and memory usage, allowing for significant improvements in LLM inference performance. The latest enhancements have led to a 6.7x speedup on the Llama 2 70B model and the ability to run the Falcon-180B model on a single GPU. These advancements are achieved through techniques like Grouped Query Attention (GQA) and custom INT4 AWQ, which reduce memory usage while maintaining high accuracy. The improvements in TensorRT-LLM software have resulted in a 2.4x increase in performance compared to previous versions. These enhancements will be included in upcoming releases of TensorRT-LLM and are aimed at optimizing GPU compute resources and reducing operational costs for deploying large language models.\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "batch_messages = []\n",
    "\n",
    "# truncating the list to limit to a subset of urls \n",
    "# this example is just to illustrate the point\n",
    "# the first 5 urls should be irrelevant to H200, the next 2 should be relevant \n",
    "urls = list(saved_summaries.keys())[0:5] + ['https://developer.nvidia.com/blog/one-giant-superchip-for-llms-recommenders-and-gnns-introducing-nvidia-gh200-nvl32/', 'https://developer.nvidia.com/blog/nvidia-tensorrt-llm-enhancements-deliver-massive-large-language-model-speedups-on-nvidia-h200/']\n",
    "\n",
    "for url in urls:\n",
    "    title = saved_summaries[url][0]['document_title']\n",
    "    summary = saved_summaries[url][0]['text']\n",
    "    print(title)\n",
    "    print(summary)\n",
    "    print(\"-----\")\n",
    "    passage = title + \"\\n\" + summary\n",
    "    messages = template.format_messages(query=\"H200\", passage=passage)\n",
    "    batch_messages.append(messages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{\n",
      "\"relevant\": false\n",
      "}\n",
      "\n",
      "The passage is not strictly relevant to a search query for \"H200\" as it does not mention or imply any connection to an NVIDIA H200 product.\n",
      "\n",
      "{\n",
      "\"relevant\": false\n",
      "}\n",
      "\n",
      "The passage discusses network acceleration in data centers, including the use of specialized hardware like CPUs, GPUs, DPUs, and SuperNICs. However, it does not specifically mention the NVIDIA H200 product. Therefore, it is not strictly relevant to a search query for \"H200\".\n",
      "\n",
      "{\n",
      "  \"relevant\": false\n",
      "}\n",
      "\n",
      "The passage discusses a Convolutional Self-Attention (CSA) module for computer vision tasks, comparing it to models like Swin Transformer, ConvNext, and Convolutional Vision Transformer. However, it does not mention or discuss the NVIDIA H200 product, making it not strictly relevant to a search query for \"H200\".\n",
      "\n",
      "{\n",
      "\"relevant\": false\n",
      "}\n",
      "\n",
      "The passage is not strictly relevant to a search query for \"H200\" as it discusses NVIDIA Metropolis microservices for Jetson and does not mention or imply any connection to the H200 product.\n",
      "\n",
      "{\n",
      "  \"relevant\": false\n",
      "}\n",
      "\n",
      "The passage discusses NVIDIA AI Enterprise, which is a product related to AI and machine learning. However, it does not mention or imply any connection to the NVIDIA H200 product, which is a graphics card. Therefore, this passage is not strictly relevant to a search query for \"H200\".\n",
      "\n",
      "{\n",
      "\"relevant\": true\n",
      "}\n",
      "\n",
      "The passage is strictly relevant to a search query for \"H200\" because it mentions the NVIDIA GH200 NVL32 superchip, which is a specific product or technology offered by NVIDIA. This indicates that the passage is discussing NVIDIA-related topics, and is therefore relevant to a search query related to NVIDIA products.\n",
      "\n",
      "{\n",
      "  \"relevant\": true\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "response = llm.generate(batch_messages)\n",
    "for gen in response.generations:\n",
    "    print(gen[0].text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These results can now be saved in a data structure and used as a proxy for ground truth when evaluating the retrieval results and calculating precision and recall.\n",
    "\n",
    "The code above can be easily modified to perform the full evaluation with an LLM as a judge. In that case, you may want to evaluate which LLM can most cost-effectively perform that task. This kind of high-intensity, sustained generation could be well-suited for a smaller model run in a batch job--and the smaller the model, the more easily you could also tune it to more closely mimic human evaluations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recap\n",
    "\n",
    "The final product of this course and accompanying notebook for Lesson 04 uses only semantic search, but hopefully you can see the potential effectiveness of combining the two methods together. \n",
    "\n",
    "How could you create a hybrid search that merged/re-ranked the results of both search types? We'll leave that for you to research and work on yourself after this course is over."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Lesson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Move to the next lesson by double-clicking *Lesson 04.ipynb* on the file-viewer on the left-hand side of your Jupyter Lab environment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
