{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8482fe96-7253-4156-82a1-9e7887232b76",
   "metadata": {},
   "source": [
    "# Techniques for Improving the Effectiveness of RAG Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786cae1e-ec39-4365-8f57-b96b4e9da913",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b32a5ad4-978b-45a6-87ad-ad66a9269084",
   "metadata": {},
   "source": [
    "> If you haven't already, please visit the [main course page](https://apps.learn.learn.nvidia.com/learning/course/course-v1:DLI+S-FX-20+V1/block-v1:DLI+S-FX-20+V1+type@sequential+block@43eee6e2d779407286f142ccb8483fe0/block-v1:DLI+S-FX-20+V1+type@vertical+block@e2b8cfd88f2a45c89ef908f7929c266c) and watch the introduction presentation video."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bedf827-b5fc-4f9e-8de4-f57e275edfd3",
   "metadata": {},
   "source": [
    "## Lesson 00: Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce89053d-4857-4ccb-9da9-468bf99915a7",
   "metadata": {},
   "source": [
    "Welcome to *Techniques for Improving the Effectiveness of RAG Systems*.\n",
    "\n",
    "In this workshop, you will learn techniques that can take your RAG system from an interesting proof-of-concept to a serious asset. \n",
    "\n",
    "We'll cover the design of hybrid retrievers, the use of multiple smaller fine-tuned expert models instead of a single large general-purpose model, and methods to evaluate RAG performance with each iterative design change, using both human-as-a-judge and LLM-as-a-judge evaluation frameworks. \n",
    "\n",
    "With the lessons learned in this workshop, youâ€™ll be able to build applications that deliver on the expectations of what serious LLM-based RAG applications can do."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2214509-f115-4878-99e0-91cd09a3696d",
   "metadata": {},
   "source": [
    "## Workshop Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbff8cba-5fa1-4edd-9cd4-3d4fc25a02c1",
   "metadata": {},
   "source": [
    "In addition to this introduction, the workshop consists of four lesson notebooks, which you will be working through in order.\n",
    "\n",
    "- **Lesson 1: Exploring and Preparing your Dataset for Retrieval.** We will prepare the data we will use in the rest of the app, using strategies for splitting data into chunks for easy retrieval and using an LLM on ingest to facilitate other use cases. We will use the Router, Chunker, and LLM.\n",
    "\n",
    "- **Lesson 2: Loading the Vector/Document Database.** We will create indexes with which to search our data--particularly vector indexes that rely on representations of the text as vectors (embeddings). We will use the Router, Embedder, and Hybrid Search.\n",
    "\n",
    "- **Lesson 3: Evaluating Retrieval.** We will implement an interface that allows us to collect data on the performance of our app--a notoriously difficult challenge with the wide scope of many language use cases. We will use the Router, Hybrid Search, LLM, Judge UI, and Human Eval Database.\n",
    "\n",
    "- **Lesson 4: Better Generations.** We will combine the previous elements into the final web app, including an initial triage step to assess the user's intent and intelligently select the right settings for the search and LLM prompt. We will use the App, the Hybrid Search, and the LLM.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0a8874-35d1-44af-9ec1-1b98b296c90c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0118d9d7-84f2-4091-9b36-344b2b3e8e43",
   "metadata": {},
   "source": [
    "## RAG Application"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1cc2e62-15e3-4bde-82cb-49d8c2e9d9eb",
   "metadata": {},
   "source": [
    "You will be working with a RAG application developed largely for internal use at NVIDIA."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8ee986",
   "metadata": {},
   "source": [
    "**The final RAG web app that we are going to build together...**\n",
    "\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "<img src=\"./img/Librarian.png\" width=\"850\" alt=\"Librarian Final Web RAG app\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554b3e49",
   "metadata": {},
   "source": [
    "**... relying on the below architecture and its components:**\n",
    "\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "<img src=\"img/00_overview.png\" width=\"850\" alt=\"Overall Architecture\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2959c1-4056-4da1-9686-5f1515b31a5b",
   "metadata": {},
   "source": [
    "Here is a rapid overview of the different components and services that will be seen and be part of this course:\n",
    "1. the Router: coordinates data movement between services and components\n",
    "2. the Chunker: splits long texts into more manageable pieces\n",
    "3. the Embedder: converts text to numbers that encode the meaning of the text\n",
    "4. the Hybrid Search: enables retrieval of the chunked and embedded text in addition to typical keyword search\n",
    "5. the LLM (large language model) Service: synthesizes retrieved text into something useful\n",
    "6. the Judge: allows data collection to evaluate system performance\n",
    "7. the Human Eval Database: stores results from the Judge UI\n",
    "8. the App: makes the whole system easy to use\n",
    "\n",
    "Note that each notebook will focus on a subset of the components, until the end when they are brought together into the full app."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d015275-dfa1-480b-985a-b3a8e2e631fc",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8493db-7adf-4b52-a163-be2657e85f9e",
   "metadata": {},
   "source": [
    "## Application Microservices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f728d4fd-1f2c-4f43-ae92-3d167e08a1d8",
   "metadata": {},
   "source": [
    "## Modular RAG System Components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "023c2ec2-2762-4f52-9949-9c378568c42e",
   "metadata": {},
   "source": [
    "We're going to be building a modular RAG system in this course--the foundation for a robust, scalable app. Each RAG component will be running in its own container. You will be launching these containers below for your present work, but we are also providing you with all the assets and source code needed to launch them yourself at another time.\n",
    "\n",
    "To take a look at the source code, navigate in the left-hand panel to the various directories, each of which represents a different component such as `chunking` or `router`. The source code for all these containers is yours to use as you see fit! It's ultimately intended as a starting point or inspiration for any application that you might be looking to build."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80da2d36-b39f-4d43-af3f-cb8fcf5dea61",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b333e5-3188-42ed-8429-c2a04091e836",
   "metadata": {},
   "source": [
    "## Launch the Application Components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55165e6-baf7-4b5d-a19f-4dddbc3c0113",
   "metadata": {},
   "source": [
    "As a first order of business, we are going to launch all the services that you will be working with throughout the workshop. To do this we are going to use `docker-compose`.\n",
    "\n",
    "The command `docker-compose up -d` will look at `docker-compose.yml` and run container-based services based on the configuration there. For the sake of time we've prebuilt the Docker images for you, but again, you can build these container images yourself at a later time using the provided source code in the component directories.\n",
    "\n",
    "Launching the 6 component services will take about a minute. The containers actually spin up faster than that but some wait for others to meet specific initialization criteria due to interdependencies.\n",
    "\n",
    "To run the `docker-compose up -d` command, copy/paste the following command into the open terminal tab and run it. You'll find the Jupyter Lab terminal tab already open for you up near the top of the Jupyter Lab environment, next to the currently focused-on \"Lesson 00.ipynb\" tab. We *could* run this command here in the notebook, but its output is spurious and can cause the notebook to stall."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575c476e-bbb8-4cdd-8ac8-e6ad2e8ada90",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "```\n",
    "docker-compose up -d\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e2418c-b6fd-4ca8-8ec2-f7a6ef8af61c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f044ed-e73c-4873-91c0-2229fb655441",
   "metadata": {},
   "source": [
    "## View the Services"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3553fc42-cd8a-4795-80c2-189101410070",
   "metadata": {},
   "source": [
    "We can view all the container services launched by `docker-compose` with the `docker-compose ps` command, which we run here with some additional formatting to make the output easier to read.\n",
    "\n",
    "Note: we are now running these bash command line commands in this notebook which has a `bash` kernel (code execution backend). We didn't do this above because the `docker-compose up -d` command streams output which can get quite long and cause the Jupyter notebook to hang."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4973144-7c63-45e0-8619-5ff8a1bcd3ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SERVICE    STATE     PORTS\n",
      "chunking   running   0.0.0.0:5005->5005/tcp, :::5005->5005/tcp\n",
      "judge      running   0.0.0.0:5007->5007/tcp, :::5007->5007/tcp\n",
      "mixtral    running   0.0.0.0:9998-9999->9998-9999/tcp, :::9998-9999->9998-9999/tcp\n",
      "redis      running   6379/tcp, 8001/tcp\n",
      "router     running   0.0.0.0:5006->5006/tcp, :::5006->5006/tcp\n",
      "triton     running   0.0.0.0:8000-8002->8000-8002/tcp, :::8000-8002->8000-8002/tcp\n",
      "web        running   0.0.0.0:5000->5000/tcp, :::5000->5000/tcp\n"
     ]
    }
   ],
   "source": [
    "docker-compose ps --format \"table {{.Service}}\\t{{.State}}\\t{{.Ports}}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf31269-52e4-4594-bda0-2abc5f771bbc",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892e5ce0-2f6d-41be-86df-6cba5863ed58",
   "metadata": {},
   "source": [
    "## Viewing Service Logs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992bd3d5-596a-4d6b-8a77-10666ec9fdd0",
   "metadata": {},
   "source": [
    "We can use `docker-compose logs <service_name>` to view the logs for any of the running services. We can obtain the names of the services from the output above, or by inspecting `docker-compose.yml`. Here we look at a few the logs for a few of the services as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fdb80b6d-a60b-43be-a6c9-072210c1af41",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36mchunking-1  | \u001b[0mINFO:     Started server process [7]\n",
      "\u001b[36mchunking-1  | \u001b[0mINFO:     Waiting for application startup.\n",
      "\u001b[36mchunking-1  | \u001b[0mINFO:     Application startup complete.\n",
      "\u001b[36mchunking-1  | \u001b[0mINFO:     Uvicorn running on http://0.0.0.0:5005 (Press CTRL+C to quit)\n",
      "\u001b[36mchunking-1  | \u001b[0mINFO:     108.31.235.14:49205 - \"GET / HTTP/1.1\" 307 Temporary Redirect\n",
      "\u001b[36mchunking-1  | \u001b[0mINFO:     108.31.235.14:49205 - \"GET /docs HTTP/1.1\" 200 OK\n",
      "\u001b[36mchunking-1  | \u001b[0mINFO:     108.31.235.14:49205 - \"GET /openapi.json HTTP/1.1\" 200 OK\n",
      "\u001b[36mchunking-1  | \u001b[0mINFO:     172.19.0.2:38518 - \"POST /api/chunking HTTP/1.1\" 200 OK\n",
      "\u001b[36mchunking-1  | \u001b[0mINFO:     172.19.0.2:35340 - \"POST /api/chunking HTTP/1.1\" 200 OK\n",
      "\u001b[36mchunking-1  | \u001b[0mINFO:     172.19.0.2:42638 - \"POST /api/chunking HTTP/1.1\" 200 OK\n",
      "\u001b[36mchunking-1  | \u001b[0mINFO:     172.19.0.2:52374 - \"POST /api/chunking HTTP/1.1\" 200 OK\n",
      "\u001b[36mchunking-1  | \u001b[0mINFO:     172.19.0.2:46368 - \"POST /api/chunking HTTP/1.1\" 200 OK\n",
      "\u001b[36mchunking-1  | \u001b[0mINFO:     172.19.0.2:40810 - \"POST /api/chunking HTTP/1.1\" 200 OK\n"
     ]
    }
   ],
   "source": [
    "docker-compose logs chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aece2325-3286-47c6-98c6-585d26acfef5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36mtriton-1  | \u001b[0m\n",
      "\u001b[36mtriton-1  | \u001b[0m=============================\n",
      "\u001b[36mtriton-1  | \u001b[0m== Triton Inference Server ==\n",
      "\u001b[36mtriton-1  | \u001b[0m=============================\n",
      "\u001b[36mtriton-1  | \u001b[0m\n",
      "\u001b[36mtriton-1  | \u001b[0mNVIDIA Release 22.01 (build 31237563)\n",
      "\u001b[36mtriton-1  | \u001b[0m\n",
      "\u001b[36mtriton-1  | \u001b[0mCopyright (c) 2018-2021, NVIDIA CORPORATION & AFFILIATES.  All rights reserved.\n",
      "\u001b[36mtriton-1  | \u001b[0m\n",
      "\u001b[36mtriton-1  | \u001b[0mVarious files include modifications (c) NVIDIA CORPORATION.  All rights reserved.\n",
      "\u001b[36mtriton-1  | \u001b[0m\n",
      "\u001b[36mtriton-1  | \u001b[0mThis container image and its contents are governed by the NVIDIA Deep Learning Container License.\n",
      "\u001b[36mtriton-1  | \u001b[0mBy pulling and using the container, you accept the terms and conditions of this license:\n",
      "\u001b[36mtriton-1  | \u001b[0mhttps://developer.nvidia.com/ngc/nvidia-deep-learning-container-license\n",
      "\u001b[36mtriton-1  | \u001b[0m\n",
      "\u001b[36mtriton-1  | \u001b[0mI0809 18:56:38.813183 7 metrics.cc:298] Collecting metrics for GPU 0: NVIDIA A100 80GB PCIe\n",
      "\u001b[36mtriton-1  | \u001b[0mI0809 18:56:39.060123 7 libtorch.cc:1227] TRITONBACKEND_Initialize: pytorch\n",
      "\u001b[36mtriton-1  | \u001b[0mI0809 18:56:39.060151 7 libtorch.cc:1237] Triton TRITONBACKEND API version: 1.7\n",
      "\u001b[36mtriton-1  | \u001b[0mI0809 18:56:39.060155 7 libtorch.cc:1243] 'pytorch' TRITONBACKEND API version: 1.7\n",
      "\u001b[36mtriton-1  | \u001b[0m2024-08-09 18:56:39.183804: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "\u001b[36mtriton-1  | \u001b[0m2024-08-09 18:56:39.210363: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "\u001b[36mtriton-1  | \u001b[0mI0809 18:56:39.210427 7 tensorflow.cc:2176] TRITONBACKEND_Initialize: tensorflow\n",
      "\u001b[36mtriton-1  | \u001b[0mI0809 18:56:39.210447 7 tensorflow.cc:2186] Triton TRITONBACKEND API version: 1.7\n",
      "\u001b[36mtriton-1  | \u001b[0mI0809 18:56:39.210452 7 tensorflow.cc:2192] 'tensorflow' TRITONBACKEND API version: 1.7\n",
      "\u001b[36mtriton-1  | \u001b[0mI0809 18:56:39.210455 7 tensorflow.cc:2216] backend configuration:\n",
      "\u001b[36mtriton-1  | \u001b[0m{}\n",
      "\u001b[36mtriton-1  | \u001b[0mI0809 18:56:39.229269 7 onnxruntime.cc:2232] TRITONBACKEND_Initialize: onnxruntime\n",
      "\u001b[36mtriton-1  | \u001b[0mI0809 18:56:39.229287 7 onnxruntime.cc:2242] Triton TRITONBACKEND API version: 1.7\n",
      "\u001b[36mtriton-1  | \u001b[0mI0809 18:56:39.229291 7 onnxruntime.cc:2248] 'onnxruntime' TRITONBACKEND API version: 1.7\n",
      "\u001b[36mtriton-1  | \u001b[0mI0809 18:56:39.229293 7 onnxruntime.cc:2278] backend configuration:\n",
      "\u001b[36mtriton-1  | \u001b[0m{}\n",
      "\u001b[36mtriton-1  | \u001b[0mI0809 18:56:39.244897 7 openvino.cc:1234] TRITONBACKEND_Initialize: openvino\n",
      "\u001b[36mtriton-1  | \u001b[0mI0809 18:56:39.244914 7 openvino.cc:1244] Triton TRITONBACKEND API version: 1.7\n",
      "\u001b[36mtriton-1  | \u001b[0mI0809 18:56:39.244918 7 openvino.cc:1250] 'openvino' TRITONBACKEND API version: 1.7\n",
      "\u001b[36mtriton-1  | \u001b[0mI0809 18:56:39.836720 7 pinned_memory_manager.cc:240] Pinned memory pool is created at '0x7fa42c000000' with size 268435456\n",
      "\u001b[36mtriton-1  | \u001b[0mI0809 18:56:39.839003 7 cuda_memory_manager.cc:105] CUDA memory pool is created on device 0 with size 67108864\n",
      "\u001b[36mtriton-1  | \u001b[0mI0809 18:56:39.846660 7 model_repository_manager.cc:994] loading: transformer_tensorrt_tokenize:1\n",
      "\u001b[36mtriton-1  | \u001b[0mI0809 18:56:39.946976 7 model_repository_manager.cc:994] loading: transformer_tensorrt_model:1\n",
      "\u001b[36mtriton-1  | \u001b[0mI0809 18:56:39.956843 7 python.cc:1897] TRITONBACKEND_ModelInstanceInitialize: transformer_tensorrt_tokenize_0 (GPU device 0)\n",
      "\u001b[36mtriton-1  | \u001b[0mNone of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n",
      "\u001b[36mtriton-1  | \u001b[0mI0809 18:56:43.793978 7 model_repository_manager.cc:1149] successfully loaded 'transformer_tensorrt_tokenize' version 1\n",
      "\u001b[36mtriton-1  | \u001b[0mI0809 18:56:43.794351 7 tensorrt.cc:5145] TRITONBACKEND_Initialize: tensorrt\n",
      "\u001b[36mtriton-1  | \u001b[0mI0809 18:56:43.794375 7 tensorrt.cc:5155] Triton TRITONBACKEND API version: 1.7\n",
      "\u001b[36mtriton-1  | \u001b[0mI0809 18:56:43.794382 7 tensorrt.cc:5161] 'tensorrt' TRITONBACKEND API version: 1.7\n",
      "\u001b[36mtriton-1  | \u001b[0mI0809 18:56:43.794463 7 tensorrt.cc:5204] backend configuration:\n",
      "\u001b[36mtriton-1  | \u001b[0m{}\n",
      "\u001b[36mtriton-1  | \u001b[0mI0809 18:56:43.794493 7 tensorrt.cc:5256] TRITONBACKEND_ModelInitialize: transformer_tensorrt_model (version 1)\n",
      "\u001b[36mtriton-1  | \u001b[0mI0809 18:56:43.802906 7 tensorrt.cc:5305] TRITONBACKEND_ModelInstanceInitialize: transformer_tensorrt_model_0 (GPU device 0)\n",
      "\u001b[36mtriton-1  | \u001b[0mI0809 18:56:44.138965 7 logging.cc:49] [MemUsageChange] Init CUDA: CPU +714, GPU +0, now: CPU 3379, GPU 1435 (MiB)\n",
      "\u001b[36mtriton-1  | \u001b[0mI0809 18:56:44.879891 7 logging.cc:49] Loaded engine size: 1278 MiB\n",
      "\u001b[36mtriton-1  | \u001b[0mI0809 18:56:45.536626 7 logging.cc:49] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +1242, GPU +320, now: CPU 7818, GPU 2393 (MiB)\n",
      "\u001b[36mtriton-1  | \u001b[0mI0809 18:56:45.537013 7 logging.cc:49] [MemUsageChange] TensorRT-managed allocation in engine deserialization: CPU +0, GPU +637, now: CPU 0, GPU 637 (MiB)\n",
      "\u001b[36mtriton-1  | \u001b[0mI0809 18:56:45.544529 7 logging.cc:49] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +10, now: CPU 5262, GPU 2393 (MiB)\n",
      "\u001b[36mtriton-1  | \u001b[0mI0809 18:56:46.540611 7 logging.cc:49] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +464, now: CPU 0, GPU 1101 (MiB)\n",
      "\u001b[36mtriton-1  | \u001b[0mI0809 18:56:46.540831 7 tensorrt.cc:1409] Created instance transformer_tensorrt_model_0 on GPU 0 with stream priority 0 and optimization profile default[0];\n",
      "\u001b[36mtriton-1  | \u001b[0mI0809 18:56:46.541228 7 model_repository_manager.cc:1149] successfully loaded 'transformer_tensorrt_model' version 1\n",
      "\u001b[36mtriton-1  | \u001b[0mI0809 18:56:46.541514 7 model_repository_manager.cc:994] loading: transformer_tensorrt_inference:1\n",
      "\u001b[36mtriton-1  | \u001b[0mI0809 18:56:46.641867 7 model_repository_manager.cc:1149] successfully loaded 'transformer_tensorrt_inference' version 1\n",
      "\u001b[36mtriton-1  | \u001b[0mI0809 18:56:46.641981 7 server.cc:519] \n",
      "\u001b[36mtriton-1  | \u001b[0m+------------------+------+\n",
      "\u001b[36mtriton-1  | \u001b[0m| Repository Agent | Path |\n",
      "\u001b[36mtriton-1  | \u001b[0m+------------------+------+\n",
      "\u001b[36mtriton-1  | \u001b[0m+------------------+------+\n",
      "\u001b[36mtriton-1  | \u001b[0m\n",
      "\u001b[36mtriton-1  | \u001b[0mI0809 18:56:46.642055 7 server.cc:546] \n",
      "\u001b[36mtriton-1  | \u001b[0m+-------------+-------------------------------------------------------------------------+--------+\n",
      "\u001b[36mtriton-1  | \u001b[0m| Backend     | Path                                                                    | Config |\n",
      "\u001b[36mtriton-1  | \u001b[0m+-------------+-------------------------------------------------------------------------+--------+\n",
      "\u001b[36mtriton-1  | \u001b[0m| pytorch     | /opt/tritonserver/backends/pytorch/libtriton_pytorch.so                 | {}     |\n",
      "\u001b[36mtriton-1  | \u001b[0m| onnxruntime | /opt/tritonserver/backends/onnxruntime/libtriton_onnxruntime.so         | {}     |\n",
      "\u001b[36mtriton-1  | \u001b[0m| openvino    | /opt/tritonserver/backends/openvino_2021_2/libtriton_openvino_2021_2.so | {}     |\n",
      "\u001b[36mtriton-1  | \u001b[0m| tensorflow  | /opt/tritonserver/backends/tensorflow1/libtriton_tensorflow1.so         | {}     |\n",
      "\u001b[36mtriton-1  | \u001b[0m| python      | /opt/tritonserver/backends/python/libtriton_python.so                   | {}     |\n",
      "\u001b[36mtriton-1  | \u001b[0m| tensorrt    | /opt/tritonserver/backends/tensorrt/libtriton_tensorrt.so               | {}     |\n",
      "\u001b[36mtriton-1  | \u001b[0m+-------------+-------------------------------------------------------------------------+--------+\n",
      "\u001b[36mtriton-1  | \u001b[0m\n",
      "\u001b[36mtriton-1  | \u001b[0mI0809 18:56:46.642089 7 server.cc:589] \n",
      "\u001b[36mtriton-1  | \u001b[0m+--------------------------------+---------+--------+\n",
      "\u001b[36mtriton-1  | \u001b[0m| Model                          | Version | Status |\n",
      "\u001b[36mtriton-1  | \u001b[0m+--------------------------------+---------+--------+\n",
      "\u001b[36mtriton-1  | \u001b[0m| transformer_tensorrt_inference | 1       | READY  |\n",
      "\u001b[36mtriton-1  | \u001b[0m| transformer_tensorrt_model     | 1       | READY  |\n",
      "\u001b[36mtriton-1  | \u001b[0m| transformer_tensorrt_tokenize  | 1       | READY  |\n",
      "\u001b[36mtriton-1  | \u001b[0m+--------------------------------+---------+--------+\n",
      "\u001b[36mtriton-1  | \u001b[0m\n",
      "\u001b[36mtriton-1  | \u001b[0mI0809 18:56:46.642161 7 tritonserver.cc:1865] \n",
      "\u001b[36mtriton-1  | \u001b[0m+----------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\u001b[36mtriton-1  | \u001b[0m| Option                           | Value                                                                                                                                                                                  |\n",
      "\u001b[36mtriton-1  | \u001b[0m+----------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\u001b[36mtriton-1  | \u001b[0m| server_id                        | triton                                                                                                                                                                                 |\n",
      "\u001b[36mtriton-1  | \u001b[0m| server_version                   | 2.18.0                                                                                                                                                                                 |\n",
      "\u001b[36mtriton-1  | \u001b[0m| server_extensions                | classification sequence model_repository model_repository(unload_dependents) schedule_policy model_configuration system_shared_memory cuda_shared_memory binary_tensor_data statistics |\n",
      "\u001b[36mtriton-1  | \u001b[0m| model_repository_path[0]         | /models                                                                                                                                                                                |\n",
      "\u001b[36mtriton-1  | \u001b[0m| model_control_mode               | MODE_NONE                                                                                                                                                                              |\n",
      "\u001b[36mtriton-1  | \u001b[0m| strict_model_config              | 1                                                                                                                                                                                      |\n",
      "\u001b[36mtriton-1  | \u001b[0m| rate_limit                       | OFF                                                                                                                                                                                    |\n",
      "\u001b[36mtriton-1  | \u001b[0m| pinned_memory_pool_byte_size     | 268435456                                                                                                                                                                              |\n",
      "\u001b[36mtriton-1  | \u001b[0m| cuda_memory_pool_byte_size{0}    | 67108864                                                                                                                                                                               |\n",
      "\u001b[36mtriton-1  | \u001b[0m| response_cache_byte_size         | 0                                                                                                                                                                                      |\n",
      "\u001b[36mtriton-1  | \u001b[0m| min_supported_compute_capability | 6.0                                                                                                                                                                                    |\n",
      "\u001b[36mtriton-1  | \u001b[0m| strict_readiness                 | 1                                                                                                                                                                                      |\n",
      "\u001b[36mtriton-1  | \u001b[0m| exit_timeout                     | 30                                                                                                                                                                                     |\n",
      "\u001b[36mtriton-1  | \u001b[0m+----------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\u001b[36mtriton-1  | \u001b[0m\n",
      "\u001b[36mtriton-1  | \u001b[0mI0809 18:56:46.643902 7 grpc_server.cc:4195] Started GRPCInferenceService at 0.0.0.0:8001\n",
      "\u001b[36mtriton-1  | \u001b[0mI0809 18:56:46.644111 7 http_server.cc:2857] Started HTTPService at 0.0.0.0:8000\n",
      "\u001b[36mtriton-1  | \u001b[0mI0809 18:56:46.687620 7 http_server.cc:167] Started Metrics Service at 0.0.0.0:8002\n",
      "\u001b[36mtriton-1  | \u001b[0m[\u001b[1;31mERROR\u001b[0;39m] \u001b[33mevhtp.c:3119     \u001b[39m\u001b[94mODDITY, resuming when not paused?!?\u001b[39m :: \u001b[35m(errno: None)\u001b[39m\n",
      "\u001b[36mtriton-1  | \u001b[0m[\u001b[1;31mERROR\u001b[0;39m] \u001b[33mevhtp.c:3119     \u001b[39m\u001b[94mODDITY, resuming when not paused?!?\u001b[39m :: \u001b[35m(errno: None)\u001b[39m\n"
     ]
    }
   ],
   "source": [
    "docker-compose logs triton"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea40f5fb-f2b8-4b38-a939-d84f6d3e0d2c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66ee9bf-874d-4f7a-8c3e-0afd37f27a30",
   "metadata": {},
   "source": [
    "## (Optional) Stopping and Restarting the Services"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec81fbe-3d00-42ea-8e23-5a1ffbbd379a",
   "metadata": {},
   "source": [
    "If at any point you need to stop and restart the services, for example if you do something inadvertent that crashes one of the application you can restart all of the services by executing the following `restart.sh` script, which basically does `docker-compose down && docker-compose up -d` along with resetting the state of the `redis` service which you will be doing later, but which takes time and we would not want you to have to repeat if a restart is required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44f6eb2-a023-4808-b1d7-8f2ad6a9e8e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bringing containerized services down...\n"
     ]
    }
   ],
   "source": [
    "./restart.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63bdb0b9-333d-4dac-9632-5b672a0caedd",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40422f7-2d03-45b8-9a47-e86d6e0e1039",
   "metadata": {},
   "source": [
    "## Next Lesson"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2d3285-e751-4a5a-935b-e65b62b26a00",
   "metadata": {},
   "source": [
    "Move to the next lesson by double-clicking *Lesson 01.ipynb* on the file-viewer on the left-hand side of your Jupyter Lab environment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bash",
   "language": "bash",
   "name": "bash"
  },
  "language_info": {
   "codemirror_mode": "shell",
   "file_extension": ".sh",
   "mimetype": "text/x-sh",
   "name": "bash"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
