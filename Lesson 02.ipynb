{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Techniques for Improving the Effectiveness of RAG Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the cell below to load the video presentation that accompanies this notebook, and watch it before working through the materials in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video controls width=\"640\" height=\"360\">\n",
       "    <source src=\"https://d36m44n9vdbmda.cloudfront.net/assets/s-fx-20-v1/lesson-02.mp4\" type=\"video/mp4\">\n",
       "    Your browser does not support the video tag.\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "video_url = \"https://d36m44n9vdbmda.cloudfront.net/assets/s-fx-20-v1/lesson-02.mp4\"\n",
    "\n",
    "video_html = f\"\"\"\n",
    "<video controls width=\"640\" height=\"360\">\n",
    "    <source src=\"{video_url}\" type=\"video/mp4\">\n",
    "    Your browser does not support the video tag.\n",
    "</video>\n",
    "\"\"\"\n",
    "\n",
    "display(HTML(video_html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 02: Loading the Vector/Document Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to Lesson 02! In this section, we will focus on how to launch a database and search it with both semantic and keyword search.\n",
    "\n",
    "Like we mentioned in Lesson 00, our RAG system is comprised of modular and independently scalable services, each running in its own container--an architecture well-suited to deploying in a cloud environment. \n",
    "\n",
    "In addition to using `docker-compose`, which is particularly well suited to single node deployments like the environment you are working in today, public cloud providers include managed container orchestration services that help run this kind of architecture; popular examples include [Amazon Elastic Container Service (ECS)](https://aws.amazon.com/ecs/), [Azure Container Apps](https://azure.microsoft.com/en-us/products/container-apps/), and [Google Kubernetes Engine](https://cloud.google.com/kubernetes-engine).\n",
    "\n",
    "NVIDIA provides a related service, with a serverless API to deploy and manage AI workloads on GPUs, called [NVIDIA Cloud Functions (NVCF)](https://docs.nvidia.com/cloud-functions/user-guide/latest/cloud-function/overview.html).\n",
    "\n",
    "The NVCF API supports HTTP polling, HTTP streaming & gRPC. Deep learning models (including embedding functions and LLMs) are especially easy to prepare and serve on NVCF through [Triton Inference Server](https://developer.nvidia.com/triton-inference-server), but NVCF supports containers using other backends as well."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This notebook will focus on the embedder and hybrid search.**\n",
    "\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "<img src=\"img/02_overview.png\" width=\"850\" alt=\"architecture diagram with the embedder and search highlighted\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restart the Services"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make sure you're staring this lesson with all your services in the correct state, please restart them by running the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bringing containerized services down...\n",
      "Services down.\n",
      "Bringing containerized services back up...\n",
      "Services back up.\n"
     ]
    }
   ],
   "source": [
    "!./restart.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Serving our Embedding Model with Triton"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You deployed a NVIDIA Triton Inference Server in Lesson 00."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36mtriton-1  | \u001b[0m\n",
      "\u001b[36mtriton-1  | \u001b[0m=============================\n",
      "\u001b[36mtriton-1  | \u001b[0m== Triton Inference Server ==\n",
      "\u001b[36mtriton-1  | \u001b[0m=============================\n",
      "\u001b[36mtriton-1  | \u001b[0m\n",
      "\u001b[36mtriton-1  | \u001b[0mNVIDIA Release 22.01 (build 31237563)\n",
      "\u001b[36mtriton-1  | \u001b[0m\n",
      "\u001b[36mtriton-1  | \u001b[0mCopyright (c) 2018-2021, NVIDIA CORPORATION & AFFILIATES.  All rights reserved.\n",
      "\u001b[36mtriton-1  | \u001b[0m\n",
      "\u001b[36mtriton-1  | \u001b[0mVarious files include modifications (c) NVIDIA CORPORATION.  All rights reserved.\n",
      "\u001b[36mtriton-1  | \u001b[0m\n",
      "\u001b[36mtriton-1  | \u001b[0mThis container image and its contents are governed by the NVIDIA Deep Learning Container License.\n",
      "\u001b[36mtriton-1  | \u001b[0mBy pulling and using the container, you accept the terms and conditions of this license:\n",
      "\u001b[36mtriton-1  | \u001b[0mhttps://developer.nvidia.com/ngc/nvidia-deep-learning-container-license\n",
      "\u001b[36mtriton-1  | \u001b[0m\n",
      "\u001b[36mtriton-1  | \u001b[0mI0809 23:49:38.170078 7 metrics.cc:298] Collecting metrics for GPU 0: NVIDIA A100 80GB PCIe\n",
      "\u001b[36mtriton-1  | \u001b[0mI0809 23:49:38.418966 7 libtorch.cc:1227] TRITONBACKEND_Initialize: pytorch\n",
      "\u001b[36mtriton-1  | \u001b[0mI0809 23:49:38.418993 7 libtorch.cc:1237] Triton TRITONBACKEND API version: 1.7\n",
      "\u001b[36mtriton-1  | \u001b[0mI0809 23:49:38.418997 7 libtorch.cc:1243] 'pytorch' TRITONBACKEND API version: 1.7\n",
      "\u001b[36mtriton-1  | \u001b[0m2024-08-09 23:49:38.545252: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "\u001b[36mtriton-1  | \u001b[0m2024-08-09 23:49:38.571552: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "\u001b[36mtriton-1  | \u001b[0mI0809 23:49:38.571607 7 tensorflow.cc:2176] TRITONBACKEND_Initialize: tensorflow\n",
      "\u001b[36mtriton-1  | \u001b[0mI0809 23:49:38.571623 7 tensorflow.cc:2186] Triton TRITONBACKEND API version: 1.7\n",
      "\u001b[36mtriton-1  | \u001b[0mI0809 23:49:38.571628 7 tensorflow.cc:2192] 'tensorflow' TRITONBACKEND API version: 1.7\n",
      "\u001b[36mtriton-1  | \u001b[0mI0809 23:49:38.571633 7 tensorflow.cc:2216] backend configuration:\n",
      "\u001b[36mtriton-1  | \u001b[0m{}\n",
      "\u001b[36mtriton-1  | \u001b[0mI0809 23:49:38.573324 7 onnxruntime.cc:2232] TRITONBACKEND_Initialize: onnxruntime\n",
      "\u001b[36mtriton-1  | \u001b[0mI0809 23:49:38.573340 7 onnxruntime.cc:2242] Triton TRITONBACKEND API version: 1.7\n",
      "\u001b[36mtriton-1  | \u001b[0mI0809 23:49:38.573344 7 onnxruntime.cc:2248] 'onnxruntime' TRITONBACKEND API version: 1.7\n",
      "\u001b[36mtriton-1  | \u001b[0mI0809 23:49:38.573346 7 onnxruntime.cc:2278] backend configuration:\n",
      "\u001b[36mtriton-1  | \u001b[0m{}\n",
      "\u001b[36mtriton-1  | \u001b[0mI0809 23:49:38.605807 7 openvino.cc:1234] TRITONBACKEND_Initialize: openvino\n",
      "\u001b[36mtriton-1  | \u001b[0mI0809 23:49:38.605823 7 openvino.cc:1244] Triton TRITONBACKEND API version: 1.7\n",
      "\u001b[36mtriton-1  | \u001b[0mI0809 23:49:38.605827 7 openvino.cc:1250] 'openvino' TRITONBACKEND API version: 1.7\n",
      "\u001b[36mtriton-1  | \u001b[0mI0809 23:49:38.791458 7 pinned_memory_manager.cc:240] Pinned memory pool is created at '0x7fae0a000000' with size 268435456\n",
      "\u001b[36mtriton-1  | \u001b[0mI0809 23:49:38.793769 7 cuda_memory_manager.cc:105] CUDA memory pool is created on device 0 with size 67108864\n",
      "\u001b[36mtriton-1  | \u001b[0mI0809 23:49:38.801620 7 model_repository_manager.cc:994] loading: transformer_tensorrt_tokenize:1\n",
      "\u001b[36mtriton-1  | \u001b[0mI0809 23:49:38.901961 7 model_repository_manager.cc:994] loading: transformer_tensorrt_model:1\n",
      "\u001b[36mtriton-1  | \u001b[0mI0809 23:49:38.911781 7 python.cc:1897] TRITONBACKEND_ModelInstanceInitialize: transformer_tensorrt_tokenize_0 (GPU device 0)\n",
      "\u001b[36mtriton-1  | \u001b[0mNone of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n",
      "\u001b[36mtriton-1  | \u001b[0mI0809 23:49:41.013444 7 model_repository_manager.cc:1149] successfully loaded 'transformer_tensorrt_tokenize' version 1\n",
      "\u001b[36mtriton-1  | \u001b[0mI0809 23:49:41.013838 7 tensorrt.cc:5145] TRITONBACKEND_Initialize: tensorrt\n",
      "\u001b[36mtriton-1  | \u001b[0mI0809 23:49:41.013874 7 tensorrt.cc:5155] Triton TRITONBACKEND API version: 1.7\n",
      "\u001b[36mtriton-1  | \u001b[0mI0809 23:49:41.013880 7 tensorrt.cc:5161] 'tensorrt' TRITONBACKEND API version: 1.7\n",
      "\u001b[36mtriton-1  | \u001b[0mI0809 23:49:41.013968 7 tensorrt.cc:5204] backend configuration:\n",
      "\u001b[36mtriton-1  | \u001b[0m{}\n",
      "\u001b[36mtriton-1  | \u001b[0mI0809 23:49:41.014003 7 tensorrt.cc:5256] TRITONBACKEND_ModelInitialize: transformer_tensorrt_model (version 1)\n",
      "\u001b[36mtriton-1  | \u001b[0mI0809 23:49:41.021997 7 tensorrt.cc:5305] TRITONBACKEND_ModelInstanceInitialize: transformer_tensorrt_model_0 (GPU device 0)\n",
      "\u001b[36mtriton-1  | \u001b[0mI0809 23:49:41.409423 7 logging.cc:49] [MemUsageChange] Init CUDA: CPU +714, GPU +0, now: CPU 3379, GPU 1435 (MiB)\n",
      "\u001b[36mtriton-1  | \u001b[0mI0809 23:49:42.208171 7 logging.cc:49] Loaded engine size: 1278 MiB\n",
      "\u001b[36mtriton-1  | \u001b[0mI0809 23:49:42.936036 7 logging.cc:49] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +1242, GPU +320, now: CPU 7818, GPU 2393 (MiB)\n",
      "\u001b[36mtriton-1  | \u001b[0mI0809 23:49:42.936399 7 logging.cc:49] [MemUsageChange] TensorRT-managed allocation in engine deserialization: CPU +0, GPU +637, now: CPU 0, GPU 637 (MiB)\n",
      "\u001b[36mtriton-1  | \u001b[0mI0809 23:49:42.944251 7 logging.cc:49] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +10, now: CPU 5262, GPU 2393 (MiB)\n",
      "\u001b[36mtriton-1  | \u001b[0mI0809 23:49:44.106045 7 logging.cc:49] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +464, now: CPU 0, GPU 1101 (MiB)\n",
      "\u001b[36mtriton-1  | \u001b[0mI0809 23:49:44.106268 7 tensorrt.cc:1409] Created instance transformer_tensorrt_model_0 on GPU 0 with stream priority 0 and optimization profile default[0];\n",
      "\u001b[36mtriton-1  | \u001b[0mI0809 23:49:44.106625 7 model_repository_manager.cc:1149] successfully loaded 'transformer_tensorrt_model' version 1\n",
      "\u001b[36mtriton-1  | \u001b[0mI0809 23:49:44.106878 7 model_repository_manager.cc:994] loading: transformer_tensorrt_inference:1\n",
      "\u001b[36mtriton-1  | \u001b[0mI0809 23:49:44.207182 7 model_repository_manager.cc:1149] successfully loaded 'transformer_tensorrt_inference' version 1\n",
      "\u001b[36mtriton-1  | \u001b[0mI0809 23:49:44.207261 7 server.cc:519] \n",
      "\u001b[36mtriton-1  | \u001b[0m+------------------+------+\n",
      "\u001b[36mtriton-1  | \u001b[0m| Repository Agent | Path |\n",
      "\u001b[36mtriton-1  | \u001b[0m+------------------+------+\n",
      "\u001b[36mtriton-1  | \u001b[0m+------------------+------+\n",
      "\u001b[36mtriton-1  | \u001b[0m\n",
      "\u001b[36mtriton-1  | \u001b[0mI0809 23:49:44.207329 7 server.cc:546] \n",
      "\u001b[36mtriton-1  | \u001b[0m+-------------+-------------------------------------------------------------------------+--------+\n",
      "\u001b[36mtriton-1  | \u001b[0m| Backend     | Path                                                                    | Config |\n",
      "\u001b[36mtriton-1  | \u001b[0m+-------------+-------------------------------------------------------------------------+--------+\n",
      "\u001b[36mtriton-1  | \u001b[0m| pytorch     | /opt/tritonserver/backends/pytorch/libtriton_pytorch.so                 | {}     |\n",
      "\u001b[36mtriton-1  | \u001b[0m| onnxruntime | /opt/tritonserver/backends/onnxruntime/libtriton_onnxruntime.so         | {}     |\n",
      "\u001b[36mtriton-1  | \u001b[0m| openvino    | /opt/tritonserver/backends/openvino_2021_2/libtriton_openvino_2021_2.so | {}     |\n",
      "\u001b[36mtriton-1  | \u001b[0m| tensorflow  | /opt/tritonserver/backends/tensorflow1/libtriton_tensorflow1.so         | {}     |\n",
      "\u001b[36mtriton-1  | \u001b[0m| python      | /opt/tritonserver/backends/python/libtriton_python.so                   | {}     |\n",
      "\u001b[36mtriton-1  | \u001b[0m| tensorrt    | /opt/tritonserver/backends/tensorrt/libtriton_tensorrt.so               | {}     |\n",
      "\u001b[36mtriton-1  | \u001b[0m+-------------+-------------------------------------------------------------------------+--------+\n",
      "\u001b[36mtriton-1  | \u001b[0m\n",
      "\u001b[36mtriton-1  | \u001b[0mI0809 23:49:44.207369 7 server.cc:589] \n",
      "\u001b[36mtriton-1  | \u001b[0m+--------------------------------+---------+--------+\n",
      "\u001b[36mtriton-1  | \u001b[0m| Model                          | Version | Status |\n",
      "\u001b[36mtriton-1  | \u001b[0m+--------------------------------+---------+--------+\n",
      "\u001b[36mtriton-1  | \u001b[0m| transformer_tensorrt_inference | 1       | READY  |\n",
      "\u001b[36mtriton-1  | \u001b[0m| transformer_tensorrt_model     | 1       | READY  |\n",
      "\u001b[36mtriton-1  | \u001b[0m| transformer_tensorrt_tokenize  | 1       | READY  |\n",
      "\u001b[36mtriton-1  | \u001b[0m+--------------------------------+---------+--------+\n",
      "\u001b[36mtriton-1  | \u001b[0m\n",
      "\u001b[36mtriton-1  | \u001b[0mI0809 23:49:44.207459 7 tritonserver.cc:1865] \n",
      "\u001b[36mtriton-1  | \u001b[0m+----------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\u001b[36mtriton-1  | \u001b[0m| Option                           | Value                                                                                                                                                                                  |\n",
      "\u001b[36mtriton-1  | \u001b[0m+----------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\u001b[36mtriton-1  | \u001b[0m| server_id                        | triton                                                                                                                                                                                 |\n",
      "\u001b[36mtriton-1  | \u001b[0m| server_version                   | 2.18.0                                                                                                                                                                                 |\n",
      "\u001b[36mtriton-1  | \u001b[0m| server_extensions                | classification sequence model_repository model_repository(unload_dependents) schedule_policy model_configuration system_shared_memory cuda_shared_memory binary_tensor_data statistics |\n",
      "\u001b[36mtriton-1  | \u001b[0m| model_repository_path[0]         | /models                                                                                                                                                                                |\n",
      "\u001b[36mtriton-1  | \u001b[0m| model_control_mode               | MODE_NONE                                                                                                                                                                              |\n",
      "\u001b[36mtriton-1  | \u001b[0m| strict_model_config              | 1                                                                                                                                                                                      |\n",
      "\u001b[36mtriton-1  | \u001b[0m| rate_limit                       | OFF                                                                                                                                                                                    |\n",
      "\u001b[36mtriton-1  | \u001b[0m| pinned_memory_pool_byte_size     | 268435456                                                                                                                                                                              |\n",
      "\u001b[36mtriton-1  | \u001b[0m| cuda_memory_pool_byte_size{0}    | 67108864                                                                                                                                                                               |\n",
      "\u001b[36mtriton-1  | \u001b[0m| response_cache_byte_size         | 0                                                                                                                                                                                      |\n",
      "\u001b[36mtriton-1  | \u001b[0m| min_supported_compute_capability | 6.0                                                                                                                                                                                    |\n",
      "\u001b[36mtriton-1  | \u001b[0m| strict_readiness                 | 1                                                                                                                                                                                      |\n",
      "\u001b[36mtriton-1  | \u001b[0m| exit_timeout                     | 30                                                                                                                                                                                     |\n",
      "\u001b[36mtriton-1  | \u001b[0m+----------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\u001b[36mtriton-1  | \u001b[0m\n",
      "\u001b[36mtriton-1  | \u001b[0mI0809 23:49:44.209183 7 grpc_server.cc:4195] Started GRPCInferenceService at 0.0.0.0:8001\n",
      "\u001b[36mtriton-1  | \u001b[0mI0809 23:49:44.209390 7 http_server.cc:2857] Started HTTPService at 0.0.0.0:8000\n",
      "\u001b[36mtriton-1  | \u001b[0mI0809 23:49:44.251889 7 http_server.cc:167] Started Metrics Service at 0.0.0.0:8002\n"
     ]
    }
   ],
   "source": [
    "!docker-compose logs triton"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NVIDIA Triton Inference Server is open-source software for fast and scalable AI inference on both GPU and CPU, and it's the standard way we run inference at NVIDIA. For more information, see the [Triton Inference Server readme on GitHub](https://github.com/triton-inference-server/server#documentation).\n",
    "\n",
    "Triton:\n",
    "- supports models and code in Python, C++, TensorFlow 1.x and 2.x, PyTorch, ONNX, TensorRT, RAPIDS FIL (for XGBoost, Scikit-learn Random Forest, and LightGBM), and OpenVINO.\n",
    "- optimizes inference for multiple query types (real-time, batch, streaming) and also supports model ensembles.\n",
    "- works with NVIDIA GPUs and x86 & ARM CPUs, including models . \n",
    "- runs on scale-out cloud or data center, enterprise edge, and even on embedded devices like the NVIDIA Jetson, in both bare metal and virtualized environments (e.g. VMware vSphere), with dedicated NVIDIA Triton builds for running on Windows, Jetson, and ARM SBSA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Embedding Model to TensorRT Engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To embed our chunks in a vector database, we'll need an embedding model. As we mentioned in the previous lesson, we're using the `SentenceTransformers` framework with the `e5-large-unsupervised` embedding model. In order to further increase inference speed, we can convert the PyTorch model into a TensorRT engine file and then serve the TensorRT engine with Triton.\n",
    "\n",
    "We have already performed this conversion for you, and the model is already available in your running `triton` service, but to do it yourself, please refer to `triton/README.md`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Health Check Triton"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a code snippet to check that Triton is up and running (check that we get a 200 status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*   Trying 172.19.0.5:8000...\n",
      "* Connected to triton (172.19.0.5) port 8000 (#0)\n",
      "> GET /v2/health/ready HTTP/1.1\n",
      "> Host: triton:8000\n",
      "> User-Agent: curl/7.88.1\n",
      "> Accept: */*\n",
      "> \n",
      "< HTTP/1.1 200 OK\n",
      "< Content-Length: 0\n",
      "< Content-Type: text/plain\n",
      "< \n",
      "* Connection #0 to host triton left intact\n"
     ]
    }
   ],
   "source": [
    "!curl -v http://triton:8000/v2/health/ready"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embed With Triton Function"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inside our `router` container's source code, there is a function that takes in text as strings, sends them to Triton Inference Server using the Triton Python client, and receives vectors of floating-point numbers in response. Here we look at that function and its supporting source code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we make the necessary imports for our function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import time\n",
    "import numpy as np\n",
    "import tritonclient.http"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define variables that will be used within our function.\n",
    "\n",
    "Note: `triton_host` in this environment is `triton`, but in your own environment, depending on how you run the `triton` container, this hostname may very well be different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "triton_host = \"triton\"\n",
    "triton_port = \"8000\"\n",
    "triton_model_name = \"transformer_tensorrt_inference\"\n",
    "triton_model_version = \"1\"\n",
    "\n",
    "triton_url = f\"{triton_host}:{triton_port}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_with_triton(query: List[str]) -> List[List[float]]:\n",
    "    triton_client = tritonclient.http.InferenceServerClient(\n",
    "        url=triton_url, verbose=False\n",
    "    )\n",
    "\n",
    "    triton_batch_size = len(query)\n",
    "    triton_inputs = []\n",
    "    triton_outputs = []\n",
    "    triton_text_input = tritonclient.http.InferInput(\n",
    "        name=\"TEXT\", shape=(triton_batch_size,), datatype=\"BYTES\"\n",
    "    )\n",
    "    triton_text_input.set_data_from_numpy(np.asarray(query, dtype=object))\n",
    "    triton_inputs.append(triton_text_input)\n",
    "    triton_outputs.append(\n",
    "        tritonclient.http.InferRequestedOutput(\"output\", binary_data=False)\n",
    "    )\n",
    "\n",
    "    inference_results = triton_client.infer(\n",
    "        model_name=triton_model_name,\n",
    "        model_version=triton_model_version,\n",
    "        inputs=triton_inputs,\n",
    "        outputs=triton_outputs,\n",
    "    )\n",
    "\n",
    "    embedded_query = inference_results.as_numpy(\"output\").tolist()\n",
    "    return embedded_query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try Embed With Triton Function"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that when used in information retrieval, this embedding model works best when text queries start with the prefix \"query: \", and text documents start with the prefix \"passage: \", according to [the model card on HuggingFace](https://huggingface.co/intfloat/e5-large-unsupervised#faq)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.0250091552734375, -0.057159423828125, -0.01119232177734375, -0.016693115234375, 0.033477783203125, 0.0107879638671875, -0.037811279296875, -0.0306243896484375, 0.0199432373046875, -0.058837890625, 0.0272979736328125, 0.0238037109375, 0.0172119140625, 0.023040771484375, -0.0020465850830078125, 0.0271759033203125, -0.0022258758544921875, 0.01262664794921875, 0.019866943359375, 0.001186370849609375, 0.01520538330078125, 0.0262603759765625, 0.0290374755859375, 0.0178985595703125, 0.0020465850830078125, 0.045562744140625, -0.010589599609375, -0.00353240966796875, 0.00022268295288085938, 0.053924560546875, -0.00968170166015625, -0.036468505859375, 0.0286407470703125, 0.0264892578125, 0.0167388916015625, -0.01507568359375, 0.01428985595703125, -0.034881591796875, -0.048248291015625, -0.015899658203125, 0.027374267578125, -0.03411865234375, 0.028167724609375, 0.03265380859375, -0.009521484375, 0.0193328857421875, -0.006137847900390625, -0.038970947265625, -0.01499176025390625, -0.031982421875, 0.05633544921875, -0.026092529296875, 0.01163482666015625, 0.01433563232421875, -0.0223541259765625, 0.0020656585693359375, 0.06927490234375, 0.0726318359375, -0.00768280029296875, -0.060638427734375, -0.0152740478515625, -0.02099609375, 0.021270751953125, 0.005916595458984375, -0.032135009765625, 0.056304931640625, -0.01068878173828125, -0.0028228759765625, -0.01483154296875, -0.032012939453125, 0.0169830322265625, 0.00994873046875, 0.0023250579833984375, -0.004058837890625, -0.006458282470703125, -0.0136871337890625, -0.029815673828125, 0.007411956787109375, 0.052764892578125, -0.003143310546875, 0.040374755859375, 0.0274810791015625, -0.0135650634765625, -0.01763916015625, 0.03997802734375, -0.010406494140625, 0.08221435546875, -0.00643157958984375, -0.01068115234375, 0.00732421875, -0.0170745849609375, 0.01727294921875, 0.005107879638671875, -0.035400390625, 0.00677490234375, 0.01012420654296875, 0.0031757354736328125, 0.022216796875, -0.002124786376953125, -0.061737060546875, 0.031646728515625, 0.0006861686706542969, 0.01322174072265625, 0.0654296875, -0.0204925537109375, -0.0240020751953125, -0.0102386474609375, 0.017242431640625, -0.00572967529296875, 0.0031414031982421875, -0.0504150390625, 0.041229248046875, 0.003421783447265625, 0.01216888427734375, 0.006786346435546875, -0.032867431640625, -0.0088958740234375, 0.0127105712890625, 0.0196075439453125, 0.08868408203125, 0.0010290145874023438, -0.0677490234375, 0.041229248046875, -0.0233154296875, -0.037384033203125, 0.034393310546875, 0.044189453125, -0.0196533203125, -0.01079559326171875, 0.0213775634765625, -0.0247344970703125, -0.03240966796875, -0.0196990966796875, 0.0031337738037109375, 0.0232086181640625, -0.006927490234375, -0.108154296875, -0.08868408203125, 0.05218505859375, -0.048431396484375, 0.00916290283203125, -0.056976318359375, 0.007053375244140625, 0.0299530029296875, 0.059173583984375, 0.056121826171875, 0.04425048828125, 0.052520751953125, -0.014251708984375, -0.018402099609375, -0.019561767578125, -0.023406982421875, 0.006481170654296875, 0.03570556640625, 0.0136871337890625, -0.07818603515625, 0.04351806640625, 0.019683837890625, -0.0120849609375, -0.0233001708984375, 0.0172882080078125, -0.02691650390625, 0.002155303955078125, -0.00444793701171875, -0.0343017578125, -0.034637451171875, 0.047149658203125, 0.0235748291015625, 0.055908203125, 0.07501220703125, -0.00659942626953125, 0.0465087890625, 0.032257080078125, 0.0457763671875, 0.0164031982421875, 0.0010776519775390625, 0.0032138824462890625, -0.005580902099609375, 0.033905029296875, 0.004695892333984375, -0.0188140869140625, -0.0118865966796875, 0.02020263671875, 0.043914794921875, -0.02056884765625, 0.0604248046875, 0.0018463134765625, -0.015869140625, 0.0082855224609375, -0.05072021484375, -0.0258026123046875, -0.05780029296875, 0.051971435546875, -0.04833984375, -0.0085906982421875, -0.01849365234375, 0.0167236328125, 0.035369873046875, -0.047088623046875, -0.0004901885986328125, -0.037445068359375, 0.0204925537109375, 0.055755615234375, -0.02703857421875, 0.0345458984375, 0.045501708984375, 0.0142822265625, -0.0023555755615234375, 0.040802001953125, -0.0150299072265625, -0.049957275390625, -0.001712799072265625, -0.013031005859375, 0.018157958984375, -0.0200653076171875, -0.0241546630859375, 0.006702423095703125, -0.02032470703125, -0.03656005859375, 0.029632568359375, -0.037628173828125, 0.07025146484375, 0.0244140625, 0.00215911865234375, -0.01515960693359375, 0.0086212158203125, -0.0241546630859375, -0.026123046875, -0.0188140869140625, 0.0166168212890625, -0.050506591796875, -0.0163116455078125, 0.0447998046875, 0.0726318359375, -0.0167236328125, -0.0039043426513671875, 0.0223388671875, 0.05572509765625, 0.020355224609375, -0.0016450881958007812, -0.0350341796875, -0.0361328125, 0.01314544677734375, -0.0084686279296875, 0.00482940673828125, 0.015777587890625, 0.019866943359375, 0.0347900390625, 0.0265045166015625, -0.017791748046875, -0.013916015625, 0.0701904296875, -0.0028228759765625, -0.0191802978515625, 0.02142333984375, -0.043731689453125, 0.0033111572265625, -0.021240234375, -0.0257720947265625, -0.00701141357421875, -0.01474761962890625, 0.0064849853515625, -0.018524169921875, -0.0201263427734375, 0.054473876953125, 0.023345947265625, -0.0274200439453125, 0.002155303955078125, 0.0650634765625, -0.029876708984375, 0.01108551025390625, -0.007740020751953125, -0.027801513671875, -0.026611328125, 0.01070404052734375, 0.04241943359375, -0.0082244873046875, -0.01593017578125, 0.01806640625, -0.0006155967712402344, 0.01482391357421875, -0.035736083984375, 0.01076507568359375, 0.038543701171875, -0.0537109375, 0.01097869873046875, -0.01177215576171875, -0.0226287841796875, 0.0265655517578125, 0.0204620361328125, 0.0149078369140625, 0.03973388671875, -0.031982421875, -0.04534912109375, -0.02325439453125, -0.026092529296875, 0.0163116455078125, -0.004261016845703125, 0.0132293701171875, 0.02325439453125, -0.026458740234375, -0.024505615234375, -0.002471923828125, 0.01114654541015625, -0.0303955078125, -0.01690673828125, -0.001781463623046875, -0.0036983489990234375, -0.004161834716796875, 0.028167724609375, 0.03680419921875, -0.03863525390625, 0.0117950439453125, -0.037322998046875, -0.035552978515625, 0.035400390625, 0.0178070068359375, 0.014434814453125, 0.039154052734375, 0.0168609619140625, 0.04901123046875, 0.02520751953125, -0.02777099609375, -0.02081298828125, -0.0154571533203125, -0.025146484375, -0.03863525390625, 0.043182373046875, -0.01064300537109375, 0.0229644775390625, -0.009490966796875, 0.00653076171875, -0.0001480579376220703, 0.059906005859375, -0.0005745887756347656, -0.004245758056640625, 0.01373291015625, 0.01282501220703125, -0.0157318115234375, -0.0299835205078125, 0.045257568359375, -0.036956787109375, 0.021148681640625, 0.010772705078125, 0.002658843994140625, 0.00658416748046875, 0.0222320556640625, 0.03289794921875, 0.0030269622802734375, 0.0279541015625, -0.03546142578125, 0.03802490234375, -0.020355224609375, -0.0106353759765625, -0.03314208984375, -0.0113372802734375, 0.05242919921875, 0.0021533966064453125, -0.0013093948364257812, 0.019195556640625, 0.042999267578125, -0.0293731689453125, -0.035064697265625, -0.045257568359375, -0.0162506103515625, 0.0144500732421875, -0.005523681640625, 0.04107666015625, 0.007099151611328125, 0.033477783203125, 0.001674652099609375, 0.031646728515625, -0.0193634033203125, 0.005771636962890625, -0.0601806640625, 0.0362548828125, -0.0096588134765625, 0.046966552734375, -0.0107879638671875, 0.04583740234375, 0.0084075927734375, 0.01374053955078125, 0.056304931640625, -0.0009546279907226562, -0.09161376953125, 0.0039825439453125, -0.0036869049072265625, -0.0088348388671875, -0.008270263671875, -0.0127410888671875, 0.01227569580078125, 0.007564544677734375, -0.0135955810546875, -0.043975830078125, 0.01088714599609375, -0.046966552734375, 0.01220703125, 0.004119873046875, 0.0211181640625, -0.0209808349609375, -0.0026683807373046875, 0.053009033203125, 0.0035991668701171875, -0.031005859375, 0.0221710205078125, 0.032470703125, -0.036834716796875, -0.032196044921875, 0.011627197265625, 0.013397216796875, 0.0780029296875, 0.034912109375, 0.03448486328125, -0.03240966796875, 0.01256561279296875, 0.008697509765625, 0.033477783203125, 0.006351470947265625, 0.0267181396484375, -0.0265045166015625, -0.0684814453125, -0.022674560546875, 0.0181121826171875, 0.0565185546875, -0.0131072998046875, 0.0207672119140625, 0.0244598388671875, 0.01160430908203125, -0.0482177734375, -0.013336181640625, 0.0125885009765625, 0.00872039794921875, -0.037078857421875, -0.0227203369140625, -0.038665771484375, -0.031707763671875, -0.00916290283203125, 0.066650390625, 0.0066680908203125, 0.006587982177734375, 0.0294952392578125, 0.0254058837890625, -0.0433349609375, -0.036834716796875, -0.042236328125, 0.0153961181640625, 0.0203094482421875, -0.004848480224609375, 0.00878143310546875, -0.0310821533203125, 0.034088134765625, 0.06317138671875, 0.00652313232421875, 0.01067352294921875, -0.1539306640625, -0.00264739990234375, 0.0180206298828125, 0.02886962890625, -0.051788330078125, 0.0022525787353515625, -0.05316162109375, -0.035858154296875, 0.0172271728515625, 0.007678985595703125, 0.0157318115234375, -0.025482177734375, -0.0016126632690429688, 0.0015163421630859375, 0.0130462646484375, -0.0006747245788574219, 0.0272064208984375, 0.006053924560546875, 0.039947509765625, -0.0003161430358886719, 0.017059326171875, -0.01433563232421875, -0.0148773193359375, -0.005767822265625, 0.035552978515625, 0.012481689453125, -0.007293701171875, -0.001102447509765625, 0.0312042236328125, -0.0153961181640625, -0.0550537109375, 0.007030487060546875, 0.0117950439453125, -0.0333251953125, 0.0016536712646484375, -0.04766845703125, -0.049835205078125, 0.036651611328125, -0.00125885009765625, -0.04852294921875, -0.010101318359375, -0.0227203369140625, 0.0022830963134765625, 0.038726806640625, 0.0291748046875, 0.04608154296875, -0.0014667510986328125, -0.013275146484375, -0.0019359588623046875, -0.018402099609375, 0.007740020751953125, -0.04913330078125, -0.038116455078125, 0.0026798248291015625, -0.028167724609375, 0.0084075927734375, 0.05999755859375, -0.01137542724609375, 0.043609619140625, -0.02264404296875, -0.00254058837890625, -0.027008056640625, 0.0299835205078125, 0.0112457275390625, -0.0267791748046875, -0.00847625732421875, -0.025543212890625, -0.01558685302734375, 0.016357421875, 0.0311279296875, -0.0153350830078125, 0.006999969482421875, 0.0282745361328125, -0.01248931884765625, -0.0254669189453125, -0.01067352294921875, -0.01403045654296875, 0.0361328125, 0.0101318359375, 0.043060302734375, 0.007595062255859375, -0.0006375312805175781, 0.016998291015625, -0.0869140625, -0.01502227783203125, -0.006969451904296875, -0.0014848709106445312, 0.0194549560546875, -0.04083251953125, 0.037384033203125, 0.0266876220703125, 0.01448822021484375, -0.040252685546875, -0.02069091796875, 0.0192718505859375, 0.003765106201171875, 0.0195159912109375, -0.046844482421875, 0.0246734619140625, 0.0173187255859375, -0.042938232421875, -0.0095977783203125, 0.06298828125, -0.0005507469177246094, 0.0200653076171875, 0.0187835693359375, -0.038909912109375, 0.032012939453125, 0.00482940673828125, -0.004009246826171875, -0.024261474609375, 0.004535675048828125, 0.01367950439453125, -0.037322998046875, 0.0153656005859375, -0.052001953125, 0.03765869140625, -0.01357269287109375, -0.033966064453125, 0.0012235641479492188, -0.0001678466796875, 0.037384033203125, 0.021759033203125, 0.006328582763671875, -0.030120849609375, -0.057952880859375, 0.005542755126953125, -0.00408935546875, -0.01073455810546875, 0.0201568603515625, -0.03778076171875, 0.0085601806640625, -0.0209197998046875, -0.0029754638671875, 0.0294036865234375, -0.0031490325927734375, -0.004192352294921875, 0.032623291015625, -0.0190582275390625, 0.019805908203125, -0.036529541015625, -0.0225067138671875, 0.037567138671875, -0.034454345703125, 0.035003662109375, 0.0011167526245117188, 0.028350830078125, -0.047332763671875, -0.0114593505859375, -0.038421630859375, 0.004230499267578125, 0.07464599609375, 0.01202392578125, 0.04107666015625, 0.0191497802734375, 0.00826263427734375, 0.0263824462890625, -0.0281524658203125, -0.008697509765625, 0.01904296875, -0.01385498046875, 0.037322998046875, 0.00824737548828125, -0.005290985107421875, -0.006256103515625, 9.620189666748047e-05, 0.01343536376953125, 0.027923583984375, -0.020721435546875, 0.0201416015625, -0.0035152435302734375, -0.06640625, -0.05511474609375, 0.00745391845703125, -0.05145263671875, 0.0036716461181640625, -0.0251007080078125, -0.035552978515625, 0.040191650390625, 0.011474609375, 0.036163330078125, 0.00475311279296875, -0.044586181640625, -0.036773681640625, 0.0207366943359375, 0.01142120361328125, 0.03521728515625, -0.01104736328125, 0.0285491943359375, 0.0034008026123046875, 0.00681304931640625, 0.0225830078125, -0.01206207275390625, -0.09710693359375, -0.018829345703125, -0.01450347900390625, -0.024505615234375, 0.046356201171875, 0.01445770263671875, -0.0501708984375, 0.016265869140625, -0.024322509765625, 0.053253173828125, 0.0187225341796875, -0.0066986083984375, -0.0003979206085205078, 0.0277557373046875, 0.0098419189453125, 0.043304443359375, 0.01232147216796875, 0.0310821533203125, 0.157958984375, 0.06024169921875, -0.002819061279296875, -0.10125732421875, 0.01335906982421875, -0.0187835693359375, -0.01171112060546875, -0.0105133056640625, 0.0013723373413085938, 0.004123687744140625, 0.0133056640625, 0.01558685302734375, 0.004756927490234375, 0.00566864013671875, -0.00946044921875, 0.0584716796875, -0.032470703125, -0.020416259765625, 0.0250396728515625, -0.003841400146484375, 0.0012617111206054688, 0.040802001953125, -0.041534423828125, -0.047882080078125, -0.01047515869140625, -0.01399993896484375, 0.0020694732666015625, -0.0294036865234375, 0.037628173828125, -0.01535797119140625, -0.035552978515625, -0.035400390625, -0.027374267578125, 0.0064697265625, 0.002532958984375, 0.018280029296875, -0.003635406494140625, -0.0219268798828125, -0.0133056640625, 0.005176544189453125, 0.0259246826171875, -0.01021575927734375, -0.021636962890625, -0.01381683349609375, -0.0238800048828125, -0.0282135009765625, -0.0390625, 0.02081298828125, -0.025177001953125, -2.4497509002685547e-05, 0.0249481201171875, 0.024749755859375, 0.0214996337890625, -0.040069580078125, 0.006595611572265625, 0.044586181640625, 0.0204925537109375, -0.041107177734375, -0.051971435546875, -0.043853759765625, -0.01432037353515625, -0.018951416015625, 0.03936767578125, -0.0302581787109375, 0.0204315185546875, -0.0025177001953125, -0.00405120849609375, 0.0250396728515625, -0.0400390625, 0.004573822021484375, -0.01287841796875, 0.03582763671875, 0.0007834434509277344, -0.03955078125, 0.029876708984375, 0.0184326171875, -0.0003371238708496094, 0.0281829833984375, -0.02886962890625, -0.017730712890625, -0.036712646484375, 0.0165252685546875, -5.8531761169433594e-05, -0.00033926963806152344, -0.0237274169921875, 0.039794921875, 0.035552978515625, 0.0148773193359375, 0.040771484375, 0.0015430450439453125, -0.0027561187744140625, -0.0023250579833984375, 0.05084228515625, -0.01568603515625, -0.00148773193359375, -0.0148773193359375, -0.03985595703125, -0.0250396728515625, -0.016387939453125, 0.0182342529296875, 0.051422119140625, 0.052093505859375, -0.0208740234375, 0.0286712646484375, -0.024627685546875, 0.02655029296875, 0.0085296630859375, -0.046142578125, -0.044525146484375, -0.004528045654296875, 0.031982421875, -0.06597900390625, -0.02301025390625, -0.068603515625, -0.0081939697265625, 0.01371002197265625, 0.01393890380859375, -0.03424072265625, 0.005657196044921875, 0.006603240966796875, 0.054351806640625, -0.02862548828125, -0.02166748046875, -0.055908203125, -0.03961181640625, -0.0167694091796875, -0.01201629638671875, 0.0014486312866210938, 0.01384735107421875, 0.01537322998046875, -0.007244110107421875, 0.03363037109375, 0.00461578369140625, 0.01450347900390625, 0.0147705078125, 0.0369873046875, -0.04400634765625, 0.0279541015625, -0.0157928466796875, -0.04510498046875, 0.019073486328125, -0.0667724609375, -0.046051025390625, 0.035369873046875, -0.0312347412109375, 0.0467529296875, 0.046966552734375, -0.02838134765625, -0.007965087890625, 0.011749267578125, -0.0517578125, -0.01183319091796875, -0.0183868408203125, -0.004993438720703125, 0.003505706787109375, 0.01482391357421875, 0.015625, -0.01531219482421875, -0.0413818359375, -0.01399993896484375, 0.034027099609375, 0.031036376953125, -0.00609588623046875, 0.03173828125, 0.0009751319885253906, 0.024383544921875, 0.01494598388671875, 0.0227508544921875, 0.00609588623046875, 0.01385498046875, 0.007137298583984375, 0.04327392578125, 0.031494140625, -0.0007710456848144531, -0.13037109375, -0.01123046875, 0.002197265625, 0.004482269287109375, 0.0294647216796875, -0.07861328125, 0.032379150390625, 0.033905029296875, -0.050262451171875, 0.034912109375, -0.0300445556640625, -0.032318115234375, 0.025482177734375, 0.04522705078125, 0.0159912109375, 0.0195465087890625, 0.0113372802734375, 0.0169525146484375, 0.01343536376953125, 0.0166168212890625, -0.002727508544921875, 0.0178375244140625, -0.0173492431640625, 0.0201263427734375, -0.016326904296875, -0.029815673828125, 0.0222930908203125, -0.03558349609375, 0.022613525390625, 0.0377197265625, 0.01462554931640625, 0.033233642578125, 0.035491943359375, 0.03179931640625, 0.0101470947265625, 0.00563812255859375, -0.0216522216796875, 0.0014238357543945312, -0.01453399658203125, 0.012481689453125, 0.0302886962890625, 0.03424072265625, -0.017486572265625, -0.01015472412109375, -0.0104217529296875, -0.01248931884765625, -0.047271728515625, 0.017974853515625, 0.005481719970703125, -0.01287841796875, 0.016998291015625, 0.014434814453125, 0.0005521774291992188, -0.06396484375, 0.00522613525390625, -0.030914306640625, 0.01311492919921875, 0.020751953125, -0.02667236328125, -0.0264129638671875, 0.0126495361328125, 0.03350830078125, -0.0274200439453125, -0.00939178466796875, -0.004962921142578125, -0.0240631103515625, -0.053741455078125, 0.0168609619140625, 0.0283966064453125, -0.0258941650390625, -0.0030612945556640625, -0.0535888671875, -0.0247955322265625, 0.07830810546875, 0.0034122467041015625, 0.03741455078125, 0.0258941650390625, -0.0496826171875, 0.003665924072265625, -0.043212890625, 0.072998046875, -0.01537322998046875, -0.034088134765625, -0.013702392578125, 0.01288604736328125, -0.02081298828125, -0.041168212890625, 0.07464599609375, -0.0076446533203125, 0.0233612060546875, -0.04425048828125, 0.026031494140625, 0.0038852691650390625, -0.01557159423828125, -0.015228271484375, -0.0149383544921875, -0.04925537109375, -0.030670166015625, -0.01520538330078125, -0.01507568359375, 0.01027679443359375, 0.004650115966796875, 0.0004696846008300781, 0.0243988037109375, 0.01004791259765625, 0.05084228515625, 0.0555419921875, 0.03173828125, -0.002300262451171875, 0.036651611328125, -0.027496337890625, 0.0006566047668457031, -0.01296234130859375, -0.01073455810546875, -0.07293701171875, -0.0085601806640625, -0.03338623046875, -0.004665374755859375, -0.0187530517578125, 0.019073486328125, 0.03765869140625, -0.060455322265625, 0.00440216064453125, 0.042144775390625, -0.042877197265625, 0.025238037109375, 0.07489013671875, 0.024322509765625, 0.03631591796875, -0.0218353271484375, 0.049041748046875, -0.007038116455078125, 0.027099609375, -0.012542724609375, 0.01119232177734375, 0.060089111328125, -0.01194000244140625, 0.05517578125, 0.011566162109375, -0.040008544921875, 0.043182373046875, -0.00890350341796875, 0.050079345703125, -0.0290069580078125, 0.01204681396484375, -0.00678253173828125, -0.034210205078125, -1.3113021850585938e-06, 0.006259918212890625, 0.017791748046875, 0.11358642578125, -0.023468017578125, -0.0080413818359375, 0.0292510986328125, -0.0186920166015625, -0.01053619384765625, 0.0157470703125, -0.0004773139953613281, 0.056976318359375, 0.0168609619140625, 0.00232696533203125, 0.0001080632209777832, -0.00614166259765625, 0.021697998046875, 0.0005645751953125, 0.044769287109375, -0.0101318359375, -0.0250091552734375, -0.01123046875, 0.00849151611328125, -0.0021953582763671875, -0.061676025390625, -0.01305389404296875, 0.035675048828125, 0.04119873046875, -0.021759033203125, 0.006439208984375, -0.029632568359375, 0.032196044921875, -0.05499267578125, -0.002849578857421875, -0.051666259765625, 0.01088714599609375, -0.01200103759765625, -0.00507354736328125]]\n"
     ]
    }
   ],
   "source": [
    "embedded_query = embed_with_triton([\"query: deep learning\"])\n",
    "print(embedded_query)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A list containing one string (in this case, a query) produces a list containing one 1024-dimensional vector of floating-point numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1024\n"
     ]
    }
   ],
   "source": [
    "print(len(embedded_query))\n",
    "print(len(embedded_query[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Redis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our embedding model set up, let's look at [Redis](https://redis.io/), an open-source database (among other things).\n",
    "\n",
    "We chose Redis as our database for a number of reasons.\n",
    "1. Redis is extremely fast, and we need to minimize latency for the operations it will be performing.\n",
    "2. Redis is well-supported and easy to deploy through a ready-to-go container.\n",
    "3. Redis supports both vector and keyword search: vector search through the relatively recent [RedisVL](https://github.com/RedisVentures/redisvl) project, and a fairly robust suite of [search and query features](https://redis.io/docs/interact/search-and-query/) for more traditional keyword search. Notably, Redis supports BM25, the default algorithm behind the popular Elasticsearch system--making it easy to transition smoothly between the two systems.\n",
    "4. Redis unifies our vector database with our document (and metadata) database, so we don't have to worry about maintaining keys in a separate index like [FAISS](https://faiss.ai/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Viewing the Redis Service"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You launched the Redis in Lesson 00."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36mredis-1  | \u001b[0m9:C 09 Aug 2024 23:49:37.779 # WARNING Memory overcommit must be enabled! Without it, a background save or replication may fail under low memory condition. Being disabled, it can also cause failures without low memory condition, see https://github.com/jemalloc/jemalloc/issues/1328. To fix this issue add 'vm.overcommit_memory = 1' to /etc/sysctl.conf and then reboot or run the command 'sysctl vm.overcommit_memory=1' for this to take effect.\n",
      "\u001b[36mredis-1  | \u001b[0m9:C 09 Aug 2024 23:49:37.779 * oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo\n",
      "\u001b[36mredis-1  | \u001b[0m9:C 09 Aug 2024 23:49:37.779 * Redis version=7.2.4, bits=64, commit=00000000, modified=0, pid=9, just started\n",
      "\u001b[36mredis-1  | \u001b[0m9:C 09 Aug 2024 23:49:37.779 * Configuration loaded\n",
      "\u001b[36mredis-1  | \u001b[0m9:M 09 Aug 2024 23:49:37.780 * monotonic clock: POSIX clock_gettime\n",
      "\u001b[36mredis-1  | \u001b[0m9:M 09 Aug 2024 23:49:37.780 * Running mode=standalone, port=6379.\n",
      "\u001b[36mredis-1  | \u001b[0m9:M 09 Aug 2024 23:49:37.780 * Module 'RedisCompat' loaded from /opt/redis-stack/lib/rediscompat.so\n",
      "\u001b[36mredis-1  | \u001b[0m9:M 09 Aug 2024 23:49:37.781 * <search> Redis version found by RedisSearch : 7.2.4 - oss\n",
      "\u001b[36mredis-1  | \u001b[0m9:M 09 Aug 2024 23:49:37.781 * <search> RediSearch version 2.8.13 (Git=2.8-c1535c6)\n",
      "\u001b[36mredis-1  | \u001b[0m9:M 09 Aug 2024 23:49:37.781 * <search> Low level api version 1 initialized successfully\n",
      "\u001b[36mredis-1  | \u001b[0m9:M 09 Aug 2024 23:49:37.781 * <search> concurrent writes: OFF, gc: ON, prefix min length: 2, prefix max expansions: 200, query timeout (ms): 500, timeout policy: return, cursor read size: 1000, cursor max idle (ms): 300000, max doctable size: 1000000, max number of search results:  10000, search pool size: 20, index pool size: 8, \n",
      "\u001b[36mredis-1  | \u001b[0m9:M 09 Aug 2024 23:49:37.782 * <search> Initialized thread pools!\n",
      "\u001b[36mredis-1  | \u001b[0m9:M 09 Aug 2024 23:49:37.782 * <search> Enabled role change notification\n",
      "\u001b[36mredis-1  | \u001b[0m9:M 09 Aug 2024 23:49:37.782 * Module 'search' loaded from /opt/redis-stack/lib/redisearch.so\n",
      "\u001b[36mredis-1  | \u001b[0m9:M 09 Aug 2024 23:49:37.783 * <timeseries> RedisTimeSeries version 11012, git_sha=81d5ef59cdaba059a0eb9705c3c4b0127949f388\n",
      "\u001b[36mredis-1  | \u001b[0m9:M 09 Aug 2024 23:49:37.783 * <timeseries> Redis version found by RedisTimeSeries : 7.2.4 - oss\n",
      "\u001b[36mredis-1  | \u001b[0m9:M 09 Aug 2024 23:49:37.783 * <timeseries> loaded default CHUNK_SIZE_BYTES policy: 4096\n",
      "\u001b[36mredis-1  | \u001b[0m9:M 09 Aug 2024 23:49:37.783 * <timeseries> loaded server DUPLICATE_POLICY: block\n",
      "\u001b[36mredis-1  | \u001b[0m9:M 09 Aug 2024 23:49:37.783 * <timeseries> Setting default series ENCODING to: compressed\n",
      "\u001b[36mredis-1  | \u001b[0m9:M 09 Aug 2024 23:49:37.783 * <timeseries> Detected redis oss\n",
      "\u001b[36mredis-1  | \u001b[0m9:M 09 Aug 2024 23:49:37.783 * Module 'timeseries' loaded from /opt/redis-stack/lib/redistimeseries.so\n",
      "\u001b[36mredis-1  | \u001b[0m9:M 09 Aug 2024 23:49:37.783 * <ReJSON> Created new data type 'ReJSON-RL'\n",
      "\u001b[36mredis-1  | \u001b[0m9:M 09 Aug 2024 23:49:37.783 * <ReJSON> version: 20610 git sha: unknown branch: unknown\n",
      "\u001b[36mredis-1  | \u001b[0m9:M 09 Aug 2024 23:49:37.783 * <ReJSON> Exported RedisJSON_V1 API\n",
      "\u001b[36mredis-1  | \u001b[0m9:M 09 Aug 2024 23:49:37.783 * <ReJSON> Exported RedisJSON_V2 API\n",
      "\u001b[36mredis-1  | \u001b[0m9:M 09 Aug 2024 23:49:37.783 * <ReJSON> Exported RedisJSON_V3 API\n",
      "\u001b[36mredis-1  | \u001b[0m9:M 09 Aug 2024 23:49:37.783 * <ReJSON> Exported RedisJSON_V4 API\n",
      "\u001b[36mredis-1  | \u001b[0m9:M 09 Aug 2024 23:49:37.783 * <ReJSON> Exported RedisJSON_V5 API\n",
      "\u001b[36mredis-1  | \u001b[0m9:M 09 Aug 2024 23:49:37.783 * <ReJSON> Enabled diskless replication\n",
      "\u001b[36mredis-1  | \u001b[0m9:M 09 Aug 2024 23:49:37.783 * Module 'ReJSON' loaded from /opt/redis-stack/lib/rejson.so\n",
      "\u001b[36mredis-1  | \u001b[0m9:M 09 Aug 2024 23:49:37.783 * <search> Acquired RedisJSON_V5 API\n",
      "\u001b[36mredis-1  | \u001b[0m9:M 09 Aug 2024 23:49:37.783 * <bf> RedisBloom version 2.6.12 (Git=unknown)\n",
      "\u001b[36mredis-1  | \u001b[0m9:M 09 Aug 2024 23:49:37.783 * Module 'bf' loaded from /opt/redis-stack/lib/redisbloom.so\n",
      "\u001b[36mredis-1  | \u001b[0m9:M 09 Aug 2024 23:49:37.784 * <redisgears_2> Created new data type 'GearsType'\n",
      "\u001b[36mredis-1  | \u001b[0m9:M 09 Aug 2024 23:49:37.784 * <redisgears_2> Detected redis oss\n",
      "\u001b[36mredis-1  | \u001b[0m9:M 09 Aug 2024 23:49:37.784 # <redisgears_2> could not initialize RedisAI_InitError\n",
      "\u001b[36mredis-1  | \u001b[0m\n",
      "\u001b[36mredis-1  | \u001b[0m9:M 09 Aug 2024 23:49:37.784 * <redisgears_2> Failed loading RedisAI API.\n",
      "\u001b[36mredis-1  | \u001b[0m9:M 09 Aug 2024 23:49:37.784 * <redisgears_2> RedisGears v2.0.20, sha='9b737886bf825fe29ddc2f8da81f73cbe0b4e858', build_type='release', built_for='Linux-ubuntu22.04.x86_64', redis_version:'7.2.4', enterprise:'false'.\n",
      "\u001b[36mredis-1  | \u001b[0m9:M 09 Aug 2024 23:49:37.785 * <redisgears_2> Registered backend: js.\n",
      "\u001b[36mredis-1  | \u001b[0m9:M 09 Aug 2024 23:49:37.785 * Module 'redisgears_2' loaded from /opt/redis-stack/lib/redisgears.so\n",
      "\u001b[36mredis-1  | \u001b[0m9:M 09 Aug 2024 23:49:37.785 * Server initialized\n",
      "\u001b[36mredis-1  | \u001b[0m9:M 09 Aug 2024 23:49:37.785 * <search> Loading event starts\n",
      "\u001b[36mredis-1  | \u001b[0m9:M 09 Aug 2024 23:49:37.785 * <redisgears_2> Got a loading start event, clear the entire functions data.\n",
      "\u001b[36mredis-1  | \u001b[0m9:M 09 Aug 2024 23:49:37.785 * Loading RDB produced by version 7.2.4\n",
      "\u001b[36mredis-1  | \u001b[0m9:M 09 Aug 2024 23:49:37.785 * RDB age 12340112 seconds\n",
      "\u001b[36mredis-1  | \u001b[0m9:M 09 Aug 2024 23:49:37.785 * RDB memory usage when created 48.67 Mb\n",
      "\u001b[36mredis-1  | \u001b[0m9:M 09 Aug 2024 23:49:37.785 # <search> creating vector index. Server memory limit: 464931864576B, required memory: 4273304B, available memory: 464930649728B\n",
      "\u001b[36mredis-1  | \u001b[0m9:M 09 Aug 2024 23:49:37.786 # <search> creating vector index. Server memory limit: 464931864576B, required memory: 4273304B, available memory: 464930417048B\n",
      "\u001b[36mredis-1  | \u001b[0m9:M 09 Aug 2024 23:49:38.492 * Done loading RDB, keys loaded: 1091, keys expired: 0.\n",
      "\u001b[36mredis-1  | \u001b[0m9:M 09 Aug 2024 23:49:38.492 # <search> Skip background reindex scan, redis version contains loaded event.\n",
      "\u001b[36mredis-1  | \u001b[0m9:M 09 Aug 2024 23:49:38.492 * <search> Loading event ends\n",
      "\u001b[36mredis-1  | \u001b[0m9:M 09 Aug 2024 23:49:38.492 * <redisgears_2> Loading finished, re-enable key space notificaitons.\n",
      "\u001b[36mredis-1  | \u001b[0m9:M 09 Aug 2024 23:49:38.492 * DB loaded from disk: 0.707 seconds\n",
      "\u001b[36mredis-1  | \u001b[0m9:M 09 Aug 2024 23:49:38.492 * Ready to accept connections tcp\n"
     ]
    }
   ],
   "source": [
    "!docker-compose logs redis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Router Service"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are also going to use a `router` service whose job is to serve as an API entry point and route calls between the other three components: `chunking`, `triton`, and `redis`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Viewing the Router Service"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You already launched the `router` service in Lesson 00."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36mrouter-1  | \u001b[0mINFO:     Started server process [1]\n",
      "\u001b[36mrouter-1  | \u001b[0mINFO:     Waiting for application startup.\n",
      "\u001b[36mrouter-1  | \u001b[0mINFO:     Application startup complete.\n",
      "\u001b[36mrouter-1  | \u001b[0mINFO:     Uvicorn running on http://0.0.0.0:5006 (Press CTRL+C to quit)\n"
     ]
    }
   ],
   "source": [
    "!docker-compose logs router"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Viewing the Router Service"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like the `chunking` service, `router` is also a FastAPI web application, with automatic documentation generation. Inside the `router` application, we use LangChain, which makes it easy to experiment by swapping out components like LLMs and prompts.\n",
    "\n",
    "The `router` service is available on port 5006. Execute the following cell to generate a link to open it in a new browser tab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "var host = window.location.host;\n",
       "var url = 'http://'+host+':5006';\n",
       "element.innerHTML = '<a style=\"color:green;\" target=\"_blank\" href='+url+'>Click to open router service API docs.</a>';\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%js\n",
    "var host = window.location.host;\n",
    "var url = 'http://'+host+':5006';\n",
    "element.innerHTML = '<a style=\"color:green;\" target=\"_blank\" href='+url+'>Click to open router service API docs.</a>';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see we have endpoints for searching as well as data insert, delete and dump endpoints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Redis Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can search, we need to fill our database with data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned previously, we'll use the asynchronous `httpx` library to load our database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the asset types the router expects\n",
    "import httpx \n",
    "import json\n",
    "\n",
    "response = httpx.get(\"http://router:5006/asset-types\")\n",
    "asset_types_json = response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"id\": \"doc:assettypes:techblogs\",\n",
      "    \"display_title\": \"TechBlog Posts\",\n",
      "    \"chunking_params\": \"{\\\"strategy\\\": \\\"heading_section_sentence\\\", \\\"code_behavior\\\": \\\"remove_code_sections\\\", \\\"chunk_min_words\\\": 250, \\\"chunk_overlap_words\\\": 50}\",\n",
      "    \"last_indexed\": \"2024-03-20T03:58:55\",\n",
      "    \"display_default\": true,\n",
      "    \"group\": \"Written Content\",\n",
      "    \"display_sort_order\": 1,\n",
      "    \"name\": \"techblogs\",\n",
      "    \"group_sort_order\": 2\n",
      "  },\n",
      "  {\n",
      "    \"id\": \"doc:assettypes:summarize_techblogs\",\n",
      "    \"display_title\": \"TechBlog Posts Summaries\",\n",
      "    \"chunking_params\": \"{\\\"strategy\\\": \\\"summarization\\\", \\\"code_behavior\\\": \\\"remove_code_sections\\\"}\",\n",
      "    \"last_indexed\": \"2024-03-20T04:01:05\",\n",
      "    \"display_default\": true,\n",
      "    \"group\": \"Written Content\",\n",
      "    \"display_sort_order\": 2,\n",
      "    \"name\": \"summarize_techblogs\",\n",
      "    \"group_sort_order\": 2\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps(asset_types_json, indent=2))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first upload the chunks we got by breaking each article into sentence groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.path.join(os.getcwd(), 'data', 'techblogs')\n",
    "file_list = [x for x in sorted(os.listdir(data_dir)) if \".json\" in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "payloads = []\n",
    "\n",
    "for i, filename in enumerate(file_list):\n",
    "    with open(os.path.join(data_dir, filename), \"r\") as in_file:\n",
    "        data = json.load(in_file)\n",
    "    for item in data:\n",
    "        # skip items that do not link to developer.nvidia.com/blog or blogs.nvidia.com\n",
    "        if not item[\"link\"].startswith(\n",
    "            \"https://developer.nvidia.com/blog\"\n",
    "        ):  # and not item['link'].startswith(\"https://blogs.nvidia.com\"):\n",
    "            # print(f\"Skipping URL {item['link']}\")\n",
    "            continue\n",
    "        document_title = item[\"title\"][\"rendered\"]\n",
    "        document_url = item[\"link\"]\n",
    "        document_html = item[\"content\"][\"rendered\"]\n",
    "        document_date = item[\"date_gmt\"]\n",
    "        document_date_modified = item[\"modified_gmt\"]\n",
    "        payloads.append(\n",
    "            {\n",
    "                \"strategy\": \"heading_section_sentence\",\n",
    "                \"code_behavior\": \"remove_code_sections\",\n",
    "                \"chunk_min_words\": 250,\n",
    "                \"chunk_overlap_words\": 50,\n",
    "                \"input_type\": \"html\",\n",
    "                \"input_str\": document_html,\n",
    "                \"additional_metadata\": {\n",
    "                    \"document_title\": document_title,\n",
    "                    \"document_url\": document_url,\n",
    "                    \"document_date\": document_date,\n",
    "                    \"document_date_modified\": document_date_modified,\n",
    "                },\n",
    "            }\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total num payloads: 150\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total num payloads: {len(payloads)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'strategy': 'heading_section_sentence',\n",
       " 'code_behavior': 'remove_code_sections',\n",
       " 'chunk_min_words': 250,\n",
       " 'chunk_overlap_words': 50,\n",
       " 'input_type': 'html',\n",
       " 'input_str': '<div style=\"margin-top: 0px; margin-bottom: 0px;\" class=\"sharethis-inline-share-buttons\" ></div>\\n<p><a href=\"https://www.nvidia.com/en-us/deep-learning-ai/solutions/data-science/workbench/\">NVIDIA AI Workbench</a> is now in beta, bringing a wealth of new features to streamline how enterprise developers create, use, and share AI and machine learning (ML) projects. Announced at SIGGRAPH 2023, NVIDIA AI Workbench enables developers to create, collaborate, and migrate AI workloads on their GPU-enabled environment of choice. To learn more, see <a href=\"https://developer.nvidia.com/blog/develop-and-deploy-scalable-generative-ai-models-seamlessly-with-nvidia-ai-workbench/\">Develop and Deploy Scalable Generative AI Models Seamlessly with NVIDIA AI Workbench</a>.</p>\\n\\n\\n\\n<p>This post explains how NVIDIA AI Workbench helps streamline the AI workflow and details new features of the beta release. It also walks through a coding copilot reference example, which enables you to use AI Workbench to create, test, and customize a pretrained generative AI model on your platform of choice.</p>\\n\\n\\n\\n<h2 class=\"wp-block-heading\">What is NVIDIA AI Workbench?</h2>\\n\\n\\n\\n<p>With AI Workbench, developers and data scientists have the flexibility to start an AI or ML project locally on a PC or workstation and then migrate it anywhere. Projects can be pushed out to a data center, public cloud, or <a href=\"https://www.nvidia.com/en-us/data-center/dgx-cloud/\">NVIDIA DGX Cloud</a>, or moved to a local RTX PC or workstation for inference and lightweight customization, depending on project requirements.</p>\\n\\n\\n\\n<p>AI Workbench helps developers simplify and shorten setup, development, and migration for AI workflows by providing the ability to work on their choice of heterogeneous compute resources. Benefits include:</p>\\n\\n\\n\\n<ul>\\n<li>Free and quick install on the system of choice with an intuitive UX or CLI for project creation and management.</li>\\n\\n\\n\\n<li>Streamlined configuration for compute resources and runtimes, providing reproducibility and flexibility to work on different GPU resources.</li>\\n\\n\\n\\n<li>Simplified version control and management for containers and Git repositories and integrations with GitHub, GitLab, and the <a href=\"https://catalog.ngc.nvidia.com/containers?filters=&amp;orderBy=scoreDESC&amp;query=workbench\">NVIDIA NGC catalog</a>.</li>\\n\\n\\n\\n<li>Automation and streamlining to handle Git and container-based developer environments, enabling users to work on their choice of system, laptop, workstation, server, or the cloud.</li>\\n\\n\\n\\n<li>Reproducibility across users and systems with transparent handling for idiosyncrasies like credentials, secrets, and file system changes without the overhead.</li>\\n\\n\\n\\n<li>Scalable creation and distribution of complex workflows and applications for generative AI, GPU-enabled ML, and data science.</li>\\n</ul>\\n\\n\\n\\n<h2 class=\"wp-block-heading\">What’s new in the beta release</h2>\\n\\n\\n\\n<p>The AI Workbench beta release includes the following exciting new features, with updates to the user interface and expanded support for container runtimes and Git servers.</p>\\n\\n\\n\\n<p>Simplified setup and installation<strong> </strong>on Windows 11, Ubuntu, 22.04, and macOS 11 or higher.&nbsp;</p>\\n\\n\\n\\n<ul>\\n<li>Install AI Workbench quickly in two ways: click-through install using the desktop app on local systems or command-line install on remote systems.</li>\\n\\n\\n\\n<li>Work from anywhere with support for the three major operating systems for a uniform experience. AI Workbench runs on Windows distributions that support WSL2, Ubuntu 22.04, and macOS version 11 and higher.</li>\\n</ul>\\n\\n\\n\\n<p>Simplified version control and streamlined development with containerized environments.</p>\\n\\n\\n\\n<ul>\\n<li>Access simple and comprehensive Git-compliant version control with both the Desktop App and CLI. Push, pull, and fetch features are now included.&nbsp;</li>\\n\\n\\n\\n<li>Create a containerized JupyterLab environment with isolation and reproducibility without having to handle details.</li>\\n\\n\\n\\n<li>Choose from two container runtime options: Docker or Podman.</li>\\n</ul>\\n\\n\\n\\n<p>Expanded feature parity between the user interface and the CLI.</p>\\n\\n\\n\\n<ul>\\n<li>See commit history and summaries directly in the Desktop App.</li>\\n\\n\\n\\n<li>View improved container state and application status notifications in the Desktop App.</li>\\n</ul>\\n\\n\\n\\n<p>Expanded default base images.</p>\\n\\n\\n\\n<ul>\\n<li>Access three new base images for project creation, in addition to the <a href=\"https://catalog.ngc.nvidia.com/orgs/nvidia/teams/ai-workbench/containers/python-basic\">Python Basic</a> and <a href=\"https://catalog.ngc.nvidia.com/orgs/nvidia/teams/ai-workbench/containers/pytorch\">PyTorch Basic</a> images already in the NGC catalog. New base images for CUDA 11.0, CUDA 12.0, and CUDA 12.2 provide the foundation for further customization.</li>\\n</ul>\\n\\n\\n\\n<p>Three new example projects for reference.</p>\\n\\n\\n\\n<ul>\\n<li><a href=\"https://github.com/NVIDIA/workbench-example-mistral-finetune\">Mistral</a>: Fine-tune a Mistral 7B large language model (LLM) on a custom code instructions dataset using QLoRA PEFT.</li>\\n\\n\\n\\n<li><a href=\"https://github.com/NVIDIA/workbench-example-local-rag\">RAG</a>: Converse with your data using a local, user-friendly developer workflow for <a href=\"https://blogs.nvidia.com/blog/what-is-retrieval-augmented-generation/\">retrieval-augmented generation</a> (RAG).</li>\\n\\n\\n\\n<li><a href=\"https://github.com/NVIDIA/workbench-example-nemotron-finetune\">NeMotron-3</a>: Fine-tune a Nemotron-3 8B LLM on a custom QA dataset using <a href=\"https://www.nvidia.com/en-us/ai-data-science/generative-ai/nemo-framework/\">NVIDIA NeMo</a>.</li>\\n</ul>\\n\\n\\n\\n<h2 class=\"wp-block-heading\">Create your own coding copilot</h2>\\n\\n\\n\\n<p>This section walks through an example of how AI Workbench can significantly simplify the process of using and fine-tuning a generative AI model on a GPU system of the user’s choice.&nbsp;</p>\\n\\n\\n\\n<h3 class=\"wp-block-heading\">Key concepts</h3>\\n\\n\\n\\n<p>A few key concepts used in this example are outlined below.</p>\\n\\n\\n\\n<h4 class=\"wp-block-heading\">AI Workbench Project</h4>\\n\\n\\n\\n<p>An AI Workbench Project is a Git repository that contains a set of configuration files that can be read by AI Workbench to automate the creation and management of a containerized development environment. A project references everything needed for a configured, containerized development environment and includes:&nbsp;</p>\\n\\n\\n\\n<ul>\\n<li>Code, data, and models</li>\\n\\n\\n\\n<li>Simple configuration files that drive AI Workbench automation for container customization and package installation</li>\\n\\n\\n\\n<li>A project specification metadata file to wrap the repository in a way that&#8217;s compatible with AI Workbench&nbsp;</li>\\n</ul>\\n\\n\\n\\n<p>Visit <a href=\"https://github.com/nvidia?q=workbench&amp;type=all&amp;language=&amp;sort=\">NVIDIA on GitHub</a> to reference NVIDIA projects that provide starting points for adapting your own data and use cases. Additionally, <a href=\"https://developer.nvidia.com/ai-workbench-early-access/join\">AI Workbench early access</a> members can contribute and use third-party <a href=\"https://developer.nvidia.com/ai-workbench-early-access/members\">community project examples</a>.</p>\\n\\n\\n\\n<p>The <a href=\"https://github.com/NVIDIA/workbench-example-mistral-finetune\">Mistral 7B fine-tuning reference project</a> showcased in this post highlights how to leverage the power of AI Workbench to build a basic coding copilot on a system of your choice.&nbsp;</p>\\n\\n\\n\\n<h4 class=\"wp-block-heading\">Fine-tuning</h4>\\n\\n\\n\\n<p>While Mistral 7B is a strong baseline for multiple downstream tasks, it can lack domain-specific knowledge based on proprietary or otherwise sensitive information. Fine-tuning is used to improve the model’s responses in these cases.&nbsp;</p>\\n\\n\\n\\n<p>There are two versions of fine-tuning. The first, <em>full fine-tuning</em>, uses the new data to update all of the model weights. This can improve domain-specific results but often requires more time and larger, more expensive GPUs. The second, <em>parameter efficient fine-tuning (PEFT)</em>, is a family of techniques that update a subset of the model weights. PEFT is often preferable to full fine-tuning because it produces comparable results in far less time and with smaller, less expensive GPUs.</p>\\n\\n\\n\\n<p>This example focuses primarily on the Quantized Low Rank Adaptation (QLoRA) method of PEFT. Low Rank Adaptation (LoRA) is a method of PEFT that uses smaller weight matrices in the retraining as approximations instead of updating the full weight matrix. This rank decomposition optimization technique enables greater memory efficiency and can reduce the GPU size required for successful fine-tuning.&nbsp;</p>\\n\\n\\n\\n<p>QLoRA<strong> </strong>is a further optimization that reduces the precision of model weights to provide even greater advances in memory and space efficiency. The most common quantization used for this LoRA fine-tuning workflow is 4-bit quantization, which provides a decent balance between model performance and fine-tuning feasibility.</p>\\n\\n\\n\\n<h3 class=\"wp-block-heading\">Walkthrough of Mistral 7B fine-tuning project in NVIDIA AI Workbench</h3>\\n\\n\\n\\n<p>This walkthrough includes high-level code and details. For more information, see the full <a href=\"https://github.com/NVIDIA/workbench-example-mistral-finetune\">Mistral 7B fine-tuning reference project</a> on GitHub. The project fine-tunes the Mistral 7B base model on the <a href=\"https://huggingface.co/datasets/TokenBender/code_instructions_122k_alpaca_style\">TokenBender code instructions dataset</a>, consisting of 122K Alpaca-style code instructions and code solutions.&nbsp;</p>\\n\\n\\n<div class=\"wp-block-image\">\\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"1480\" height=\"955\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/mistral-7b-fine-tuning-project-nvidia-ai-workbench.png\" alt=\"Screenshot of the Mistral 7B fine-tuning project in the NVIDIA AI Workbench user interface.\" class=\"wp-image-76708\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/mistral-7b-fine-tuning-project-nvidia-ai-workbench.png 1480w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/mistral-7b-fine-tuning-project-nvidia-ai-workbench-300x194.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/mistral-7b-fine-tuning-project-nvidia-ai-workbench-625x403.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/mistral-7b-fine-tuning-project-nvidia-ai-workbench-179x115.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/mistral-7b-fine-tuning-project-nvidia-ai-workbench-768x496.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/mistral-7b-fine-tuning-project-nvidia-ai-workbench-645x416.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/mistral-7b-fine-tuning-project-nvidia-ai-workbench-465x300.png 465w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/mistral-7b-fine-tuning-project-nvidia-ai-workbench-139x90.png 139w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/mistral-7b-fine-tuning-project-nvidia-ai-workbench-362x234.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/mistral-7b-fine-tuning-project-nvidia-ai-workbench-170x110.png 170w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/mistral-7b-fine-tuning-project-nvidia-ai-workbench-1024x661.png 1024w\" sizes=\"(max-width: 1480px) 100vw, 1480px\" /><figcaption class=\"wp-element-caption\"><em>Figure 1. Building the Mistral 7B fine-tuning project in NVIDIA AI Workbench</em></figcaption></figure></div>\\n\\n\\n<p>First, download the data and split it into 80% training, 10% validation, and 10% testing datasets. One entry of an instruction in the dataset is shown below as an example:</p>\\n\\n\\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: plain; title: ; notranslate\" title=\"\">\\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.\\n### Instruction: Output the maximum element in an array. ### Input: &#91;1, 5, 10, 8, 15] ### Output: 15\\n</pre></div>\\n\\n\\n<p>Next, download the Mistral 7B model weights to this project:</p>\\n\\n\\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: plain; title: ; notranslate\" title=\"\">\\nmodel_id = &quot;mistralai/Mistral-7B-v0.1&quot;\\nbb_config = BitsAndBytesConfig(\\n    load_in_4bit=True,\\n    bnb_4bit_use_double_quant=True,\\n    bnb_4bit_quant_type=&quot;nf4&quot;,\\n)\\n\\nmodel = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bb_config)\\n</pre></div>\\n\\n\\n<p>Notice that the 4-bit quantization configuration is specified for the base model.&nbsp;</p>\\n\\n\\n\\n<p>Next, evaluate the performance of the base model on a specific sample programming question. This establishes a baseline for comparison between the base model and the final, fine-tuned model.&nbsp;</p>\\n\\n\\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: plain; title: ; notranslate\" title=\"\">\\nbase_prompt = &quot;&quot;&quot;Write a function to output the prime factorization of 2023 in python, C, and C++&quot;&quot;&quot;\\n\\nbase_tokenizer = AutoTokenizer.from_pretrained(\\n    model_id,\\n    add_bos_token=True,\\n)\\n\\nmodel_input = base_tokenizer(base_prompt, return_tensors=&quot;pt&quot;).to(&quot;cuda&quot;)\\n\\nmodel.eval()\\nwith torch.no_grad():\\n    print(base_tokenizer.decode(model.generate(**model_input, max_new_tokens=256)&#91;0], skip_special_tokens=True))\\n\\n***** Output ***** \\n\\n## Prime Factorization of 2023\\n\\nThe prime factorization of 2023 is 13 x 157.\\n\\n## Prime Factorization of 2023 in Python\\n\\nThe prime factorization of 2023 in python is given below.\\n\\ndef prime_factorization(n):\\n    factors = &#91;]\\n    for i in range(2, n + 1):\\n        if n % i == 0:\\n            factors.append(i)\\n    return factors\\n\\nprint(prime_factorization(2023))\\n\\n...\\n</pre></div>\\n\\n\\n<p>Notice that the base model doesn&#8217;t perform well out of the box. First, the base model seems to think the prime factorization of 2,023 is 13 x 157. This amounts to 2041. The actual answer is 7 x 17 x 17.</p>\\n\\n\\n\\n<p>Second, the Python function the model outputs is incorrect as well. Running the suggested code gives an answer of [7, 17, 119, 289, 2,023] when in fact 119, 289, and 2,023 are not prime factors.</p>\\n\\n\\n\\n<p>Fine-tuning is necessary to improve model performance. Begin with preprocessing the dataset by reformatting the dataset entries to better fit the instruction prompt [INST] for fine-tuning. Then tokenize each of these prompts.&nbsp;</p>\\n\\n\\n\\n<p>Next, specify the configuration for QLoRA fine-tuning and perform the fine-tuning. By default, the fine-tuning takes 1,000 iterations, with checkpointing and evaluation every 50 steps. These hyperparameters can be adjusted depending on hardware resource constraints. On an <a href=\"https://www.nvidia.com/en-us/data-center/a100/\">NVIDIA A100</a> 80 GB GPU system, this configuration can take about 6.5 hours.&nbsp;</p>\\n\\n\\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: plain; title: ; notranslate\" title=\"\">\\nconfig = LoraConfig(\\n    r=8,\\n    lora_alpha=16,\\n    target_modules=&#91;\\n        &quot;q_proj&quot;,\\n        &quot;k_proj&quot;,\\n        &quot;v_proj&quot;,\\n        &quot;o_proj&quot;,\\n        &quot;gate_proj&quot;,\\n        &quot;up_proj&quot;,\\n        &quot;down_proj&quot;,\\n        &quot;lm_head&quot;,\\n    ],\\n    bias=&quot;none&quot;,\\n    lora_dropout=0.05,\\n    task_type=&quot;CAUSAL_LM&quot;,\\n)\\n\\nmodel = get_peft_model(model, config)\\n\\n# Training configs\\ntrainer = transformers.Trainer(\\n    model=model,\\n    train_dataset=tokenized_train_ds,\\n    eval_dataset=tokenized_val_ds,\\n    args=transformers.TrainingArguments(\\n        output_dir=&quot;./mistral-code-instruct&quot;,\\n        warmup_steps=5,\\n        per_device_train_batch_size=2,\\n        gradient_checkpointing=True,\\n        gradient_accumulation_steps=4,\\n        max_steps=1000,\\n        learning_rate=2.5e-5,\\n        logging_steps=50,\\n        bf16=True,\\n        optim=&quot;paged_adamw_8bit&quot;,\\n        logging_dir=&quot;./logs&quot;,\\n        save_strategy=&quot;steps&quot;,\\n        save_steps=50,\\n        evaluation_strategy=&quot;steps&quot;, \\n        eval_steps=50,\\n        report_to=&quot;none&quot;,\\n        do_eval=True,\\n    ),\\n    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\\n)\\n\\n# Train! \\ntrainer.train()\\n</pre></div>\\n\\n\\n<p>Using the final fine-tuning checkpoint, define the updated Mistral 7B model:</p>\\n\\n\\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: plain; title: ; notranslate\" title=\"\">\\nft_model = PeftModel.from_pretrained(base_model, &quot;mistral-code-instruct/checkpoint-1000&quot;)\\n</pre></div>\\n\\n\\n<p>To evaluate the fine-tuned model’s performance, ask a coding question similar to the initial one and request the generation of a code snippet:</p>\\n\\n\\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: plain; title: ; notranslate\" title=\"\">\\neval_prompt = f&quot;&quot;&quot;\\nFor a given integer n, print out all its prime factors one on each line. \\nn = 30\\n&quot;&quot;&quot;\\n\\ninput_ids = tokenizer(eval_prompt, return_tensors=&quot;pt&quot;, truncation=True).input_ids.cuda()\\noutputs = ft_model.generate(input_ids=input_ids, max_new_tokens=256, do_sample=True, top_p=0.9,temperature=0.5)\\n\\n***** Output ***** \\n\\nGenerated response:\\n2\\n3\\n5 \\n\\n#include &lt;stdio.h&gt;\\n\\nint main() {\\n    int n = 30;\\n    int i;\\n    for (i = 2; i &lt;= n; i++) {\\n        while (n % i == 0) {\\n            printf(&quot;%d\\\\n&quot;, i);\\n            n /= i;\\n        }\\n    }\\n    return 0;\\n}\\n\\n...\\n</pre></div>\\n\\n\\n<p>The generated code snippet response from the fine-tuned model looks much better. Use a sandbox environment to try the code for yourself.</p>\\n\\n\\n\\n<p>That’s all there is to fine-tuning the Mistral 7B LLM. This project provides a reference workflow for your development needs. You can always choose to customize the project to better suit your enterprise data or use case. Switch out the dataset with one of your own, or fine-tune the model to another use case, such as text summarization or question-answering.&nbsp;</p>\\n\\n\\n\\n<h2 class=\"wp-block-heading\">Get started with AI Workbench</h2>\\n\\n\\n\\n<p>NVIDIA AI Workbench helps you create, share, and scale enterprise AI and ML workflows between different GPU-enabled environments. <a href=\"https://developer.nvidia.com/ai-workbench-early-access\">Sign up for beta access to NVIDIA AI Workbench</a>. To learn more about AI Workbench, check out these resources:</p>\\n\\n\\n\\n<ul>\\n<li>Watch a <a href=\"https://youtu.be/ntMRzPzSvM4?feature=shared\">video demo</a> of AI Workbench that walks through a custom image generation example project with Stable Diffusion XL.&nbsp;</li>\\n\\n\\n\\n<li>Read <a href=\"https://developer.nvidia.com/blog/develop-and-deploy-scalable-generative-ai-models-seamlessly-with-nvidia-ai-workbench/\">Develop and Deploy Scalable Generative AI Models Seamlessly with NVIDIA AI Workbench</a> to get a sneak peek of more example projects.</li>\\n\\n\\n\\n<li>Reference the <a href=\"https://docs.nvidia.com/ai-workbench/\">NVIDIA AI Workbench User Guide</a> to get your AI and ML projects up and running with NVIDIA AI Workbench.</li>\\n</ul>\\n',\n",
       " 'additional_metadata': {'document_title': 'Create, Share, and Scale Enterprise AI Workflows with NVIDIA AI Workbench, Now in Beta',\n",
       "  'document_url': 'https://developer.nvidia.com/blog/create-share-and-scale-enterprise-ai-workflows-with-nvidia-ai-workbench-now-in-beta/',\n",
       "  'document_date': '2024-01-30T20:02:55',\n",
       "  'document_date_modified': '2024-01-31T01:06:02'}}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "payloads[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import httpx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunking_url = \"http://chunking:5005/api/chunking\"\n",
    "existing_items_url = \"http://router:5006/search/keyword\"\n",
    "delete_url = \"http://router:5006/data/delete\"\n",
    "insert_url = \"http://router:5006/data/insert\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a semaphore object with a limit of 3.\n",
    "limit = asyncio.Semaphore(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chunk up an article\n",
    "async def chunking_request(client: httpx.AsyncClient, payload: dict):\n",
    "    chunking_resp = await client.post(chunking_url, json=payload, timeout=15)\n",
    "    return chunking_resp.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see if any chunks already exist in the db that match this document url\n",
    "async def get_existing_items_request(client: httpx.AsyncClient, payload: dict, asset_type: str):\n",
    "    existing_items_resp = await client.post(\n",
    "        existing_items_url,\n",
    "        json={\n",
    "            \"field\": \"document_url\",\n",
    "            \"value\": payload[\"additional_metadata\"][\"document_url\"],\n",
    "            \"asset_types\": [asset_type],\n",
    "            \"search_type\": \"exact\",\n",
    "            \"k\": 1000,  # some large number to ensure we don't hit default limit of 10\n",
    "        },\n",
    "        timeout=15,\n",
    "    )\n",
    "    return existing_items_resp.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete items with certain ids\n",
    "async def delete_request(client: httpx.AsyncClient, results: list, asset_type: str):\n",
    "    delete_resp = await client.post(\n",
    "        delete_url,\n",
    "        json={\n",
    "            \"asset_type\": asset_type,\n",
    "            \"ids\": [x[\"id\"] for x in results],\n",
    "        },\n",
    "        timeout=15,\n",
    "    )\n",
    "    print(delete_resp.status_code)\n",
    "    return delete_resp.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def upload_techblogs_chunks(client: httpx.AsyncClient, payload: dict):\n",
    "    async with limit:\n",
    "        try:\n",
    "            chunks = await chunking_request(client, payload)\n",
    "        except:  # retry once\n",
    "            chunks = await chunking_request(client, payload)\n",
    "        print(\n",
    "            f\"{payload['additional_metadata']['document_url']} | num chunks: {len(chunks)}\"\n",
    "        )\n",
    "\n",
    "        # gets ids of existing items with this url\n",
    "        try:\n",
    "            existing_items = await get_existing_items_request(client, payload, \"techblogs\")\n",
    "        except:  # retry once\n",
    "            existing_items = await get_existing_items_request(client, payload, \"techblogs\")\n",
    "\n",
    "        if len(existing_items) > 0:\n",
    "            results = existing_items[0][\"results\"]\n",
    "            if len(results) > 0:\n",
    "                # delete items that are associated with this url\n",
    "                try:\n",
    "                    deleted_items = await delete_request(client, results, \"techblogs\")\n",
    "                except:  # retry once\n",
    "                    deleted_items = await delete_request(client, results, \"techblogs\")\n",
    "                print(f\"Deleted ids reponse: {deleted_items}\")\n",
    "\n",
    "        # insert: send chunks to redis\n",
    "        resp = await client.post(\n",
    "            insert_url,\n",
    "            json={\n",
    "                \"asset_type\": \"techblogs\",\n",
    "                \"chunks\": chunks,\n",
    "            },\n",
    "            timeout=15,\n",
    "        )\n",
    "        print(f\"Inserted {len(resp.json())} chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def main():\n",
    "    async with httpx.AsyncClient() as client:\n",
    "        tasks = []\n",
    "        for payload in payloads:\n",
    "            tasks.append(upload_techblogs_chunks(client, payload))\n",
    "\n",
    "        await asyncio.gather(*tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://developer.nvidia.com/blog/create-share-and-scale-enterprise-ai-workflows-with-nvidia-ai-workbench-now-in-beta/ | num chunks: 7\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 7}\n",
      "Inserted 7 chunks\n",
      "https://developer.nvidia.com/blog/modernizing-the-data-center-with-accelerated-networking/ | num chunks: 4\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 4}\n",
      "Inserted 4 chunks\n",
      "https://developer.nvidia.com/blog/emulating-the-attention-mechanism-in-transformer-models-with-a-fully-convolutional-network/ | num chunks: 10\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 10}\n",
      "Inserted 10 chunks\n",
      "https://developer.nvidia.com/blog/announcing-metropolis-microservices-on-nvidia-jetson-orin-for-rapid-edge-ai-development/ | num chunks: 4\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 4}\n",
      "Inserted 4 chunks\n",
      "https://developer.nvidia.com/blog/advancing-production-ai-with-nvidia-ai-enterprise/ | num chunks: 5\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 5}\n",
      "Inserted 5 chunks\n",
      "https://developer.nvidia.com/blog/build-enterprise-grade-ai-with-nvidia-ai-software/ | num chunks: 4\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 4}\n",
      "Inserted 4 chunks\n",
      "https://developer.nvidia.com/blog/delivering-efficient-high-performance-ai-clouds-with-nvidia-doca-2-5/ | num chunks: 7\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 7}\n",
      "Inserted 7 chunks\n",
      "https://developer.nvidia.com/blog/using-the-power-of-ai-to-make-factories-safer/ | num chunks: 4\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 4}\n",
      "Inserted 4 chunks\n",
      "https://developer.nvidia.com/blog/simplifying-network-operations-for-ai-with-nvidia-quantum-infiniband/ | num chunks: 3\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 3}\n",
      "Inserted 3 chunks\n",
      "https://developer.nvidia.com/blog/how-to-build-vision-ai-applications-at-the-edge-with-nvidia-metropolis-microservices-and-apis/ | num chunks: 11\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 11}\n",
      "Inserted 11 chunks\n",
      "https://developer.nvidia.com/blog/bringing-generative-ai-to-the-edge-with-nvidia-metropolis-microservices-for-jetson/ | num chunks: 9\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 9}\n",
      "Inserted 9 chunks\n",
      "https://developer.nvidia.com/blog/model-monday-query-graphs-with-optimized-deplot-model/ | num chunks: 3\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 3}\n",
      "Inserted 3 chunks\n",
      "https://developer.nvidia.com/blog/benchmarking-camera-performance-on-your-workstation-with-nvidia-isaac-sim/ | num chunks: 4\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 4}\n",
      "Inserted 4 chunks\n",
      "https://developer.nvidia.com/blog/generate-synthetic-data-for-deep-object-pose-estimation-training-with-nvidia-isaac-ros/ | num chunks: 7\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 7}\n",
      "Inserted 7 chunks\n",
      "https://developer.nvidia.com/blog/simulating-railroads-with-openusd/ | num chunks: 6\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 6}\n",
      "Inserted 6 chunks\n",
      "https://developer.nvidia.com/blog/new-support-for-dutch-and-persian-released-by-nemo-asr/ | num chunks: 2\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 2}\n",
      "Inserted 2 chunks\n",
      "https://developer.nvidia.com/blog/robust-scene-text-detection-and-recognition-inference-optimization/ | num chunks: 6\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 6}\n",
      "Inserted 6 chunks\n",
      "https://developer.nvidia.com/blog/robust-scene-text-detection-and-recognition-implementation/ | num chunks: 4\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 4}\n",
      "Inserted 4 chunks\n",
      "https://developer.nvidia.com/blog/robust-scene-text-detection-and-recognition-introduction/ | num chunks: 6\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 6}\n",
      "Inserted 6 chunks\n",
      "https://developer.nvidia.com/blog/experience-real-time-audio-and-video-communication-with-nvidia-maxine/ | num chunks: 4\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 4}\n",
      "Inserted 4 chunks\n",
      "https://developer.nvidia.com/blog/enhancing-phone-customer-service-with-asr-customization/ | num chunks: 6\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 6}\n",
      "Inserted 6 chunks\n",
      "https://developer.nvidia.com/blog/new-models-molmim-and-diffdock-power-molecule-generation-and-molecular-docking-in-bionemo/ | num chunks: 3\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 3}\n",
      "Inserted 3 chunks\n",
      "https://developer.nvidia.com/blog/new-stable-diffusion-models-accelerated-with-nvidia-tensorrt/ | num chunks: 2\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 2}\n",
      "Inserted 2 chunks\n",
      "https://developer.nvidia.com/blog/get-started-with-generative-ai-development-for-windows-pcs-with-rtx-systems/ | num chunks: 3\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 3}\n",
      "Inserted 3 chunks\n",
      "https://developer.nvidia.com/blog/spotlight-convai-reinvents-non-playable-character-interactions/ | num chunks: 4\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 4}\n",
      "Inserted 4 chunks\n",
      "https://developer.nvidia.com/blog/supercharging-llm-applications-on-windows-pcs-with-nvidia-rtx-systems/ | num chunks: 4\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 4}\n",
      "Inserted 4 chunks\n",
      "https://developer.nvidia.com/blog/building-lifelike-digital-avatars-with-nvidia-ace-microservices/ | num chunks: 4\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 4}\n",
      "Inserted 4 chunks\n",
      "https://developer.nvidia.com/blog/improving-cuda-initialization-times-using-cgroups-in-certain-scenarios/ | num chunks: 3\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 3}\n",
      "Inserted 3 chunks\n",
      "https://developer.nvidia.com/blog/develop-ml-ai-with-metaflow-deploy-with-triton-inference-server/ | num chunks: 10\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 10}\n",
      "Inserted 10 chunks\n",
      "https://developer.nvidia.com/blog/video-encoding-at-8k60-with-split-frame-encoding-and-nvidia-ada-lovelace-architecture/ | num chunks: 8\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 8}\n",
      "Inserted 8 chunks\n",
      "https://developer.nvidia.com/blog/accelerating-inference-on-end-to-end-workflows-with-h2o-ai-and-nvidia/ | num chunks: 10\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 10}\n",
      "Inserted 10 chunks\n",
      "https://developer.nvidia.com/blog/q-and-a-looking-back-to-when-1997s-quake-2-got-a-path-tracing-update/ | num chunks: 3\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 3}\n",
      "Inserted 3 chunks\n",
      "https://developer.nvidia.com/blog/qa-real-time-ray-tracing-in-a-cinematic-scene/ | num chunks: 4\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 4}\n",
      "Inserted 4 chunks\n",
      "https://developer.nvidia.com/blog/breakthrough-in-functional-annotation-with-hifi-nn/ | num chunks: 5\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 5}\n",
      "Inserted 5 chunks\n",
      "https://developer.nvidia.com/blog/year-in-review-trending-posts-of-2023/ | num chunks: 3\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 3}\n",
      "Inserted 3 chunks\n",
      "https://developer.nvidia.com/blog/accelerate-quantum-circuit-simulation-with-nvidia-cuquantum-23-10/ | num chunks: 3\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 3}\n",
      "Inserted 3 chunks\n",
      "https://developer.nvidia.com/blog/teaching-avs-the-language-of-human-driving-behavior-with-trajeglish/ | num chunks: 4\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 4}\n",
      "Inserted 4 chunks\n",
      "https://developer.nvidia.com/blog/rag-101-retrieval-augmented-generation-questions-answered/ | num chunks: 8\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 8}\n",
      "Inserted 8 chunks\n",
      "https://developer.nvidia.com/blog/rag-101-demystifying-retrieval-augmented-generation-pipelines/ | num chunks: 4\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 4}\n",
      "Inserted 4 chunks\n",
      "https://developer.nvidia.com/blog/deploying-retrieval-augmented-generation-applications-on-nvidia-gh200-delivers-accelerated-performance/ | num chunks: 8\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 8}\n",
      "Inserted 8 chunks\n",
      "https://developer.nvidia.com/blog/streamline-etl-workflows-with-nested-data-types-in-rapids-libcudf/ | num chunks: 8\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 8}\n",
      "Inserted 8 chunks\n",
      "https://developer.nvidia.com/blog/advanced-api-performance-swap-chains/ | num chunks: 3\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 3}\n",
      "Inserted 3 chunks\n",
      "https://developer.nvidia.com/blog/achieving-top-inference-performance-with-the-nvidia-h100-tensor-core-gpu-and-nvidia-tensorrt-llm/ | num chunks: 3\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 3}\n",
      "Inserted 3 chunks\n",
      "https://developer.nvidia.com/blog/generative-ai-research-spotlight-demystifying-diffusion-based-models/ | num chunks: 19\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 19}\n",
      "Inserted 19 chunks\n",
      "https://developer.nvidia.com/blog/fast-track-computer-vision-deployments-with-nvidia-deepstream-and-edge-impulse/ | num chunks: 9\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 9}\n",
      "Inserted 9 chunks\n",
      "https://developer.nvidia.com/blog/simulate-and-localize-a-husky-robot-with-nvidia-isaac/ | num chunks: 6\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 6}\n",
      "Inserted 6 chunks\n",
      "https://developer.nvidia.com/blog/next-generation-seismic-monitoring-with-neural-operators/ | num chunks: 5\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 5}\n",
      "Inserted 5 chunks\n",
      "https://developer.nvidia.com/blog/benchmarking-quantum-computing-applications-with-bmw-group-and-nvidia-cuquantum/ | num chunks: 3\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 3}\n",
      "Inserted 3 chunks\n",
      "https://developer.nvidia.com/blog/automating-data-center-networks-with-nvidia-nvue-and-ansible/ | num chunks: 3\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 3}\n",
      "Inserted 3 chunks\n",
      "https://developer.nvidia.com/blog/nvidia-research-shows-interactive-texture-painting-with-gen-ai-at-siggraph-asia-real-time-live/ | num chunks: 4\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 4}\n",
      "Inserted 4 chunks\n",
      "https://developer.nvidia.com/blog/available-now-nvidia-ai-accelerated-dgl-and-pyg-containers-for-gnns/ | num chunks: 7\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 7}\n",
      "Inserted 7 chunks\n",
      "https://developer.nvidia.com/blog/develop-and-optimize-vision-ai-models-for-trillions-of-devices-with-nvidia-tao/ | num chunks: 2\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 2}\n",
      "Inserted 2 chunks\n",
      "https://developer.nvidia.com/blog/reconstructing-dynamic-driving-scenarios-using-self-supervised-learning/ | num chunks: 5\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 5}\n",
      "Inserted 5 chunks\n",
      "https://developer.nvidia.com/blog/nvidia-tensorrt-llm-enhancements-deliver-massive-large-language-model-speedups-on-nvidia-h200/ | num chunks: 4\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 4}\n",
      "Inserted 4 chunks\n",
      "https://developer.nvidia.com/blog/create-lifelike-avatars-with-ai-animation-and-speech-features-in-nvidia-ace/ | num chunks: 2\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 2}\n",
      "Inserted 2 chunks\n",
      "https://developer.nvidia.com/blog/new-nvidia-nemo-framework-features-and-nvidia-h200-supercharge-llm-training-performance-and-versatility/ | num chunks: 7\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 7}\n",
      "Inserted 7 chunks\n",
      "https://developer.nvidia.com/blog/building-your-first-llm-agent-application/ | num chunks: 7\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 7}\n",
      "Inserted 7 chunks\n",
      "https://developer.nvidia.com/blog/introduction-to-llm-agents/ | num chunks: 7\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 7}\n",
      "Inserted 7 chunks\n",
      "https://developer.nvidia.com/blog/boost-meeting-productivity-with-ai-powered-note-taking-and-summarization/ | num chunks: 4\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 4}\n",
      "Inserted 4 chunks\n",
      "https://developer.nvidia.com/blog/train-generative-ai-models-for-drug-discovery-with-bionemo-framework/ | num chunks: 5\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 5}\n",
      "Inserted 5 chunks\n",
      "https://developer.nvidia.com/blog/streamline-job-initialization-and-cpu-based-tasks-with-nvidia-base-command-platform/ | num chunks: 3\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 3}\n",
      "Inserted 3 chunks\n",
      "https://developer.nvidia.com/blog/cuda-quantum-0-5-delivers-new-features-for-quantum-classical-computing/ | num chunks: 3\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 3}\n",
      "Inserted 3 chunks\n",
      "https://developer.nvidia.com/blog/one-giant-superchip-for-llms-recommenders-and-gnns-introducing-nvidia-gh200-nvl32/ | num chunks: 7\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 7}\n",
      "Inserted 7 chunks\n",
      "https://developer.nvidia.com/blog/build-enterprise-retrieval-augmented-generation-apps-with-nvidia-retrieval-qa-embedding-model/ | num chunks: 9\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 9}\n",
      "Inserted 9 chunks\n",
      "https://developer.nvidia.com/blog/simulating-realistic-traffic-behavior-with-a-bi-level-imitation-learning-ai-model/ | num chunks: 4\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 4}\n",
      "Inserted 4 chunks\n",
      "https://developer.nvidia.com/blog/new-risk-calculation-record-in-financial-services-with-dell-and-h100-system-for-hpc-and-ai/ | num chunks: 6\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 6}\n",
      "Inserted 6 chunks\n",
      "https://developer.nvidia.com/blog/announcing-helpsteer-an-open-source-dataset-for-building-helpful-llms/ | num chunks: 5\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 5}\n",
      "Inserted 5 chunks\n",
      "https://developer.nvidia.com/blog/bolstering-cybersecurity-how-large-language-models-and-generative-ai-are-transforming-digital-security/ | num chunks: 7\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 7}\n",
      "Inserted 7 chunks\n",
      "https://developer.nvidia.com/blog/accelerate-ai-workflows-for-3d-medical-imaging-with-nvidia-monai-cloud-apis/ | num chunks: 3\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 3}\n",
      "Inserted 3 chunks\n",
      "https://developer.nvidia.com/blog/advanced-api-performance-intrinsics/ | num chunks: 1\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 1}\n",
      "Inserted 1 chunks\n",
      "https://developer.nvidia.com/blog/unlocking-gpu-intrinsics-in-hlsl/ | num chunks: 7\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 7}\n",
      "Inserted 7 chunks\n",
      "https://developer.nvidia.com/blog/transforming-industrial-defect-detection-with-nvidia-tao-and-vision-ai-models/ | num chunks: 8\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 8}\n",
      "Inserted 8 chunks\n",
      "https://developer.nvidia.com/blog/boosting-custom-ros-graphs-using-nvidia-isaac-transport-for-ros/ | num chunks: 9\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 9}\n",
      "Inserted 9 chunks\n",
      "https://developer.nvidia.com/blog/mastering-llm-techniques-inference-optimization/ | num chunks: 20\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 20}\n",
      "Inserted 20 chunks\n",
      "https://developer.nvidia.com/blog/unlock-the-power-of-nvidia-grace-and-nvidia-hopper-architectures-with-foundational-hpc-software/ | num chunks: 6\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 6}\n",
      "Inserted 6 chunks\n",
      "https://developer.nvidia.com/blog/mastering-llm-techniques-training/ | num chunks: 13\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 13}\n",
      "Inserted 13 chunks\n",
      "https://developer.nvidia.com/blog/best-practices-for-securing-llm-enabled-applications/ | num chunks: 9\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 9}\n",
      "Inserted 9 chunks\n",
      "https://developer.nvidia.com/blog/mastering-llm-techniques-llmops/ | num chunks: 10\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 10}\n",
      "Inserted 10 chunks\n",
      "https://developer.nvidia.com/blog/deploy-large-language-models-at-the-edge-with-nvidia-igx-orin-developer-kit/ | num chunks: 7\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 7}\n",
      "Inserted 7 chunks\n",
      "https://developer.nvidia.com/blog/build-custom-enterprise-grade-generative-ai-with-nvidia-ai-foundation-models/ | num chunks: 6\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 6}\n",
      "Inserted 6 chunks\n",
      "https://developer.nvidia.com/blog/elevate-enterprise-generative-ai-app-development-with-nvidia-ai-on-azure-machine-learning/ | num chunks: 4\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 4}\n",
      "Inserted 4 chunks\n",
      "https://developer.nvidia.com/blog/nvidia-ai-foundation-models-build-custom-enterprise-chatbots-and-co-pilots-with-production-ready-llms/ | num chunks: 10\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 10}\n",
      "Inserted 10 chunks\n",
      "https://developer.nvidia.com/blog/accelerating-ptychography-workflows-with-nvidia-holoscan-at-diamond-light-source/ | num chunks: 10\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 10}\n",
      "Inserted 10 chunks\n",
      "https://developer.nvidia.com/blog/energy-efficiency-in-high-performance-computing-balancing-speed-and-sustainability/ | num chunks: 14\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 14}\n",
      "Inserted 14 chunks\n",
      "https://developer.nvidia.com/blog/nvidia-deep-learning-institute-launches-science-and-engineering-teaching-kit/ | num chunks: 4\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 4}\n",
      "Inserted 4 chunks\n",
      "https://developer.nvidia.com/blog/simplifying-gpu-programming-for-hpc-with-the-nvidia-grace-hopper-superchip/ | num chunks: 14\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 14}\n",
      "Inserted 14 chunks\n",
      "https://developer.nvidia.com/blog/using-synthetic-data-to-address-novel-viewpoints-for-autonomous-vehicle-perception/ | num chunks: 5\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 5}\n",
      "Inserted 5 chunks\n",
      "https://developer.nvidia.com/blog/optimize-energy-efficiency-of-multi-node-vasp-simulations-with-nvidia-magnum-io/ | num chunks: 14\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 14}\n",
      "Inserted 14 chunks\n",
      "https://developer.nvidia.com/blog/enabling-greater-patient-specific-cardiovascular-care-with-ai-surrogates/ | num chunks: 6\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 6}\n",
      "Inserted 6 chunks\n",
      "https://developer.nvidia.com/blog/accelerating-neurosymbolic-ai-with-rapids-and-vadalog-parallel/ | num chunks: 9\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 9}\n",
      "Inserted 9 chunks\n",
      "https://developer.nvidia.com/blog/whole-human-brain-neuro-mapping-at-cellular-resolution-on-dgx/ | num chunks: 7\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 7}\n",
      "Inserted 7 chunks\n",
      "https://developer.nvidia.com/blog/setting-new-records-at-data-center-scale-using-nvidia-h100-gpus-and-quantum-2-infiniband/ | num chunks: 15\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 15}\n",
      "Inserted 15 chunks\n",
      "https://developer.nvidia.com/blog/accelerating-networkx-on-nvidia-gpus-for-high-performance-graph-analytics/ | num chunks: 8\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 8}\n",
      "Inserted 8 chunks\n",
      "https://developer.nvidia.com/blog/rapids-cudf-accelerates-pandas-nearly-150x-with-zero-code-changes/ | num chunks: 4\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 4}\n",
      "Inserted 4 chunks\n",
      "https://developer.nvidia.com/blog/cuda-accelerated-robot-motion-generation-in-milliseconds-with-curobo/ | num chunks: 2\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 2}\n",
      "Inserted 2 chunks\n",
      "https://developer.nvidia.com/blog/getting-started-with-large-language-models-for-enterprise-solutions/ | num chunks: 11\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 11}\n",
      "Inserted 11 chunks\n",
      "https://developer.nvidia.com/blog/video-exploring-speech-ai-from-research-to-practical-production-applications/ | num chunks: 2\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 2}\n",
      "Inserted 2 chunks\n",
      "https://developer.nvidia.com/blog/level-up-your-lighting-qa-with-lighting-artist-ted-mebratu/ | num chunks: 2\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 2}\n",
      "Inserted 2 chunks\n",
      "https://developer.nvidia.com/blog/join-the-first-nvidia-llm-developer-day-elevate-your-app-building-skills/ | num chunks: 2\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 2}\n",
      "Inserted 2 chunks\n",
      "https://developer.nvidia.com/blog/analyze-visualize-and-optimize-real-world-processes-with-openusd-in-flexsim/ | num chunks: 4\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 4}\n",
      "Inserted 4 chunks\n",
      "https://developer.nvidia.com/blog/cuda-toolkit-12-3-delivers-new-features-for-accelerated-computing/ | num chunks: 3\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 3}\n",
      "Inserted 3 chunks\n",
      "https://developer.nvidia.com/blog/how-to-train-autonomous-mobile-robots-to-detect-warehouse-pallet-jacks-using-synthetic-data/ | num chunks: 7\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 7}\n",
      "Inserted 7 chunks\n",
      "https://developer.nvidia.com/blog/accelerate-genomic-analysis-for-any-sequencer-with-parabricks-v4-2/ | num chunks: 5\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 5}\n",
      "Inserted 5 chunks\n",
      "https://developer.nvidia.com/blog/advanced-api-performance-descriptors/ | num chunks: 4\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 4}\n",
      "Inserted 4 chunks\n",
      "https://developer.nvidia.com/blog/reduce-apache-spark-ml-compute-costs-with-new-algorithms-in-spark-rapids-ml-library/ | num chunks: 7\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 7}\n",
      "Inserted 7 chunks\n",
      "https://developer.nvidia.com/blog/efficient-cuda-debugging-memory-initialization-and-thread-synchronization-with-nvidia-compute-sanitizer/ | num chunks: 9\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 9}\n",
      "Inserted 9 chunks\n",
      "https://developer.nvidia.com/blog/differentiable-slang-example-applications/ | num chunks: 4\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 4}\n",
      "Inserted 4 chunks\n",
      "https://developer.nvidia.com/blog/differentiable-slang-a-shading-language-for-renderers-that-learn/ | num chunks: 9\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 9}\n",
      "Inserted 9 chunks\n",
      "https://developer.nvidia.com/blog/bringing-generative-ai-to-life-with-jetson/ | num chunks: 8\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 8}\n",
      "Inserted 8 chunks\n",
      "https://developer.nvidia.com/blog/ai-red-team-machine-learning-security-training/ | num chunks: 3\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 3}\n",
      "Inserted 3 chunks\n",
      "https://developer.nvidia.com/blog/optimizing-inference-on-llms-with-tensorrt-llm-now-publicly-available/ | num chunks: 8\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 8}\n",
      "Inserted 8 chunks\n",
      "https://developer.nvidia.com/blog/accelerate-ai-enabled-robotics-with-advanced-simulation-and-perception-tools-in-nvidia-isaac-platform/ | num chunks: 5\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 5}\n",
      "Inserted 5 chunks\n",
      "https://developer.nvidia.com/blog/boost-synthetic-data-generation-with-low-code-workflows-in-nvidia-omniverse-replicator-1-10/ | num chunks: 4\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 4}\n",
      "Inserted 4 chunks\n",
      "https://developer.nvidia.com/blog/unlock-faster-image-generation-in-stable-diffusion-web-ui-with-nvidia-tensorrt/ | num chunks: 3\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 3}\n",
      "Inserted 3 chunks\n",
      "https://developer.nvidia.com/blog/supercharge-graph-analytics-at-scale-with-gpu-cpu-fusion-for-100x-performance/ | num chunks: 9\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 9}\n",
      "Inserted 9 chunks\n",
      "https://developer.nvidia.com/blog/advanced-api-performance-debugging/ | num chunks: 5\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 5}\n",
      "Inserted 5 chunks\n",
      "https://developer.nvidia.com/blog/networking-for-data-centers-and-the-era-of-ai/ | num chunks: 4\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 4}\n",
      "Inserted 4 chunks\n",
      "https://developer.nvidia.com/blog/announcing-steerlm-a-simple-and-practical-technique-to-customize-llms-during-inference/ | num chunks: 8\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 8}\n",
      "Inserted 8 chunks\n",
      "https://developer.nvidia.com/blog/power-optimization-with-nvidia-jetson/ | num chunks: 5\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 5}\n",
      "Inserted 5 chunks\n",
      "https://developer.nvidia.com/blog/analyzing-the-security-of-machine-learning-research-code/ | num chunks: 9\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 9}\n",
      "Inserted 9 chunks\n",
      "https://developer.nvidia.com/blog/ai-powered-simulation-tools-for-surrogate-modeling-engineering-workflows-with-siml-ai-and-nvidia-modulus/ | num chunks: 4\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 4}\n",
      "Inserted 4 chunks\n",
      "https://developer.nvidia.com/blog/building-software-defined-high-performance-and-efficient-vran-requires-programmable-inline-acceleration/ | num chunks: 13\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 13}\n",
      "Inserted 13 chunks\n",
      "https://developer.nvidia.com/blog/accelerated-vector-search-approximating-with-rapids-raft-ivf-flat/ | num chunks: 11\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 11}\n",
      "Inserted 11 chunks\n",
      "https://developer.nvidia.com/blog/comparing-solutions-for-boosting-data-center-redundancy/ | num chunks: 5\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 5}\n",
      "Inserted 5 chunks\n",
      "https://developer.nvidia.com/blog/preventing-health-data-leaks-with-federated-learning-using-nvidia-flare/ | num chunks: 7\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 7}\n",
      "Inserted 7 chunks\n",
      "https://developer.nvidia.com/blog/nvidia-h100-system-sets-records-for-hpc-and-generative-ai-financial-risk-calculations/ | num chunks: 5\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 5}\n",
      "Inserted 5 chunks\n",
      "https://developer.nvidia.com/blog/enabling-the-worlds-first-gpu-accelerated-5g-open-ran-for-ntt-docomo-with-nvidia-aerial/ | num chunks: 7\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 7}\n",
      "Inserted 7 chunks\n",
      "https://developer.nvidia.com/blog/validating-nvidia-drive-sim-radar-models/ | num chunks: 12\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 12}\n",
      "Inserted 12 chunks\n",
      "https://developer.nvidia.com/blog/new-video-series-cuda-developer-tools-tutorials/ | num chunks: 2\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 2}\n",
      "Inserted 2 chunks\n",
      "https://developer.nvidia.com/blog/new-video-representing-data-with-openusd-custom-schemas/ | num chunks: 2\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 2}\n",
      "Inserted 2 chunks\n",
      "https://developer.nvidia.com/blog/how-to-train-an-object-detection-model-for-visual-inspection-with-synthetic-data/ | num chunks: 7\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 7}\n",
      "Inserted 7 chunks\n",
      "https://developer.nvidia.com/blog/software-defined-broadcast-with-nvidia-holoscan-for-media/ | num chunks: 4\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 4}\n",
      "Inserted 4 chunks\n",
      "https://developer.nvidia.com/blog/power-your-business-with-nvidia-ai-enterprise-4-0-for-production-ready-generative-ai/ | num chunks: 4\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 4}\n",
      "Inserted 4 chunks\n",
      "https://developer.nvidia.com/blog/generative-ai-and-accelerated-computing-for-spear-phishing-detection/ | num chunks: 4\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 4}\n",
      "Inserted 4 chunks\n",
      "https://developer.nvidia.com/blog/selecting-the-right-camera-for-the-nvidia-jetson-and-other-embedded-systems/ | num chunks: 7\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 7}\n",
      "Inserted 7 chunks\n",
      "https://developer.nvidia.com/blog/accelerating-vector-search-fine-tuning-gpu-index-algorithms/ | num chunks: 10\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 10}\n",
      "Inserted 10 chunks\n",
      "https://developer.nvidia.com/blog/accelerating-vector-search-using-gpu-powered-indexes-with-rapids-raft/ | num chunks: 8\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 8}\n",
      "Inserted 8 chunks\n",
      "https://developer.nvidia.com/blog/creating-immersive-events-with-openusd-and-digital-twins/ | num chunks: 6\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 6}\n",
      "Inserted 6 chunks\n",
      "https://developer.nvidia.com/blog/nvidia-tensorrt-llm-supercharges-large-language-model-inference-on-nvidia-h100-gpus/ | num chunks: 7\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 7}\n",
      "Inserted 7 chunks\n",
      "https://developer.nvidia.com/blog/leading-mlperf-inference-v3-1-results-gh200-grace-hopper-superchip-debut/ | num chunks: 10\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 10}\n",
      "Inserted 10 chunks\n",
      "https://developer.nvidia.com/blog/cuda-toolkit-symbol-server/ | num chunks: 2\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 2}\n",
      "Inserted 2 chunks\n",
      "https://developer.nvidia.com/blog/unlocking-multi-gpu-model-training-with-dask-xgboost/ | num chunks: 9\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 9}\n",
      "Inserted 9 chunks\n",
      "https://developer.nvidia.com/blog/supercharge-ransomware-detection-with-ai-enhanced-cybersecurity-solutions/ | num chunks: 4\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 4}\n",
      "Inserted 4 chunks\n",
      "https://developer.nvidia.com/blog/gpus-for-etl-optimizing-etl-architecture-for-apache-spark-sql-operations/ | num chunks: 6\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 6}\n",
      "Inserted 6 chunks\n",
      "https://developer.nvidia.com/blog/advanced-api-performance-shaders/ | num chunks: 5\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 5}\n",
      "Inserted 5 chunks\n",
      "https://developer.nvidia.com/blog/speeding-up-text-to-speech-diffusion-models-by-distillation/ | num chunks: 5\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 5}\n",
      "Inserted 5 chunks\n",
      "https://developer.nvidia.com/blog/solving-self-intersection-artifacts-in-directx-raytracing/ | num chunks: 14\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 14}\n",
      "Inserted 14 chunks\n",
      "https://developer.nvidia.com/blog/deploying-yolov5-on-nvidia-jetson-orin-with-cudla-quantization-aware-training-to-inference/ | num chunks: 9\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 9}\n",
      "Inserted 9 chunks\n",
      "https://developer.nvidia.com/blog/introduction-to-graph-neural-networks-with-nvidia-cugraph-dgl/ | num chunks: 5\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 5}\n",
      "Inserted 5 chunks\n",
      "https://developer.nvidia.com/blog/how-to-build-a-distributed-inference-cache-with-nvidia-triton-and-redis/ | num chunks: 10\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 10}\n",
      "Inserted 10 chunks\n",
      "Took 133.0439793170001 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time.perf_counter()\n",
    "\n",
    "# If this were not in Jupyter we would run this\n",
    "# asyncio.run(main())\n",
    "\n",
    "# Since we are in a notebook, Jupyter is already running its own event loop\n",
    "# so we can just simply await main()\n",
    "await main()\n",
    "\n",
    "end = time.perf_counter()\n",
    "\n",
    "print(f\"Took {end - start} seconds\")\n",
    "\n",
    "# This should take around 2-3 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "techblogs_assettype = None\n",
    "\n",
    "for assettype in asset_types_json:\n",
    "    if assettype[\"name\"] ==\"techblogs\":\n",
    "        techblogs_assettype = assettype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"doc:assettypes:techblogs\",\n",
      "  \"display_title\": \"TechBlog Posts\",\n",
      "  \"chunking_params\": \"{\\\"strategy\\\": \\\"heading_section_sentence\\\", \\\"code_behavior\\\": \\\"remove_code_sections\\\", \\\"chunk_min_words\\\": 250, \\\"chunk_overlap_words\\\": 50}\",\n",
      "  \"last_indexed\": \"2024-03-20T03:58:55\",\n",
      "  \"display_default\": true,\n",
      "  \"group\": \"Written Content\",\n",
      "  \"display_sort_order\": 1,\n",
      "  \"name\": \"techblogs\",\n",
      "  \"group_sort_order\": 2\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps(techblogs_assettype, indent=2))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to update the `assettypes` redis index with some metadata about how the `techblogs` index was created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "techblogs_assettype[\"chunking_params\"] = json.dumps(\n",
    "    {\n",
    "        \"strategy\": \"heading_section_sentence\",\n",
    "        \"code_behavior\": \"remove_code_sections\",\n",
    "        \"chunk_min_words\": 250,\n",
    "        \"chunk_overlap_words\": 50,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"doc:assettypes:techblogs\",\n",
      "  \"display_title\": \"TechBlog Posts\",\n",
      "  \"chunking_params\": \"{\\\"strategy\\\": \\\"heading_section_sentence\\\", \\\"code_behavior\\\": \\\"remove_code_sections\\\", \\\"chunk_min_words\\\": 250, \\\"chunk_overlap_words\\\": 50}\",\n",
      "  \"last_indexed\": \"2024-03-20T03:58:55\",\n",
      "  \"display_default\": true,\n",
      "  \"group\": \"Written Content\",\n",
      "  \"display_sort_order\": 1,\n",
      "  \"name\": \"techblogs\",\n",
      "  \"group_sort_order\": 2\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps(techblogs_assettype, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"display_title\": \"TechBlog Posts\",\n",
      "  \"chunking_params\": \"{\\\"strategy\\\": \\\"heading_section_sentence\\\", \\\"code_behavior\\\": \\\"remove_code_sections\\\", \\\"chunk_min_words\\\": 250, \\\"chunk_overlap_words\\\": 50}\",\n",
      "  \"last_indexed\": \"2024-08-09T23:52:23\",\n",
      "  \"display_default\": 1,\n",
      "  \"group\": \"Written Content\",\n",
      "  \"display_sort_order\": 1,\n",
      "  \"name\": \"techblogs\",\n",
      "  \"group_sort_order\": 2\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "update_asset_types_url = \"http://router:5006/asset-types/update\"\n",
    "response = httpx.post(update_asset_types_url, json={\"data\": techblogs_assettype})\n",
    "print(json.dumps(response.json(), indent=2))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can save our database by hitting the `/data/dump` router endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump_response = httpx.post(\"http://router:5006/data/dump\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"success\": true\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps(dump_response.json(), indent=2))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can confirm that the Redis database was indeed saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2024, 8, 9, 23, 51, 27)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import redis \n",
    "\n",
    "r = redis.Redis(host='redis', port=6379)\n",
    "r.lastsave()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2024, 8, 9, 23, 52, 24, 172130)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if we wanted to drop an index\n",
    "\n",
    "# from langchain.vectorstores.redis import Redis\n",
    "\n",
    "# Redis.drop_index(\n",
    "#     index_name=\"assettypes\", delete_documents=True, redis_url=\"redis://localhost:6379\"\n",
    "# )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can head back to our API docs and confirm that our data are available. Try the semantic search examples. Use the dropdown to select the second example: semantic search. Set k and asset_types to non-default values.\n",
    "\n",
    "Next, try the keyword search example, but modify the query to `cell phone`. The literal phrase `cell phone` doesn't return any articles, but the phrase `mobile phone` does. In cases where the concept is more important than the exact words, semantic search can help.\n",
    "\n",
    "If we go back to the semantic search and modify the third example to try `cell phone`, we do get relevant results. The second result should be good. The first result talks about pixels and cameras--both associated with cell phones, by themselves, and also through the Google Pixel cell phone. If you see many cases like this where words' multiple meanings throw off your domain-specific search interests, you can finetune the embedding model to prefer domain-related matches to general matches.\n",
    "\n",
    "Conversely, let's try a search for a specific product name that has no general-language meaning, like `H200`. Here is where a keyword search makes more sense, because we want the exact product name and not the meaning of \"H\" and \"200\" (try in particular wildcard search: `*H200`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Programmatic Search\n",
    "\n",
    "Now instead of using the docs, let's use Python and httpx to hit our `/search/semantic` endpoint. In this environment, the `router` service is available at the hostname `router`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'asset_type': 'techblogs',\n",
       "  'display_title': 'TechBlog Posts',\n",
       "  'results': [{'id': 'doc:techblogs:af9ca8617af14615992e18c76c822cb8',\n",
       "    'text': 'Improving CUDA Initialization Times Using cgroups in Certain Scenarios\\nMany CUDA applications running on multi-GPU platforms usually use a single GPU for their compute needs. In such scenarios, a performance penalty is paid by applications because CUDA has to enumerate/initialize all the GPUs on the system. If a CUDA application does not require other GPUs to be visible and accessible, you can launch such applications by isolating the unwanted GPUs from the CUDA process and eliminating unnecessary initialization steps. This post discusses the various methods to accomplish this and their performance benefits. \\nGPU isolation\\nGPU isolation can be achieved on Linux systems by using Linux tools like ```cgroups```. In this section, we first discuss a lower-level approach and then a higher-level possible approach. Another method exposed by CUDA to isolate devices is the use of ```CUDA_VISIBLE_DEVICES```. Although functionally similar, this approach has limited initialization performance gains compared to the ```cgroups``` approach. \\nIsolating GPUs using cgroups V1\\nControl groups provide a mechanism for aggregating or partitioning sets of tasks and all their future children into hierarchical groups with specialized behavior. You can use ```cgroups``` to control which GPUs are visible to a CUDA process. This ensures that only the GPUs that are needed by the CUDA process are made available to it. The following code provides a low-level example of how to employ ```cgroups``` and fully isolate a GPU to a single process.',\n",
       "    'text_components': '[\"Many CUDA applications running on multi-GPU platforms usually use a single GPU for their compute needs.\", \"In such scenarios, a performance penalty is paid by applications because CUDA has to enumerate/initialize all the GPUs on the system.\", \"If a CUDA application does not require other GPUs to be visible and accessible, you can launch such applications by isolating the unwanted GPUs from the CUDA process and eliminating unnecessary initialization steps.\", \"This post discusses the various methods to accomplish this and their performance benefits.\", \"GPU isolation can be achieved on Linux systems by using Linux tools like ```cgroups```.\", \"In this section, we first discuss a lower-level approach and then a higher-level possible approach.\", \"Another method exposed by CUDA to isolate devices is the use of ```CUDA_VISIBLE_DEVICES```.\", \"Although functionally similar, this approach has limited initialization performance gains compared to the ```cgroups``` approach.\", \"Control groups provide a mechanism for aggregating or partitioning sets of tasks and all their future children into hierarchical groups with specialized behavior.\", \"You can use ```cgroups``` to control which GPUs are visible to a CUDA process.\", \"This ensures that only the GPUs that are needed by the CUDA process are made available to it.\", \"The following code provides a low-level example of how to employ ```cgroups``` and fully isolate a GPU to a single process.\"]',\n",
       "    'word_count': '[17, 22, 33, 13, 20, 17, 19, 21, 23, 20, 18, 28]',\n",
       "    'contains_code': '[false, false, false, false, true, true, true, true, true, true, true, true]',\n",
       "    'heading_section_tag': '[\"h1\", \"h1\", \"h1\", \"h1\", \"h2\", \"h2\", \"h2\", \"h2\", \"h3\", \"h3\", \"h3\", \"h3\"]',\n",
       "    'heading_section_index': '[0, 0, 0, 0, 1, 1, 1, 1, 2, 2, 2, 2]',\n",
       "    'heading_section_title': '[\"Improving CUDA Initialization Times Using cgroups in Certain Scenarios\", \"Improving CUDA Initialization Times Using cgroups in Certain Scenarios\", \"Improving CUDA Initialization Times Using cgroups in Certain Scenarios\", \"Improving CUDA Initialization Times Using cgroups in Certain Scenarios\", \"GPU isolation\", \"GPU isolation\", \"GPU isolation\", \"GPU isolation\", \"Isolating GPUs using cgroups V1\", \"Isolating GPUs using cgroups V1\", \"Isolating GPUs using cgroups V1\", \"Isolating GPUs using cgroups V1\"]',\n",
       "    'only_code': '[false, false, false, false, false, false, false, false, false, false, false, false]',\n",
       "    'paragraph_index': '[0, 0, 0, 1, 2, 2, 3, 3, 4, 4, 4, 5]',\n",
       "    'paragraph_sentence_index': '[0, 1, 2, 0, 0, 1, 0, 1, 0, 1, 2, 0]',\n",
       "    'document_title': 'Improving CUDA Initialization Times Using cgroups in Certain Scenarios',\n",
       "    'document_date': '2024-01-05T22:14:41',\n",
       "    'document_date_modified': '2024-01-11T19:49:33',\n",
       "    'document_full_text': None,\n",
       "    'last_indexed': '2024-08-09T23:50:31',\n",
       "    'document_url': 'https://developer.nvidia.com/blog/improving-cuda-initialization-times-using-cgroups-in-certain-scenarios/',\n",
       "    'score': 0.5685},\n",
       "   {'id': 'doc:techblogs:34c34a6d8bf34dcf9d8c3086017b67e0',\n",
       "    'text': 'Isolating GPUs using cgroups V1\\nMultiple processes can be added to the ```tasks``` file to propagate the same controls to more than one process. \\nIsolating GPUs using the bubblewrap utility\\nThe bubblewrap utility (bwrap) is a higher-level utility available for sandboxing and access control in Linux operating systems, which can be used to achieve the same effect as the solution presented earlier. You can use this to conveniently restrict or allow access to specific GPUs from a CUDA process: More than one GPU can be exposed to a CUDA process by extending the ```dev-bind``` option in the code example. \\nPerformance benefits of GPU isolation\\nIn this section, we compare the performance of the CUDA driver initialization API (cuInit) with and without GPU isolation, measured over 256 iterations. The APIs are being run on an x86-based machine with four A100 class GPUs. Bar graph shows the performance of cuInit API running on an A104 system with and without GPU isolation using cgroups. The bar on the left shows the performance of cuInit when only a single GPU is exposed to the calling CUDA process via cgroups (~65 ms). The bar on the right shows the performance of cuInit when all four GPUs on the system are made available to the CUDA process (225 ms). Figure 1. CUDA initialisation performance comparison between a cgroup-constrained process and the default scenario on a four-GPU test system \\nSummary\\nGPU isolation using ```cgroups``` offers you the option of improving CUDA initialization times in a limited number of use cases where all the GPUs on the system are not required to be used by a given CUDA process. For more information, see the following resources: Control Groups version 1 — The Linux Kernel documentation cuInit',\n",
       "    'text_components': '[\"Multiple processes can be added to the ```tasks``` file to propagate the same controls to more than one process.\", \"The bubblewrap utility (bwrap) is a higher-level utility available for sandboxing and access control in Linux operating systems, which can be used to achieve the same effect as the solution presented earlier.\", \"You can use this to conveniently restrict or allow access to specific GPUs from a CUDA process:\", \"```\", \"# install bubblewrap utility on Debian-like systems\", \"$>sudo apt-get install -y bubblewrap\", \"# create a simple shell script that uses bubblewap for binding the required GPU to the launched process\", \"#!/bin/sh\", \"# bwrap.sh\", \"GPU=$1;shift   # 0, 1, 2, 3, ..\", \"if [ \\\\\"$GPU\\\\\" = \\\\\"\\\\\" ]; then echo \\\\\"missing arg: gpu id\\\\\"; exit 1; fi\", \"bwrap \\\\\\\\\", \"        --bind / / \\\\\\\\\", \"        --dev /dev --dev-bind /dev/nvidiactl /dev/nvidiactl --dev-bind /dev/nvidia-uvm /dev/nvidia-uvm  \\\\\\\\\", \"        --dev-bind /dev/nvidia$GPU /dev/nvidia$GPU \\\\\\\\\", \"        \\\\\"$@\\\\\"\", \"# Launch the CUDA process with the bubblewrap utility to only allow access to a specific GPU while running\", \"$> ./bwrap.sh 0 ./test_cuda_app <args>\", \"```\", \"More than one GPU can be exposed to a CUDA process by extending the ```dev-bind``` option in the code example.\", \"In this section, we compare the performance of the CUDA driver initialization API (cuInit) with and without GPU isolation, measured over 256 iterations.\", \"The APIs are being run on an x86-based machine with four A100 class GPUs.\", \"Bar graph shows the performance of cuInit API running on an A104 system with and without GPU isolation using cgroups.\", \"The bar on the left shows the performance of cuInit when only a single GPU is exposed to the calling CUDA process via cgroups (~65 ms).\", \"The bar on the right shows the performance of cuInit when all four GPUs on the system are made available to the CUDA process (225 ms).\", \"Figure 1.\", \"CUDA initialisation performance comparison between a cgroup-constrained process and the default scenario on a four-GPU test system\", \"GPU isolation using ```cgroups``` offers you the option of improving CUDA initialization times in a limited number of use cases where all the GPUs on the system are not required to be used by a given CUDA process.\", \"For more information, see the following resources:\", \"Control Groups version 1 \\\\u2014 The Linux Kernel documentation\", \"cuInit\"]',\n",
       "    'word_count': '[25, 33, 17, 0, 7, 6, 17, 2, 2, 7, 11, 1, 1, 16, 8, 0, 18, 5, 0, 27, 23, 15, 20, 26, 26, 2, 19, 44, 7, 8, 1]',\n",
       "    'contains_code': '[true, false, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, true, false, false, false]',\n",
       "    'heading_section_tag': '[\"h3\", \"h3\", \"h3\", \"h3\", \"h3\", \"h3\", \"h3\", \"h3\", \"h3\", \"h3\", \"h3\", \"h3\", \"h3\", \"h3\", \"h3\", \"h3\", \"h3\", \"h3\", \"h3\", \"h3\", \"h2\", \"h2\", \"h2\", \"h2\", \"h2\", \"h2\", \"h2\", \"h2\", \"h2\", \"h2\", \"h2\"]',\n",
       "    'heading_section_index': '[2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5]',\n",
       "    'heading_section_title': '[\"Isolating GPUs using cgroups V1\", \"Isolating GPUs using the bubblewrap utility\", \"Isolating GPUs using the bubblewrap utility\", \"Isolating GPUs using the bubblewrap utility\", \"Isolating GPUs using the bubblewrap utility\", \"Isolating GPUs using the bubblewrap utility\", \"Isolating GPUs using the bubblewrap utility\", \"Isolating GPUs using the bubblewrap utility\", \"Isolating GPUs using the bubblewrap utility\", \"Isolating GPUs using the bubblewrap utility\", \"Isolating GPUs using the bubblewrap utility\", \"Isolating GPUs using the bubblewrap utility\", \"Isolating GPUs using the bubblewrap utility\", \"Isolating GPUs using the bubblewrap utility\", \"Isolating GPUs using the bubblewrap utility\", \"Isolating GPUs using the bubblewrap utility\", \"Isolating GPUs using the bubblewrap utility\", \"Isolating GPUs using the bubblewrap utility\", \"Isolating GPUs using the bubblewrap utility\", \"Isolating GPUs using the bubblewrap utility\", \"Performance benefits of GPU isolation\", \"Performance benefits of GPU isolation\", \"Performance benefits of GPU isolation\", \"Performance benefits of GPU isolation\", \"Performance benefits of GPU isolation\", \"Performance benefits of GPU isolation\", \"Performance benefits of GPU isolation\", \"Summary\", \"Summary\", \"Summary\", \"Summary\"]',\n",
       "    'only_code': '[false, false, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false]',\n",
       "    'paragraph_index': '[15, 16, 16, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 18, 19, 19, 20, 20, 20, 21, 21, 22, 23, 24, 25]',\n",
       "    'paragraph_sentence_index': '[1, 0, 1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 0, 0, 1, 0, 1, 2, 0, 1, 0, 0, 0, 0]',\n",
       "    'document_title': 'Improving CUDA Initialization Times Using cgroups in Certain Scenarios',\n",
       "    'document_date': '2024-01-05T22:14:41',\n",
       "    'document_date_modified': '2024-01-11T19:49:33',\n",
       "    'document_full_text': None,\n",
       "    'last_indexed': '2024-08-09T23:50:31',\n",
       "    'document_url': 'https://developer.nvidia.com/blog/improving-cuda-initialization-times-using-cgroups-in-certain-scenarios/',\n",
       "    'score': 0.544},\n",
       "   {'id': 'doc:techblogs:b8d4d5df64a04f4db967341e47f811e2',\n",
       "    'text': 'Isolating GPUs using cgroups V1\\nYou can use ```cgroups``` to control which GPUs are visible to a CUDA process. This ensures that only the GPUs that are needed by the CUDA process are made available to it. The following code provides a low-level example of how to employ ```cgroups``` and fully isolate a GPU to a single process. Be aware that you will likely have to run these commands in a root shell to work properly. We show a more convenient, higher-level utility later in this post. When you are done with the tasks, unmount the ```/cgroupV1Device``` folder with the umount command. To allow or deny the user access to any other GPUs on the system, write those GPU numbers to the appropriate file. Here’s an example of denying access to only GPU5 and GPU6 on a multi-GPU system. In the ```/gpugroup``` folder created earlier, write the PID of the shell from where the CUDA process is to be launched into the ```tasks``` file: Now add GPU5 and GPU6 to the denied list: At this point, the CUDA process can’t see or access the two GPUs. To enable only specific GPUs to a CUDA process, those GPUs should be added to the ```devices.allow``` file and the rest of the GPUs should be added to the ```devices.deny``` file. The access controls apply per process. Multiple processes can be added to the ```tasks``` file to propagate the same controls to more than one process. \\nIsolating GPUs using the bubblewrap utility\\nThe bubblewrap utility (bwrap) is a higher-level utility available for sandboxing and access control in Linux operating systems, which can be used to achieve the same effect as the solution presented earlier.',\n",
       "    'text_components': '[\"You can use ```cgroups``` to control which GPUs are visible to a CUDA process.\", \"This ensures that only the GPUs that are needed by the CUDA process are made available to it.\", \"The following code provides a low-level example of how to employ ```cgroups``` and fully isolate a GPU to a single process.\", \"Be aware that you will likely have to run these commands in a root shell to work properly.\", \"We show a more convenient, higher-level utility later in this post.\", \"```\", \"# Create a mountpoint for the cgroup hierarchy as root\", \"$> cd /mnt\", \"$> mkdir cgroupV1Device\", \"# Use mount command to mount the hierarchy and attach the device subsystem to it\", \"$> mount -t cgroup -o devices devices cgroupV1Device\", \"$> cd cgroupV1Device\", \"# Now create a gpu subgroup directory to restrict/allow GPU access\", \"$> mkdir gpugroup\", \"$> cd gpugroup\", \"# in the gpugroup, you will see many cgroupfs files, the ones that interest us are tasks, device.deny and device.allow\", \"$> ls gpugroup\", \"tasks      devices.deny     devices.allow\", \"# Launch a shell from where the CUDA process will be executed. Gets the shells PID\", \"$> echo $$\", \"# Write this PID into the tasks files in the gpugroups folder\", \"$> echo <PID> tasks\", \"# List the device numbers of nvidia devices with the ls command\", \"$> ls -l /dev/nvidia*\", \"crw-rw-rw- 1 root root 195,   0 Jul 11 14:28 /dev/nvidia0\", \"crw-rw-rw- 1 root root 195,   0 Jul 11 14:28 /dev/nvidia1\", \"# Assuming that you only want to allow the CUDA process to access GPU0, you deny the CUDA process access to GPU1 by writing the following command to devices.deny\", \"$> echo \\'c 195:1 rmw\\' > devices.deny\", \"# Now GPU1 will not be visible to The CUDA process that you launch from the second shell.\", \"# To provide the CUDA process access to GPU1, we should write the following to devices.allow\", \"$> echo \\'c 195:1 rmw\\' > devices.allow\", \"```\", \"When you are done with the tasks, unmount the ```/cgroupV1Device``` folder with the umount command.\", \"```\", \"umount /mnt/cgroupV1Device\", \"```\", \"To allow or deny the user access to any other GPUs on the system, write those GPU numbers to the appropriate file.\", \"Here\\\\u2019s an example of denying access to only GPU5 and GPU6 on a multi-GPU system.\", \"In the ```/gpugroup``` folder created earlier, write the PID of the shell from where the CUDA process is to be launched into the ```tasks``` file:\", \"```\", \"$> echo <PID> tasks\", \"```\", \"Now add GPU5 and GPU6 to the denied list:\", \"```\", \"$> echo \\'c 195:5 rmw\\' > devices.deny\", \"$> echo \\'c 195:6 rmw\\' > devices.deny\", \"```\", \"At this point, the CUDA process can\\\\u2019t see or access the two GPUs.\", \"To enable only specific GPUs to a CUDA process, those GPUs should be added to the ```devices.allow``` file and the rest of the GPUs should be added to the ```devices.deny``` file.\", \"The access controls apply per process.\", \"Multiple processes can be added to the ```tasks``` file to propagate the same controls to more than one process.\", \"The bubblewrap utility (bwrap) is a higher-level utility available for sandboxing and access control in Linux operating systems, which can be used to achieve the same effect as the solution presented earlier.\"]',\n",
       "    'word_count': '[20, 18, 28, 18, 12, 0, 9, 2, 2, 14, 7, 2, 11, 2, 2, 21, 2, 5, 15, 1, 11, 3, 11, 4, 14, 14, 29, 7, 17, 16, 7, 0, 21, 0, 3, 0, 22, 17, 37, 0, 3, 0, 9, 0, 7, 7, 0, 14, 43, 6, 25, 33]',\n",
       "    'contains_code': '[true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, true, true, true, true, false, true, true, true, true, true, true, true, true, false]',\n",
       "    'heading_section_tag': '[\"h3\", \"h3\", \"h3\", \"h3\", \"h3\", \"h3\", \"h3\", \"h3\", \"h3\", \"h3\", \"h3\", \"h3\", \"h3\", \"h3\", \"h3\", \"h3\", \"h3\", \"h3\", \"h3\", \"h3\", \"h3\", \"h3\", \"h3\", \"h3\", \"h3\", \"h3\", \"h3\", \"h3\", \"h3\", \"h3\", \"h3\", \"h3\", \"h3\", \"h3\", \"h3\", \"h3\", \"h3\", \"h3\", \"h3\", \"h3\", \"h3\", \"h3\", \"h3\", \"h3\", \"h3\", \"h3\", \"h3\", \"h3\", \"h3\", \"h3\", \"h3\", \"h3\"]',\n",
       "    'heading_section_index': '[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3]',\n",
       "    'heading_section_title': '[\"Isolating GPUs using cgroups V1\", \"Isolating GPUs using cgroups V1\", \"Isolating GPUs using cgroups V1\", \"Isolating GPUs using cgroups V1\", \"Isolating GPUs using cgroups V1\", \"Isolating GPUs using cgroups V1\", \"Isolating GPUs using cgroups V1\", \"Isolating GPUs using cgroups V1\", \"Isolating GPUs using cgroups V1\", \"Isolating GPUs using cgroups V1\", \"Isolating GPUs using cgroups V1\", \"Isolating GPUs using cgroups V1\", \"Isolating GPUs using cgroups V1\", \"Isolating GPUs using cgroups V1\", \"Isolating GPUs using cgroups V1\", \"Isolating GPUs using cgroups V1\", \"Isolating GPUs using cgroups V1\", \"Isolating GPUs using cgroups V1\", \"Isolating GPUs using cgroups V1\", \"Isolating GPUs using cgroups V1\", \"Isolating GPUs using cgroups V1\", \"Isolating GPUs using cgroups V1\", \"Isolating GPUs using cgroups V1\", \"Isolating GPUs using cgroups V1\", \"Isolating GPUs using cgroups V1\", \"Isolating GPUs using cgroups V1\", \"Isolating GPUs using cgroups V1\", \"Isolating GPUs using cgroups V1\", \"Isolating GPUs using cgroups V1\", \"Isolating GPUs using cgroups V1\", \"Isolating GPUs using cgroups V1\", \"Isolating GPUs using cgroups V1\", \"Isolating GPUs using cgroups V1\", \"Isolating GPUs using cgroups V1\", \"Isolating GPUs using cgroups V1\", \"Isolating GPUs using cgroups V1\", \"Isolating GPUs using cgroups V1\", \"Isolating GPUs using cgroups V1\", \"Isolating GPUs using cgroups V1\", \"Isolating GPUs using cgroups V1\", \"Isolating GPUs using cgroups V1\", \"Isolating GPUs using cgroups V1\", \"Isolating GPUs using cgroups V1\", \"Isolating GPUs using cgroups V1\", \"Isolating GPUs using cgroups V1\", \"Isolating GPUs using cgroups V1\", \"Isolating GPUs using cgroups V1\", \"Isolating GPUs using cgroups V1\", \"Isolating GPUs using cgroups V1\", \"Isolating GPUs using cgroups V1\", \"Isolating GPUs using cgroups V1\", \"Isolating GPUs using the bubblewrap utility\"]',\n",
       "    'only_code': '[false, false, false, false, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, false, false, false, true, true, true, false, true, true, true, true, false, false, false, false, false]',\n",
       "    'paragraph_index': '[4, 4, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 8, 8, 8, 9, 9, 10, 11, 11, 11, 12, 13, 13, 13, 13, 14, 14, 15, 15, 16]',\n",
       "    'paragraph_sentence_index': '[1, 2, 0, 1, 2, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 0, 0, 1, 2, 0, 1, 0, 0, 1, 2, 0, 0, 1, 2, 3, 0, 1, 0, 1, 0]',\n",
       "    'document_title': 'Improving CUDA Initialization Times Using cgroups in Certain Scenarios',\n",
       "    'document_date': '2024-01-05T22:14:41',\n",
       "    'document_date_modified': '2024-01-11T19:49:33',\n",
       "    'document_full_text': None,\n",
       "    'last_indexed': '2024-08-09T23:50:31',\n",
       "    'document_url': 'https://developer.nvidia.com/blog/improving-cuda-initialization-times-using-cgroups-in-certain-scenarios/',\n",
       "    'score': 0.4848}]}]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_endpoint = \"http://router:5006/search/semantic\"\n",
    "\n",
    "response = httpx.post(\n",
    "    search_endpoint, json={\"query\": \"cgroups\", \"k\": 3, \"asset_types\": [\"techblogs\"]}\n",
    ")\n",
    "response.json()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice we have some valuable metadata from our chunking service.\n",
    "\n",
    "- `text_components` contains a list of the sentences that were found in this section.\n",
    "- `contains_code` is a boolean that indicates whether code was in that particular sentence.\n",
    "- `only_code` is another boolean that indicates whether the entire sentence is comprised of code.\n",
    "\n",
    "Because we specified that `code_behavior` was `remove_code_sections` the `text` attribute of the items in the `results` in the response is essentially a concatenated string formed by the sentences that were not \n",
    "entirely made up of code (i.e., `only_code == False`). You will still see some small amount of code in the text (i.e., `contains_code == True and only_code == False`), but these are usually single words in \n",
    "a sentence of natural language.\n",
    "\n",
    "The advantage of this is, we can still go and extract the `only_code` sections because they are available through the metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'doc:techblogs:34c34a6d8bf34dcf9d8c3086017b67e0',\n",
       " 'text': 'Isolating GPUs using cgroups V1\\nMultiple processes can be added to the ```tasks``` file to propagate the same controls to more than one process. \\nIsolating GPUs using the bubblewrap utility\\nThe bubblewrap utility (bwrap) is a higher-level utility available for sandboxing and access control in Linux operating systems, which can be used to achieve the same effect as the solution presented earlier. You can use this to conveniently restrict or allow access to specific GPUs from a CUDA process: More than one GPU can be exposed to a CUDA process by extending the ```dev-bind``` option in the code example. \\nPerformance benefits of GPU isolation\\nIn this section, we compare the performance of the CUDA driver initialization API (cuInit) with and without GPU isolation, measured over 256 iterations. The APIs are being run on an x86-based machine with four A100 class GPUs. Bar graph shows the performance of cuInit API running on an A104 system with and without GPU isolation using cgroups. The bar on the left shows the performance of cuInit when only a single GPU is exposed to the calling CUDA process via cgroups (~65 ms). The bar on the right shows the performance of cuInit when all four GPUs on the system are made available to the CUDA process (225 ms). Figure 1. CUDA initialisation performance comparison between a cgroup-constrained process and the default scenario on a four-GPU test system \\nSummary\\nGPU isolation using ```cgroups``` offers you the option of improving CUDA initialization times in a limited number of use cases where all the GPUs on the system are not required to be used by a given CUDA process. For more information, see the following resources: Control Groups version 1 — The Linux Kernel documentation cuInit',\n",
       " 'text_components': '[\"Multiple processes can be added to the ```tasks``` file to propagate the same controls to more than one process.\", \"The bubblewrap utility (bwrap) is a higher-level utility available for sandboxing and access control in Linux operating systems, which can be used to achieve the same effect as the solution presented earlier.\", \"You can use this to conveniently restrict or allow access to specific GPUs from a CUDA process:\", \"```\", \"# install bubblewrap utility on Debian-like systems\", \"$>sudo apt-get install -y bubblewrap\", \"# create a simple shell script that uses bubblewap for binding the required GPU to the launched process\", \"#!/bin/sh\", \"# bwrap.sh\", \"GPU=$1;shift   # 0, 1, 2, 3, ..\", \"if [ \\\\\"$GPU\\\\\" = \\\\\"\\\\\" ]; then echo \\\\\"missing arg: gpu id\\\\\"; exit 1; fi\", \"bwrap \\\\\\\\\", \"        --bind / / \\\\\\\\\", \"        --dev /dev --dev-bind /dev/nvidiactl /dev/nvidiactl --dev-bind /dev/nvidia-uvm /dev/nvidia-uvm  \\\\\\\\\", \"        --dev-bind /dev/nvidia$GPU /dev/nvidia$GPU \\\\\\\\\", \"        \\\\\"$@\\\\\"\", \"# Launch the CUDA process with the bubblewrap utility to only allow access to a specific GPU while running\", \"$> ./bwrap.sh 0 ./test_cuda_app <args>\", \"```\", \"More than one GPU can be exposed to a CUDA process by extending the ```dev-bind``` option in the code example.\", \"In this section, we compare the performance of the CUDA driver initialization API (cuInit) with and without GPU isolation, measured over 256 iterations.\", \"The APIs are being run on an x86-based machine with four A100 class GPUs.\", \"Bar graph shows the performance of cuInit API running on an A104 system with and without GPU isolation using cgroups.\", \"The bar on the left shows the performance of cuInit when only a single GPU is exposed to the calling CUDA process via cgroups (~65 ms).\", \"The bar on the right shows the performance of cuInit when all four GPUs on the system are made available to the CUDA process (225 ms).\", \"Figure 1.\", \"CUDA initialisation performance comparison between a cgroup-constrained process and the default scenario on a four-GPU test system\", \"GPU isolation using ```cgroups``` offers you the option of improving CUDA initialization times in a limited number of use cases where all the GPUs on the system are not required to be used by a given CUDA process.\", \"For more information, see the following resources:\", \"Control Groups version 1 \\\\u2014 The Linux Kernel documentation\", \"cuInit\"]',\n",
       " 'word_count': '[25, 33, 17, 0, 7, 6, 17, 2, 2, 7, 11, 1, 1, 16, 8, 0, 18, 5, 0, 27, 23, 15, 20, 26, 26, 2, 19, 44, 7, 8, 1]',\n",
       " 'contains_code': '[true, false, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, true, false, false, false]',\n",
       " 'heading_section_tag': '[\"h3\", \"h3\", \"h3\", \"h3\", \"h3\", \"h3\", \"h3\", \"h3\", \"h3\", \"h3\", \"h3\", \"h3\", \"h3\", \"h3\", \"h3\", \"h3\", \"h3\", \"h3\", \"h3\", \"h3\", \"h2\", \"h2\", \"h2\", \"h2\", \"h2\", \"h2\", \"h2\", \"h2\", \"h2\", \"h2\", \"h2\"]',\n",
       " 'heading_section_index': '[2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5]',\n",
       " 'heading_section_title': '[\"Isolating GPUs using cgroups V1\", \"Isolating GPUs using the bubblewrap utility\", \"Isolating GPUs using the bubblewrap utility\", \"Isolating GPUs using the bubblewrap utility\", \"Isolating GPUs using the bubblewrap utility\", \"Isolating GPUs using the bubblewrap utility\", \"Isolating GPUs using the bubblewrap utility\", \"Isolating GPUs using the bubblewrap utility\", \"Isolating GPUs using the bubblewrap utility\", \"Isolating GPUs using the bubblewrap utility\", \"Isolating GPUs using the bubblewrap utility\", \"Isolating GPUs using the bubblewrap utility\", \"Isolating GPUs using the bubblewrap utility\", \"Isolating GPUs using the bubblewrap utility\", \"Isolating GPUs using the bubblewrap utility\", \"Isolating GPUs using the bubblewrap utility\", \"Isolating GPUs using the bubblewrap utility\", \"Isolating GPUs using the bubblewrap utility\", \"Isolating GPUs using the bubblewrap utility\", \"Isolating GPUs using the bubblewrap utility\", \"Performance benefits of GPU isolation\", \"Performance benefits of GPU isolation\", \"Performance benefits of GPU isolation\", \"Performance benefits of GPU isolation\", \"Performance benefits of GPU isolation\", \"Performance benefits of GPU isolation\", \"Performance benefits of GPU isolation\", \"Summary\", \"Summary\", \"Summary\", \"Summary\"]',\n",
       " 'only_code': '[false, false, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false]',\n",
       " 'paragraph_index': '[15, 16, 16, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 18, 19, 19, 20, 20, 20, 21, 21, 22, 23, 24, 25]',\n",
       " 'paragraph_sentence_index': '[1, 0, 1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 0, 0, 1, 0, 1, 2, 0, 1, 0, 0, 0, 0]',\n",
       " 'document_title': 'Improving CUDA Initialization Times Using cgroups in Certain Scenarios',\n",
       " 'document_date': '2024-01-05T22:14:41',\n",
       " 'document_date_modified': '2024-01-11T19:49:33',\n",
       " 'document_full_text': None,\n",
       " 'last_indexed': '2024-08-09T23:50:31',\n",
       " 'document_url': 'https://developer.nvidia.com/blog/improving-cuda-initialization-times-using-cgroups-in-certain-scenarios/',\n",
       " 'score': 0.544}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result1 = response.json()[0]['results'][1]\n",
    "result1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, because of some constraints from how data structures can be stored in redis, we'll need to convert the JSON strings into lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "heading_section_index = json.loads(result1[\"heading_section_index\"])\n",
    "heading_section_title = json.loads(result1[\"heading_section_title\"])\n",
    "paragraph_index = json.loads(result1[\"paragraph_index\"])\n",
    "contains_code = json.loads(result1[\"contains_code\"])\n",
    "only_code = json.loads(result1[\"only_code\"])\n",
    "text_components = json.loads(result1[\"text_components\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(heading_section_index) == len(heading_section_title) == len(paragraph_index) == len(contains_code) == len(only_code) == len(text_components)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's get all the text, including both natural language and code sections, and join it together as it appeared in the article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Isolating GPUs using cgroups V1\n",
      "Multiple processes can be added to the ```tasks``` file to propagate the same controls to more than one process. \n",
      "Isolating GPUs using the bubblewrap utility\n",
      "The bubblewrap utility (bwrap) is a higher-level utility available for sandboxing and access control in Linux operating systems, which can be used to achieve the same effect as the solution presented earlier. You can use this to conveniently restrict or allow access to specific GPUs from a CUDA process: \n",
      "```\n",
      "# install bubblewrap utility on Debian-like systems\n",
      "$>sudo apt-get install -y bubblewrap\n",
      "# create a simple shell script that uses bubblewap for binding the required GPU to the launched process\n",
      "#!/bin/sh\n",
      "# bwrap.sh\n",
      "GPU=$1;shift   # 0, 1, 2, 3, ..\n",
      "if [ \"$GPU\" = \"\" ]; then echo \"missing arg: gpu id\"; exit 1; fi\n",
      "bwrap \\\n",
      "        --bind / / \\\n",
      "        --dev /dev --dev-bind /dev/nvidiactl /dev/nvidiactl --dev-bind /dev/nvidia-uvm /dev/nvidia-uvm  \\\n",
      "        --dev-bind /dev/nvidia$GPU /dev/nvidia$GPU \\\n",
      "        \"$@\"\n",
      "# Launch the CUDA process with the bubblewrap utility to only allow access to a specific GPU while running\n",
      "$> ./bwrap.sh 0 ./test_cuda_app <args>\n",
      "```\n",
      "\n",
      "More than one GPU can be exposed to a CUDA process by extending the ```dev-bind``` option in the code example. \n",
      "Performance benefits of GPU isolation\n",
      "In this section, we compare the performance of the CUDA driver initialization API (cuInit) with and without GPU isolation, measured over 256 iterations. The APIs are being run on an x86-based machine with four A100 class GPUs. \n",
      "Bar graph shows the performance of cuInit API running on an A104 system with and without GPU isolation using cgroups. The bar on the left shows the performance of cuInit when only a single GPU is exposed to the calling CUDA process via cgroups (~65 ms). The bar on the right shows the performance of cuInit when all four GPUs on the system are made available to the CUDA process (225 ms). \n",
      "Figure 1. CUDA initialisation performance comparison between a cgroup-constrained process and the default scenario on a four-GPU test system \n",
      "Summary\n",
      "GPU isolation using ```cgroups``` offers you the option of improving CUDA initialization times in a limited number of use cases where all the GPUs on the system are not required to be used by a given CUDA process. \n",
      "For more information, see the following resources: \n",
      "Control Groups version 1 — The Linux Kernel documentation \n",
      "cuInit\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\n",
    "last_hsi = None\n",
    "\n",
    "for i in range(len(text_components)):\n",
    "    if last_hsi is None or last_hsi != heading_section_index[i]:\n",
    "        text += heading_section_title[i] + \"\\n\"\n",
    "    text += text_components[i]\n",
    "    if only_code[i]:\n",
    "        text += \"\\n\"\n",
    "    else:\n",
    "        text += \" \"\n",
    "    # look ahead\n",
    "    if i < len(text_components) - 1:\n",
    "        if paragraph_index[i] != paragraph_index[i+1]:\n",
    "            text += \"\\n\"\n",
    "    \n",
    "    last_hsi = heading_section_index[i]\n",
    "\n",
    "print(text.strip())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or, we can extract exclusively the code and ignore the natural language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```\n",
      "# install bubblewrap utility on Debian-like systems\n",
      "$>sudo apt-get install -y bubblewrap\n",
      "# create a simple shell script that uses bubblewap for binding the required GPU to the launched process\n",
      "#!/bin/sh\n",
      "# bwrap.sh\n",
      "GPU=$1;shift   # 0, 1, 2, 3, ..\n",
      "if [ \"$GPU\" = \"\" ]; then echo \"missing arg: gpu id\"; exit 1; fi\n",
      "bwrap \\\n",
      "        --bind / / \\\n",
      "        --dev /dev --dev-bind /dev/nvidiactl /dev/nvidiactl --dev-bind /dev/nvidia-uvm /dev/nvidia-uvm  \\\n",
      "        --dev-bind /dev/nvidia$GPU /dev/nvidia$GPU \\\n",
      "        \"$@\"\n",
      "# Launch the CUDA process with the bubblewrap utility to only allow access to a specific GPU while running\n",
      "$> ./bwrap.sh 0 ./test_cuda_app <args>\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\n",
    "\n",
    "for i in range(len(text_components)):    \n",
    "    if only_code[i]:\n",
    "        text += text_components[i]\n",
    "        text += \"\\n\"\n",
    "\n",
    "print(text.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storing TechBlog Summaries in Redis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take our code from the previous lesson and use it to generate summaries for each article in our batch asynchronously.\n",
    "\n",
    "We will use an LLM to generate the summaries, so let's begin by instatiating an LLM instance to work with. Here we import a `ChatOpenAI` instance of our local NIM Mixtral 8x7B model configured and ready for use with LangChain from an [`llms` helper file](llms.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llms import llms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = llms.nim_mixtral_llm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional Remote LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optionally, instead of using our local model, you can also use either NVIDIA AI Foundation's Mixtral 8x7B model or OpenAI's gpt-3.5-turbo.\n",
    "\n",
    "For either of these 2 options you'll need an API key. For more details about NVIDIA AI Foundation and obtaining a free API key, see [the notebook *NVIDIA AI Foundation.ipynb*](./NVIDIA%20AI%20Foundation.ipynb).\n",
    "\n",
    "After obtaining an appropriate API key, uncomment the appropriate cell below, add your API key, and run the cell to set `llm` to the remote LLM you chose to work with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NVIDIA AI Foundation Mixtral 8x7B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from llms import set_api_key\n",
    "# set_api_key('NVIDIA_API_KEY', '<your_nvidia_api_key>')\n",
    "# llm = llms.nvai_mixtral_llm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OpenAI GPT-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from llms import set_api_key\n",
    "# set_api_key('OPENAI_API_KEY', '<your_openai_api_key>')\n",
    "# llm = llms.openai_gpt3_llm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct Tech Blog Payloads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll construct a payload for each techblog that contains the blog's HTML, along with various metadata fields, and chunking guidelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "payloads = []\n",
    "\n",
    "file_list = [x for x in sorted(os.listdir(data_dir)) if '.json' in x]\n",
    "\n",
    "for i, filename in enumerate(file_list):\n",
    "    with open(os.path.join(data_dir, filename), 'r') as in_file:\n",
    "        data = json.load(in_file)\n",
    "        \n",
    "    for item in data:\n",
    "        \n",
    "        # skip items that do not link to developer.nvidia.com/blog or blogs.nvidia.com\n",
    "        if not item['link'].startswith(\"https://developer.nvidia.com/blog\"): # and not item['link'].startswith(\"https://blogs.nvidia.com\"):\n",
    "            # print(f\"Skipping URL {item['link']}\")\n",
    "            continue\n",
    "            \n",
    "        document_title = item['title']['rendered']\n",
    "        document_url = item['link']\n",
    "        document_html = item['content']['rendered']\n",
    "        document_date = item['date_gmt']\n",
    "        document_date_modified = item['modified_gmt']\n",
    "        \n",
    "        payload = {\n",
    "            \"strategy\": \"heading_section\",\n",
    "            \"code_behavior\": \"remove_code_sections\",\n",
    "            \"input_type\": \"html\",\n",
    "            \"input_str\": document_html,\n",
    "            \"additional_metadata\": {\n",
    "                \"document_title\": document_title,\n",
    "                \"document_url\": document_url,\n",
    "                \"document_date\": document_date,\n",
    "                \"document_date_modified\": document_date_modified,\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        payloads.append(payload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total num payloads: 150\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total num payloads: {len(payloads)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'strategy': 'heading_section',\n",
       " 'code_behavior': 'remove_code_sections',\n",
       " 'input_type': 'html',\n",
       " 'input_str': '<div style=\"margin-top: 0px; margin-bottom: 0px;\" class=\"sharethis-inline-share-buttons\" ></div>\\n<p><a href=\"https://www.nvidia.com/en-us/deep-learning-ai/solutions/data-science/workbench/\">NVIDIA AI Workbench</a> is now in beta, bringing a wealth of new features to streamline how enterprise developers create, use, and share AI and machine learning (ML) projects. Announced at SIGGRAPH 2023, NVIDIA AI Workbench enables developers to create, collaborate, and migrate AI workloads on their GPU-enabled environment of choice. To learn more, see <a href=\"https://developer.nvidia.com/blog/develop-and-deploy-scalable-generative-ai-models-seamlessly-with-nvidia-ai-workbench/\">Develop and Deploy Scalable Generative AI Models Seamlessly with NVIDIA AI Workbench</a>.</p>\\n\\n\\n\\n<p>This post explains how NVIDIA AI Workbench helps streamline the AI workflow and details new features of the beta release. It also walks through a coding copilot reference example, which enables you to use AI Workbench to create, test, and customize a pretrained generative AI model on your platform of choice.</p>\\n\\n\\n\\n<h2 class=\"wp-block-heading\">What is NVIDIA AI Workbench?</h2>\\n\\n\\n\\n<p>With AI Workbench, developers and data scientists have the flexibility to start an AI or ML project locally on a PC or workstation and then migrate it anywhere. Projects can be pushed out to a data center, public cloud, or <a href=\"https://www.nvidia.com/en-us/data-center/dgx-cloud/\">NVIDIA DGX Cloud</a>, or moved to a local RTX PC or workstation for inference and lightweight customization, depending on project requirements.</p>\\n\\n\\n\\n<p>AI Workbench helps developers simplify and shorten setup, development, and migration for AI workflows by providing the ability to work on their choice of heterogeneous compute resources. Benefits include:</p>\\n\\n\\n\\n<ul>\\n<li>Free and quick install on the system of choice with an intuitive UX or CLI for project creation and management.</li>\\n\\n\\n\\n<li>Streamlined configuration for compute resources and runtimes, providing reproducibility and flexibility to work on different GPU resources.</li>\\n\\n\\n\\n<li>Simplified version control and management for containers and Git repositories and integrations with GitHub, GitLab, and the <a href=\"https://catalog.ngc.nvidia.com/containers?filters=&amp;orderBy=scoreDESC&amp;query=workbench\">NVIDIA NGC catalog</a>.</li>\\n\\n\\n\\n<li>Automation and streamlining to handle Git and container-based developer environments, enabling users to work on their choice of system, laptop, workstation, server, or the cloud.</li>\\n\\n\\n\\n<li>Reproducibility across users and systems with transparent handling for idiosyncrasies like credentials, secrets, and file system changes without the overhead.</li>\\n\\n\\n\\n<li>Scalable creation and distribution of complex workflows and applications for generative AI, GPU-enabled ML, and data science.</li>\\n</ul>\\n\\n\\n\\n<h2 class=\"wp-block-heading\">What’s new in the beta release</h2>\\n\\n\\n\\n<p>The AI Workbench beta release includes the following exciting new features, with updates to the user interface and expanded support for container runtimes and Git servers.</p>\\n\\n\\n\\n<p>Simplified setup and installation<strong> </strong>on Windows 11, Ubuntu, 22.04, and macOS 11 or higher.&nbsp;</p>\\n\\n\\n\\n<ul>\\n<li>Install AI Workbench quickly in two ways: click-through install using the desktop app on local systems or command-line install on remote systems.</li>\\n\\n\\n\\n<li>Work from anywhere with support for the three major operating systems for a uniform experience. AI Workbench runs on Windows distributions that support WSL2, Ubuntu 22.04, and macOS version 11 and higher.</li>\\n</ul>\\n\\n\\n\\n<p>Simplified version control and streamlined development with containerized environments.</p>\\n\\n\\n\\n<ul>\\n<li>Access simple and comprehensive Git-compliant version control with both the Desktop App and CLI. Push, pull, and fetch features are now included.&nbsp;</li>\\n\\n\\n\\n<li>Create a containerized JupyterLab environment with isolation and reproducibility without having to handle details.</li>\\n\\n\\n\\n<li>Choose from two container runtime options: Docker or Podman.</li>\\n</ul>\\n\\n\\n\\n<p>Expanded feature parity between the user interface and the CLI.</p>\\n\\n\\n\\n<ul>\\n<li>See commit history and summaries directly in the Desktop App.</li>\\n\\n\\n\\n<li>View improved container state and application status notifications in the Desktop App.</li>\\n</ul>\\n\\n\\n\\n<p>Expanded default base images.</p>\\n\\n\\n\\n<ul>\\n<li>Access three new base images for project creation, in addition to the <a href=\"https://catalog.ngc.nvidia.com/orgs/nvidia/teams/ai-workbench/containers/python-basic\">Python Basic</a> and <a href=\"https://catalog.ngc.nvidia.com/orgs/nvidia/teams/ai-workbench/containers/pytorch\">PyTorch Basic</a> images already in the NGC catalog. New base images for CUDA 11.0, CUDA 12.0, and CUDA 12.2 provide the foundation for further customization.</li>\\n</ul>\\n\\n\\n\\n<p>Three new example projects for reference.</p>\\n\\n\\n\\n<ul>\\n<li><a href=\"https://github.com/NVIDIA/workbench-example-mistral-finetune\">Mistral</a>: Fine-tune a Mistral 7B large language model (LLM) on a custom code instructions dataset using QLoRA PEFT.</li>\\n\\n\\n\\n<li><a href=\"https://github.com/NVIDIA/workbench-example-local-rag\">RAG</a>: Converse with your data using a local, user-friendly developer workflow for <a href=\"https://blogs.nvidia.com/blog/what-is-retrieval-augmented-generation/\">retrieval-augmented generation</a> (RAG).</li>\\n\\n\\n\\n<li><a href=\"https://github.com/NVIDIA/workbench-example-nemotron-finetune\">NeMotron-3</a>: Fine-tune a Nemotron-3 8B LLM on a custom QA dataset using <a href=\"https://www.nvidia.com/en-us/ai-data-science/generative-ai/nemo-framework/\">NVIDIA NeMo</a>.</li>\\n</ul>\\n\\n\\n\\n<h2 class=\"wp-block-heading\">Create your own coding copilot</h2>\\n\\n\\n\\n<p>This section walks through an example of how AI Workbench can significantly simplify the process of using and fine-tuning a generative AI model on a GPU system of the user’s choice.&nbsp;</p>\\n\\n\\n\\n<h3 class=\"wp-block-heading\">Key concepts</h3>\\n\\n\\n\\n<p>A few key concepts used in this example are outlined below.</p>\\n\\n\\n\\n<h4 class=\"wp-block-heading\">AI Workbench Project</h4>\\n\\n\\n\\n<p>An AI Workbench Project is a Git repository that contains a set of configuration files that can be read by AI Workbench to automate the creation and management of a containerized development environment. A project references everything needed for a configured, containerized development environment and includes:&nbsp;</p>\\n\\n\\n\\n<ul>\\n<li>Code, data, and models</li>\\n\\n\\n\\n<li>Simple configuration files that drive AI Workbench automation for container customization and package installation</li>\\n\\n\\n\\n<li>A project specification metadata file to wrap the repository in a way that&#8217;s compatible with AI Workbench&nbsp;</li>\\n</ul>\\n\\n\\n\\n<p>Visit <a href=\"https://github.com/nvidia?q=workbench&amp;type=all&amp;language=&amp;sort=\">NVIDIA on GitHub</a> to reference NVIDIA projects that provide starting points for adapting your own data and use cases. Additionally, <a href=\"https://developer.nvidia.com/ai-workbench-early-access/join\">AI Workbench early access</a> members can contribute and use third-party <a href=\"https://developer.nvidia.com/ai-workbench-early-access/members\">community project examples</a>.</p>\\n\\n\\n\\n<p>The <a href=\"https://github.com/NVIDIA/workbench-example-mistral-finetune\">Mistral 7B fine-tuning reference project</a> showcased in this post highlights how to leverage the power of AI Workbench to build a basic coding copilot on a system of your choice.&nbsp;</p>\\n\\n\\n\\n<h4 class=\"wp-block-heading\">Fine-tuning</h4>\\n\\n\\n\\n<p>While Mistral 7B is a strong baseline for multiple downstream tasks, it can lack domain-specific knowledge based on proprietary or otherwise sensitive information. Fine-tuning is used to improve the model’s responses in these cases.&nbsp;</p>\\n\\n\\n\\n<p>There are two versions of fine-tuning. The first, <em>full fine-tuning</em>, uses the new data to update all of the model weights. This can improve domain-specific results but often requires more time and larger, more expensive GPUs. The second, <em>parameter efficient fine-tuning (PEFT)</em>, is a family of techniques that update a subset of the model weights. PEFT is often preferable to full fine-tuning because it produces comparable results in far less time and with smaller, less expensive GPUs.</p>\\n\\n\\n\\n<p>This example focuses primarily on the Quantized Low Rank Adaptation (QLoRA) method of PEFT. Low Rank Adaptation (LoRA) is a method of PEFT that uses smaller weight matrices in the retraining as approximations instead of updating the full weight matrix. This rank decomposition optimization technique enables greater memory efficiency and can reduce the GPU size required for successful fine-tuning.&nbsp;</p>\\n\\n\\n\\n<p>QLoRA<strong> </strong>is a further optimization that reduces the precision of model weights to provide even greater advances in memory and space efficiency. The most common quantization used for this LoRA fine-tuning workflow is 4-bit quantization, which provides a decent balance between model performance and fine-tuning feasibility.</p>\\n\\n\\n\\n<h3 class=\"wp-block-heading\">Walkthrough of Mistral 7B fine-tuning project in NVIDIA AI Workbench</h3>\\n\\n\\n\\n<p>This walkthrough includes high-level code and details. For more information, see the full <a href=\"https://github.com/NVIDIA/workbench-example-mistral-finetune\">Mistral 7B fine-tuning reference project</a> on GitHub. The project fine-tunes the Mistral 7B base model on the <a href=\"https://huggingface.co/datasets/TokenBender/code_instructions_122k_alpaca_style\">TokenBender code instructions dataset</a>, consisting of 122K Alpaca-style code instructions and code solutions.&nbsp;</p>\\n\\n\\n<div class=\"wp-block-image\">\\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"1480\" height=\"955\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/mistral-7b-fine-tuning-project-nvidia-ai-workbench.png\" alt=\"Screenshot of the Mistral 7B fine-tuning project in the NVIDIA AI Workbench user interface.\" class=\"wp-image-76708\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/mistral-7b-fine-tuning-project-nvidia-ai-workbench.png 1480w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/mistral-7b-fine-tuning-project-nvidia-ai-workbench-300x194.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/mistral-7b-fine-tuning-project-nvidia-ai-workbench-625x403.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/mistral-7b-fine-tuning-project-nvidia-ai-workbench-179x115.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/mistral-7b-fine-tuning-project-nvidia-ai-workbench-768x496.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/mistral-7b-fine-tuning-project-nvidia-ai-workbench-645x416.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/mistral-7b-fine-tuning-project-nvidia-ai-workbench-465x300.png 465w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/mistral-7b-fine-tuning-project-nvidia-ai-workbench-139x90.png 139w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/mistral-7b-fine-tuning-project-nvidia-ai-workbench-362x234.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/mistral-7b-fine-tuning-project-nvidia-ai-workbench-170x110.png 170w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/mistral-7b-fine-tuning-project-nvidia-ai-workbench-1024x661.png 1024w\" sizes=\"(max-width: 1480px) 100vw, 1480px\" /><figcaption class=\"wp-element-caption\"><em>Figure 1. Building the Mistral 7B fine-tuning project in NVIDIA AI Workbench</em></figcaption></figure></div>\\n\\n\\n<p>First, download the data and split it into 80% training, 10% validation, and 10% testing datasets. One entry of an instruction in the dataset is shown below as an example:</p>\\n\\n\\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: plain; title: ; notranslate\" title=\"\">\\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.\\n### Instruction: Output the maximum element in an array. ### Input: &#91;1, 5, 10, 8, 15] ### Output: 15\\n</pre></div>\\n\\n\\n<p>Next, download the Mistral 7B model weights to this project:</p>\\n\\n\\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: plain; title: ; notranslate\" title=\"\">\\nmodel_id = &quot;mistralai/Mistral-7B-v0.1&quot;\\nbb_config = BitsAndBytesConfig(\\n    load_in_4bit=True,\\n    bnb_4bit_use_double_quant=True,\\n    bnb_4bit_quant_type=&quot;nf4&quot;,\\n)\\n\\nmodel = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bb_config)\\n</pre></div>\\n\\n\\n<p>Notice that the 4-bit quantization configuration is specified for the base model.&nbsp;</p>\\n\\n\\n\\n<p>Next, evaluate the performance of the base model on a specific sample programming question. This establishes a baseline for comparison between the base model and the final, fine-tuned model.&nbsp;</p>\\n\\n\\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: plain; title: ; notranslate\" title=\"\">\\nbase_prompt = &quot;&quot;&quot;Write a function to output the prime factorization of 2023 in python, C, and C++&quot;&quot;&quot;\\n\\nbase_tokenizer = AutoTokenizer.from_pretrained(\\n    model_id,\\n    add_bos_token=True,\\n)\\n\\nmodel_input = base_tokenizer(base_prompt, return_tensors=&quot;pt&quot;).to(&quot;cuda&quot;)\\n\\nmodel.eval()\\nwith torch.no_grad():\\n    print(base_tokenizer.decode(model.generate(**model_input, max_new_tokens=256)&#91;0], skip_special_tokens=True))\\n\\n***** Output ***** \\n\\n## Prime Factorization of 2023\\n\\nThe prime factorization of 2023 is 13 x 157.\\n\\n## Prime Factorization of 2023 in Python\\n\\nThe prime factorization of 2023 in python is given below.\\n\\ndef prime_factorization(n):\\n    factors = &#91;]\\n    for i in range(2, n + 1):\\n        if n % i == 0:\\n            factors.append(i)\\n    return factors\\n\\nprint(prime_factorization(2023))\\n\\n...\\n</pre></div>\\n\\n\\n<p>Notice that the base model doesn&#8217;t perform well out of the box. First, the base model seems to think the prime factorization of 2,023 is 13 x 157. This amounts to 2041. The actual answer is 7 x 17 x 17.</p>\\n\\n\\n\\n<p>Second, the Python function the model outputs is incorrect as well. Running the suggested code gives an answer of [7, 17, 119, 289, 2,023] when in fact 119, 289, and 2,023 are not prime factors.</p>\\n\\n\\n\\n<p>Fine-tuning is necessary to improve model performance. Begin with preprocessing the dataset by reformatting the dataset entries to better fit the instruction prompt [INST] for fine-tuning. Then tokenize each of these prompts.&nbsp;</p>\\n\\n\\n\\n<p>Next, specify the configuration for QLoRA fine-tuning and perform the fine-tuning. By default, the fine-tuning takes 1,000 iterations, with checkpointing and evaluation every 50 steps. These hyperparameters can be adjusted depending on hardware resource constraints. On an <a href=\"https://www.nvidia.com/en-us/data-center/a100/\">NVIDIA A100</a> 80 GB GPU system, this configuration can take about 6.5 hours.&nbsp;</p>\\n\\n\\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: plain; title: ; notranslate\" title=\"\">\\nconfig = LoraConfig(\\n    r=8,\\n    lora_alpha=16,\\n    target_modules=&#91;\\n        &quot;q_proj&quot;,\\n        &quot;k_proj&quot;,\\n        &quot;v_proj&quot;,\\n        &quot;o_proj&quot;,\\n        &quot;gate_proj&quot;,\\n        &quot;up_proj&quot;,\\n        &quot;down_proj&quot;,\\n        &quot;lm_head&quot;,\\n    ],\\n    bias=&quot;none&quot;,\\n    lora_dropout=0.05,\\n    task_type=&quot;CAUSAL_LM&quot;,\\n)\\n\\nmodel = get_peft_model(model, config)\\n\\n# Training configs\\ntrainer = transformers.Trainer(\\n    model=model,\\n    train_dataset=tokenized_train_ds,\\n    eval_dataset=tokenized_val_ds,\\n    args=transformers.TrainingArguments(\\n        output_dir=&quot;./mistral-code-instruct&quot;,\\n        warmup_steps=5,\\n        per_device_train_batch_size=2,\\n        gradient_checkpointing=True,\\n        gradient_accumulation_steps=4,\\n        max_steps=1000,\\n        learning_rate=2.5e-5,\\n        logging_steps=50,\\n        bf16=True,\\n        optim=&quot;paged_adamw_8bit&quot;,\\n        logging_dir=&quot;./logs&quot;,\\n        save_strategy=&quot;steps&quot;,\\n        save_steps=50,\\n        evaluation_strategy=&quot;steps&quot;, \\n        eval_steps=50,\\n        report_to=&quot;none&quot;,\\n        do_eval=True,\\n    ),\\n    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\\n)\\n\\n# Train! \\ntrainer.train()\\n</pre></div>\\n\\n\\n<p>Using the final fine-tuning checkpoint, define the updated Mistral 7B model:</p>\\n\\n\\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: plain; title: ; notranslate\" title=\"\">\\nft_model = PeftModel.from_pretrained(base_model, &quot;mistral-code-instruct/checkpoint-1000&quot;)\\n</pre></div>\\n\\n\\n<p>To evaluate the fine-tuned model’s performance, ask a coding question similar to the initial one and request the generation of a code snippet:</p>\\n\\n\\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: plain; title: ; notranslate\" title=\"\">\\neval_prompt = f&quot;&quot;&quot;\\nFor a given integer n, print out all its prime factors one on each line. \\nn = 30\\n&quot;&quot;&quot;\\n\\ninput_ids = tokenizer(eval_prompt, return_tensors=&quot;pt&quot;, truncation=True).input_ids.cuda()\\noutputs = ft_model.generate(input_ids=input_ids, max_new_tokens=256, do_sample=True, top_p=0.9,temperature=0.5)\\n\\n***** Output ***** \\n\\nGenerated response:\\n2\\n3\\n5 \\n\\n#include &lt;stdio.h&gt;\\n\\nint main() {\\n    int n = 30;\\n    int i;\\n    for (i = 2; i &lt;= n; i++) {\\n        while (n % i == 0) {\\n            printf(&quot;%d\\\\n&quot;, i);\\n            n /= i;\\n        }\\n    }\\n    return 0;\\n}\\n\\n...\\n</pre></div>\\n\\n\\n<p>The generated code snippet response from the fine-tuned model looks much better. Use a sandbox environment to try the code for yourself.</p>\\n\\n\\n\\n<p>That’s all there is to fine-tuning the Mistral 7B LLM. This project provides a reference workflow for your development needs. You can always choose to customize the project to better suit your enterprise data or use case. Switch out the dataset with one of your own, or fine-tune the model to another use case, such as text summarization or question-answering.&nbsp;</p>\\n\\n\\n\\n<h2 class=\"wp-block-heading\">Get started with AI Workbench</h2>\\n\\n\\n\\n<p>NVIDIA AI Workbench helps you create, share, and scale enterprise AI and ML workflows between different GPU-enabled environments. <a href=\"https://developer.nvidia.com/ai-workbench-early-access\">Sign up for beta access to NVIDIA AI Workbench</a>. To learn more about AI Workbench, check out these resources:</p>\\n\\n\\n\\n<ul>\\n<li>Watch a <a href=\"https://youtu.be/ntMRzPzSvM4?feature=shared\">video demo</a> of AI Workbench that walks through a custom image generation example project with Stable Diffusion XL.&nbsp;</li>\\n\\n\\n\\n<li>Read <a href=\"https://developer.nvidia.com/blog/develop-and-deploy-scalable-generative-ai-models-seamlessly-with-nvidia-ai-workbench/\">Develop and Deploy Scalable Generative AI Models Seamlessly with NVIDIA AI Workbench</a> to get a sneak peek of more example projects.</li>\\n\\n\\n\\n<li>Reference the <a href=\"https://docs.nvidia.com/ai-workbench/\">NVIDIA AI Workbench User Guide</a> to get your AI and ML projects up and running with NVIDIA AI Workbench.</li>\\n</ul>\\n',\n",
       " 'additional_metadata': {'document_title': 'Create, Share, and Scale Enterprise AI Workflows with NVIDIA AI Workbench, Now in Beta',\n",
       "  'document_url': 'https://developer.nvidia.com/blog/create-share-and-scale-enterprise-ai-workflows-with-nvidia-ai-workbench-now-in-beta/',\n",
       "  'document_date': '2024-01-30T20:02:55',\n",
       "  'document_date_modified': '2024-01-31T01:06:02'}}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "payloads[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import httpx\n",
    "import json\n",
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries = [None] * len(payloads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the summaries from the json file\n",
    "with open(\"data/techblogs_summaries/saved.json\", \"r\") as f:\n",
    "    saved_summaries = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a semaphore object with a limit of 3.\n",
    "limit = asyncio.Semaphore(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def async_generate(llm, msg):\n",
    "    resp = await llm.agenerate([msg])\n",
    "    return resp.generations[0][0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the following line if we want to save new summaries.\n",
    "# saved_summaries = {}\n",
    "\n",
    "async def upload_techblogs_summaries(llm, client: httpx.AsyncClient, payload: dict):\n",
    "    async with limit:\n",
    "\n",
    "        try:\n",
    "            chunks = await chunking_request(client, payload)\n",
    "        except:  # retry once\n",
    "            chunks = await chunking_request(client, payload)\n",
    "        print(\n",
    "            f\"{payload['additional_metadata']['document_url']} | num chunks: {len(chunks)}\"\n",
    "        )\n",
    "\n",
    "        clean_text_no_code = \"\\n\".join([x[\"text\"] for x in chunks])\n",
    "        clean_text_with_code = \"\\n\".join([ x[\"heading_section_title\"][0] + \"\\n\" + \"\\n\".join(x[\"text_components\"]) for x in chunks])\n",
    "        \n",
    "        # Ask LLM for summaries\n",
    "\n",
    "        # uncomment if we want to save new summaries\n",
    "        # template = ChatPromptTemplate.from_messages(\n",
    "        #     [(\"user\", \"Summarize the following article in 200 words or less:\\n{user_input}\")]\n",
    "        # )\n",
    "\n",
    "        # msg = template.format_messages(\n",
    "        #     user_input=clean_text_no_code\n",
    "        # )\n",
    "\n",
    "        # summary = await async_generate(llm, msg)\n",
    "        # summary_with_metadata = [\n",
    "        #     {\n",
    "        #         \"text\": payload[\"additional_metadata\"][\"document_title\"] + \"\\n\" + summary,\n",
    "        #         \"text_components\": [ x[\"heading_section_title\"][0] + \"\\n\" + \"\\n\".join(x[\"text_components\"]) for x in chunks],\n",
    "        #         \"document_title\": payload[\"additional_metadata\"][\"document_title\"],\n",
    "        #         \"document_url\": payload[\"additional_metadata\"][\"document_url\"],\n",
    "        #         \"document_date\": payload[\"additional_metadata\"][\"document_date\"],\n",
    "        #         \"document_date_modified\": payload[\"additional_metadata\"][\"document_date_modified\"],\n",
    "        #         \"document_full_text\": clean_text_with_code\n",
    "        #     }\n",
    "        # ]\n",
    "        # saved_summaries[payload[\"additional_metadata\"][\"document_url\"]] = summary_with_metadata\n",
    "\n",
    "        # load summary we've already generated\n",
    "        # comment the following line if we want to save new summaries\n",
    "        summary_with_metadata = saved_summaries[payload[\"additional_metadata\"][\"document_url\"]]\n",
    "\n",
    "        # gets ids of existing items with this url\n",
    "        try:\n",
    "            existing_items = await get_existing_items_request(client, payload, \"summarize_techblogs\")\n",
    "        except:  # retry once\n",
    "            existing_items = await get_existing_items_request(client, payload, \"summarize_techblogs\")\n",
    "\n",
    "        if len(existing_items) > 0:\n",
    "            results = existing_items[0][\"results\"]\n",
    "            if len(results) > 0:\n",
    "                # delete items that are associated with this url\n",
    "                try:\n",
    "                    deleted_items = await delete_request(client, results, \"summarize_techblogs\")\n",
    "                except:  # retry once\n",
    "                    deleted_items = await delete_request(client, results, \"summarize_techblogs\")\n",
    "                print(f\"Deleted ids reponse: {deleted_items}\")\n",
    "\n",
    "        # insert: send chunks to redis\n",
    "        resp = await client.post(\n",
    "            insert_url,\n",
    "            json={\n",
    "                \"asset_type\": \"summarize_techblogs\",\n",
    "                \"chunks\": summary_with_metadata,\n",
    "            },\n",
    "            timeout=15,\n",
    "        )\n",
    "        print(f\"Inserted {len(resp.json())} chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def main():\n",
    "    async with httpx.AsyncClient() as client:\n",
    "        tasks = []\n",
    "        # for i in range(0, 7):\n",
    "        for i in range(0, len(payloads)):\n",
    "            tasks.append(upload_techblogs_summaries(llm, client, payloads[i]))\n",
    "\n",
    "        await asyncio.gather(*tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://developer.nvidia.com/blog/create-share-and-scale-enterprise-ai-workflows-with-nvidia-ai-workbench-now-in-beta/ | num chunks: 9\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 1}\n",
      "Inserted 1 chunks\n",
      "https://developer.nvidia.com/blog/emulating-the-attention-mechanism-in-transformer-models-with-a-fully-convolutional-network/ | num chunks: 6\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 1}\n",
      "Inserted 1 chunks\n",
      "https://developer.nvidia.com/blog/modernizing-the-data-center-with-accelerated-networking/ | num chunks: 8\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 1}\n",
      "Inserted 1 chunks\n",
      "https://developer.nvidia.com/blog/announcing-metropolis-microservices-on-nvidia-jetson-orin-for-rapid-edge-ai-development/ | num chunks: 10\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 1}\n",
      "Inserted 1 chunks\n",
      "https://developer.nvidia.com/blog/advancing-production-ai-with-nvidia-ai-enterprise/ | num chunks: 9\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 1}\n",
      "Inserted 1 chunks\n",
      "https://developer.nvidia.com/blog/build-enterprise-grade-ai-with-nvidia-ai-software/ | num chunks: 6\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 1}\n",
      "Inserted 1 chunks\n",
      "https://developer.nvidia.com/blog/delivering-efficient-high-performance-ai-clouds-with-nvidia-doca-2-5/ | num chunks: 12\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 1}\n",
      "Inserted 1 chunks\n",
      "https://developer.nvidia.com/blog/using-the-power-of-ai-to-make-factories-safer/ | num chunks: 3\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 1}\n",
      "Inserted 1 chunks\n",
      "https://developer.nvidia.com/blog/simplifying-network-operations-for-ai-with-nvidia-quantum-infiniband/ | num chunks: 4\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 1}\n",
      "Inserted 1 chunks\n",
      "https://developer.nvidia.com/blog/how-to-build-vision-ai-applications-at-the-edge-with-nvidia-metropolis-microservices-and-apis/ | num chunks: 24\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 1}\n",
      "Inserted 1 chunks\n",
      "https://developer.nvidia.com/blog/bringing-generative-ai-to-the-edge-with-nvidia-metropolis-microservices-for-jetson/ | num chunks: 18\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 1}\n",
      "Inserted 1 chunks\n",
      "https://developer.nvidia.com/blog/model-monday-query-graphs-with-optimized-deplot-model/ | num chunks: 10\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 1}\n",
      "Inserted 1 chunks\n",
      "https://developer.nvidia.com/blog/benchmarking-camera-performance-on-your-workstation-with-nvidia-isaac-sim/ | num chunks: 5\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 1}\n",
      "Inserted 1 chunks\n",
      "https://developer.nvidia.com/blog/generate-synthetic-data-for-deep-object-pose-estimation-training-with-nvidia-isaac-ros/ | num chunks: 11\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 1}\n",
      "Inserted 1 chunks\n",
      "https://developer.nvidia.com/blog/simulating-railroads-with-openusd/ | num chunks: 5\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 1}\n",
      "Inserted 1 chunks\n",
      "https://developer.nvidia.com/blog/new-support-for-dutch-and-persian-released-by-nemo-asr/ | num chunks: 4\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 1}\n",
      "Inserted 1 chunks\n",
      "https://developer.nvidia.com/blog/robust-scene-text-detection-and-recognition-inference-optimization/ | num chunks: 6\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 1}\n",
      "Inserted 1 chunks\n",
      "https://developer.nvidia.com/blog/robust-scene-text-detection-and-recognition-implementation/ | num chunks: 5\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 1}\n",
      "Inserted 1 chunks\n",
      "https://developer.nvidia.com/blog/robust-scene-text-detection-and-recognition-introduction/ | num chunks: 6\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 1}\n",
      "Inserted 1 chunks\n",
      "https://developer.nvidia.com/blog/experience-real-time-audio-and-video-communication-with-nvidia-maxine/ | num chunks: 7\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 1}\n",
      "Inserted 1 chunks\n",
      "https://developer.nvidia.com/blog/enhancing-phone-customer-service-with-asr-customization/ | num chunks: 8\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 1}\n",
      "Inserted 1 chunks\n",
      "https://developer.nvidia.com/blog/new-models-molmim-and-diffdock-power-molecule-generation-and-molecular-docking-in-bionemo/ | num chunks: 1\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 1}\n",
      "Inserted 1 chunks\n",
      "https://developer.nvidia.com/blog/new-stable-diffusion-models-accelerated-with-nvidia-tensorrt/ | num chunks: 5\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 1}\n",
      "Inserted 1 chunks\n",
      "https://developer.nvidia.com/blog/get-started-with-generative-ai-development-for-windows-pcs-with-rtx-systems/ | num chunks: 8\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 1}\n",
      "Inserted 1 chunks\n",
      "https://developer.nvidia.com/blog/spotlight-convai-reinvents-non-playable-character-interactions/ | num chunks: 5\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 1}\n",
      "Inserted 1 chunks\n",
      "https://developer.nvidia.com/blog/supercharging-llm-applications-on-windows-pcs-with-nvidia-rtx-systems/ | num chunks: 7\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 1}\n",
      "Inserted 1 chunks\n",
      "https://developer.nvidia.com/blog/building-lifelike-digital-avatars-with-nvidia-ace-microservices/ | num chunks: 4\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 1}\n",
      "Inserted 1 chunks\n",
      "https://developer.nvidia.com/blog/improving-cuda-initialization-times-using-cgroups-in-certain-scenarios/ | num chunks: 6\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 1}\n",
      "Inserted 1 chunks\n",
      "https://developer.nvidia.com/blog/develop-ml-ai-with-metaflow-deploy-with-triton-inference-server/ | num chunks: 11\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 1}\n",
      "Inserted 1 chunks\n",
      "https://developer.nvidia.com/blog/video-encoding-at-8k60-with-split-frame-encoding-and-nvidia-ada-lovelace-architecture/ | num chunks: 6\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 1}\n",
      "Inserted 1 chunks\n",
      "https://developer.nvidia.com/blog/accelerating-inference-on-end-to-end-workflows-with-h2o-ai-and-nvidia/ | num chunks: 12\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 1}\n",
      "Inserted 1 chunks\n",
      "https://developer.nvidia.com/blog/q-and-a-looking-back-to-when-1997s-quake-2-got-a-path-tracing-update/ | num chunks: 2\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 1}\n",
      "Inserted 1 chunks\n",
      "https://developer.nvidia.com/blog/qa-real-time-ray-tracing-in-a-cinematic-scene/ | num chunks: 1\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 1}\n",
      "Inserted 1 chunks\n",
      "https://developer.nvidia.com/blog/breakthrough-in-functional-annotation-with-hifi-nn/ | num chunks: 6\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 1}\n",
      "Inserted 1 chunks\n",
      "https://developer.nvidia.com/blog/year-in-review-trending-posts-of-2023/ | num chunks: 1\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 1}\n",
      "Inserted 1 chunks\n",
      "https://developer.nvidia.com/blog/accelerate-quantum-circuit-simulation-with-nvidia-cuquantum-23-10/ | num chunks: 5\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 1}\n",
      "Inserted 1 chunks\n",
      "https://developer.nvidia.com/blog/teaching-avs-the-language-of-human-driving-behavior-with-trajeglish/ | num chunks: 4\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 1}\n",
      "Inserted 1 chunks\n",
      "https://developer.nvidia.com/blog/rag-101-retrieval-augmented-generation-questions-answered/ | num chunks: 12\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 1}\n",
      "Inserted 1 chunks\n",
      "https://developer.nvidia.com/blog/rag-101-demystifying-retrieval-augmented-generation-pipelines/ | num chunks: 13\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 1}\n",
      "Inserted 1 chunks\n",
      "https://developer.nvidia.com/blog/deploying-retrieval-augmented-generation-applications-on-nvidia-gh200-delivers-accelerated-performance/ | num chunks: 9\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 1}\n",
      "Inserted 1 chunks\n",
      "https://developer.nvidia.com/blog/streamline-etl-workflows-with-nested-data-types-in-rapids-libcudf/ | num chunks: 6\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 1}\n",
      "Inserted 1 chunks\n",
      "https://developer.nvidia.com/blog/advanced-api-performance-swap-chains/ | num chunks: 4\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 1}\n",
      "Inserted 1 chunks\n",
      "https://developer.nvidia.com/blog/achieving-top-inference-performance-with-the-nvidia-h100-tensor-core-gpu-and-nvidia-tensorrt-llm/ | num chunks: 2\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 1}\n",
      "Inserted 1 chunks\n",
      "https://developer.nvidia.com/blog/generative-ai-research-spotlight-demystifying-diffusion-based-models/ | num chunks: 13\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 1}\n",
      "Inserted 1 chunks\n",
      "https://developer.nvidia.com/blog/fast-track-computer-vision-deployments-with-nvidia-deepstream-and-edge-impulse/ | num chunks: 11\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 1}\n",
      "Inserted 1 chunks\n",
      "https://developer.nvidia.com/blog/simulate-and-localize-a-husky-robot-with-nvidia-isaac/ | num chunks: 9\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 1}\n",
      "Inserted 1 chunks\n",
      "https://developer.nvidia.com/blog/next-generation-seismic-monitoring-with-neural-operators/ | num chunks: 6\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 1}\n",
      "Inserted 1 chunks\n",
      "https://developer.nvidia.com/blog/benchmarking-quantum-computing-applications-with-bmw-group-and-nvidia-cuquantum/ | num chunks: 6\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 1}\n",
      "Inserted 1 chunks\n",
      "https://developer.nvidia.com/blog/automating-data-center-networks-with-nvidia-nvue-and-ansible/ | num chunks: 8\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 1}\n",
      "Inserted 1 chunks\n",
      "https://developer.nvidia.com/blog/nvidia-research-shows-interactive-texture-painting-with-gen-ai-at-siggraph-asia-real-time-live/ | num chunks: 4\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 1}\n",
      "Inserted 1 chunks\n",
      "https://developer.nvidia.com/blog/available-now-nvidia-ai-accelerated-dgl-and-pyg-containers-for-gnns/ | num chunks: 13\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 1}\n",
      "Inserted 1 chunks\n",
      "https://developer.nvidia.com/blog/develop-and-optimize-vision-ai-models-for-trillions-of-devices-with-nvidia-tao/ | num chunks: 5\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 1}\n",
      "Inserted 1 chunks\n",
      "https://developer.nvidia.com/blog/reconstructing-dynamic-driving-scenarios-using-self-supervised-learning/ | num chunks: 6\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 1}\n",
      "Inserted 1 chunks\n",
      "https://developer.nvidia.com/blog/nvidia-tensorrt-llm-enhancements-deliver-massive-large-language-model-speedups-on-nvidia-h200/ | num chunks: 5\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 1}\n",
      "Inserted 1 chunks\n",
      "https://developer.nvidia.com/blog/create-lifelike-avatars-with-ai-animation-and-speech-features-in-nvidia-ace/ | num chunks: 5\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 1}\n",
      "Inserted 1 chunks\n",
      "https://developer.nvidia.com/blog/new-nvidia-nemo-framework-features-and-nvidia-h200-supercharge-llm-training-performance-and-versatility/ | num chunks: 7\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 1}\n",
      "Inserted 1 chunks\n",
      "https://developer.nvidia.com/blog/building-your-first-llm-agent-application/ | num chunks: 15\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 1}\n",
      "Inserted 1 chunks\n",
      "https://developer.nvidia.com/blog/introduction-to-llm-agents/ | num chunks: 15\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 1}\n",
      "Inserted 1 chunks\n",
      "https://developer.nvidia.com/blog/boost-meeting-productivity-with-ai-powered-note-taking-and-summarization/ | num chunks: 14\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 1}\n",
      "Inserted 1 chunks\n",
      "https://developer.nvidia.com/blog/train-generative-ai-models-for-drug-discovery-with-bionemo-framework/ | num chunks: 11\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 1}\n",
      "Inserted 1 chunks\n",
      "https://developer.nvidia.com/blog/streamline-job-initialization-and-cpu-based-tasks-with-nvidia-base-command-platform/ | num chunks: 6\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 1}\n",
      "Inserted 1 chunks\n",
      "https://developer.nvidia.com/blog/cuda-quantum-0-5-delivers-new-features-for-quantum-classical-computing/ | num chunks: 6\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 1}\n",
      "Inserted 1 chunks\n",
      "https://developer.nvidia.com/blog/one-giant-superchip-for-llms-recommenders-and-gnns-introducing-nvidia-gh200-nvl32/ | num chunks: 7\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 1}\n",
      "Inserted 1 chunks\n",
      "https://developer.nvidia.com/blog/build-enterprise-retrieval-augmented-generation-apps-with-nvidia-retrieval-qa-embedding-model/ | num chunks: 14\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 1}\n",
      "Inserted 1 chunks\n",
      "https://developer.nvidia.com/blog/simulating-realistic-traffic-behavior-with-a-bi-level-imitation-learning-ai-model/ | num chunks: 5\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 1}\n",
      "Inserted 1 chunks\n",
      "https://developer.nvidia.com/blog/new-risk-calculation-record-in-financial-services-with-dell-and-h100-system-for-hpc-and-ai/ | num chunks: 3\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 1}\n",
      "Inserted 1 chunks\n",
      "https://developer.nvidia.com/blog/announcing-helpsteer-an-open-source-dataset-for-building-helpful-llms/ | num chunks: 5\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 1}\n",
      "Inserted 1 chunks\n",
      "https://developer.nvidia.com/blog/bolstering-cybersecurity-how-large-language-models-and-generative-ai-are-transforming-digital-security/ | num chunks: 8\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 1}\n",
      "Inserted 1 chunks\n",
      "https://developer.nvidia.com/blog/accelerate-ai-workflows-for-3d-medical-imaging-with-nvidia-monai-cloud-apis/ | num chunks: 4\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 1}\n",
      "Inserted 1 chunks\n",
      "https://developer.nvidia.com/blog/advanced-api-performance-intrinsics/ | num chunks: 2\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 1}\n",
      "Inserted 1 chunks\n",
      "https://developer.nvidia.com/blog/unlocking-gpu-intrinsics-in-hlsl/ | num chunks: 8\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 1}\n",
      "Inserted 1 chunks\n",
      "https://developer.nvidia.com/blog/transforming-industrial-defect-detection-with-nvidia-tao-and-vision-ai-models/ | num chunks: 8\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 1}\n",
      "Inserted 1 chunks\n",
      "https://developer.nvidia.com/blog/boosting-custom-ros-graphs-using-nvidia-isaac-transport-for-ros/ | num chunks: 8\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 1}\n",
      "Inserted 1 chunks\n",
      "https://developer.nvidia.com/blog/mastering-llm-techniques-inference-optimization/ | num chunks: 25\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 1}\n",
      "Inserted 1 chunks\n",
      "https://developer.nvidia.com/blog/unlock-the-power-of-nvidia-grace-and-nvidia-hopper-architectures-with-foundational-hpc-software/ | num chunks: 7\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 1}\n",
      "Inserted 1 chunks\n",
      "https://developer.nvidia.com/blog/mastering-llm-techniques-training/ | num chunks: 14\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 1}\n",
      "Inserted 1 chunks\n",
      "https://developer.nvidia.com/blog/best-practices-for-securing-llm-enabled-applications/ | num chunks: 10\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 1}\n",
      "Inserted 1 chunks\n",
      "https://developer.nvidia.com/blog/mastering-llm-techniques-llmops/ | num chunks: 11\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 1}\n",
      "Inserted 1 chunks\n",
      "https://developer.nvidia.com/blog/deploy-large-language-models-at-the-edge-with-nvidia-igx-orin-developer-kit/ | num chunks: 6\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 1}\n",
      "Inserted 1 chunks\n",
      "https://developer.nvidia.com/blog/build-custom-enterprise-grade-generative-ai-with-nvidia-ai-foundation-models/ | num chunks: 8\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 1}\n",
      "Inserted 1 chunks\n",
      "https://developer.nvidia.com/blog/elevate-enterprise-generative-ai-app-development-with-nvidia-ai-on-azure-machine-learning/ | num chunks: 5\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 1}\n",
      "Inserted 1 chunks\n",
      "https://developer.nvidia.com/blog/nvidia-ai-foundation-models-build-custom-enterprise-chatbots-and-co-pilots-with-production-ready-llms/ | num chunks: 16\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 1}\n",
      "Inserted 1 chunks\n",
      "https://developer.nvidia.com/blog/accelerating-ptychography-workflows-with-nvidia-holoscan-at-diamond-light-source/ | num chunks: 2\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 1}\n",
      "Inserted 1 chunks\n",
      "https://developer.nvidia.com/blog/energy-efficiency-in-high-performance-computing-balancing-speed-and-sustainability/ | num chunks: 17\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 1}\n",
      "Inserted 1 chunks\n",
      "https://developer.nvidia.com/blog/nvidia-deep-learning-institute-launches-science-and-engineering-teaching-kit/ | num chunks: 3\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 1}\n",
      "Inserted 1 chunks\n",
      "https://developer.nvidia.com/blog/simplifying-gpu-programming-for-hpc-with-the-nvidia-grace-hopper-superchip/ | num chunks: 12\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 1}\n",
      "Inserted 1 chunks\n",
      "https://developer.nvidia.com/blog/using-synthetic-data-to-address-novel-viewpoints-for-autonomous-vehicle-perception/ | num chunks: 5\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 1}\n",
      "Inserted 1 chunks\n",
      "https://developer.nvidia.com/blog/optimize-energy-efficiency-of-multi-node-vasp-simulations-with-nvidia-magnum-io/ | num chunks: 14\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 1}\n",
      "Inserted 1 chunks\n",
      "https://developer.nvidia.com/blog/enabling-greater-patient-specific-cardiovascular-care-with-ai-surrogates/ | num chunks: 4\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 1}\n",
      "Inserted 1 chunks\n",
      "https://developer.nvidia.com/blog/accelerating-neurosymbolic-ai-with-rapids-and-vadalog-parallel/ | num chunks: 11\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 1}\n",
      "Inserted 1 chunks\n",
      "https://developer.nvidia.com/blog/whole-human-brain-neuro-mapping-at-cellular-resolution-on-dgx/ | num chunks: 4\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 1}\n",
      "Inserted 1 chunks\n",
      "https://developer.nvidia.com/blog/setting-new-records-at-data-center-scale-using-nvidia-h100-gpus-and-quantum-2-infiniband/ | num chunks: 25\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 1}\n",
      "Inserted 1 chunks\n",
      "https://developer.nvidia.com/blog/accelerating-networkx-on-nvidia-gpus-for-high-performance-graph-analytics/ | num chunks: 11\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 1}\n",
      "Inserted 1 chunks\n",
      "https://developer.nvidia.com/blog/rapids-cudf-accelerates-pandas-nearly-150x-with-zero-code-changes/ | num chunks: 4\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 1}\n",
      "Inserted 1 chunks\n",
      "https://developer.nvidia.com/blog/cuda-accelerated-robot-motion-generation-in-milliseconds-with-curobo/ | num chunks: 2\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 1}\n",
      "Inserted 1 chunks\n",
      "https://developer.nvidia.com/blog/getting-started-with-large-language-models-for-enterprise-solutions/ | num chunks: 21\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 1}\n",
      "Inserted 1 chunks\n",
      "https://developer.nvidia.com/blog/video-exploring-speech-ai-from-research-to-practical-production-applications/ | num chunks: 3\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 1}\n",
      "Inserted 1 chunks\n",
      "https://developer.nvidia.com/blog/level-up-your-lighting-qa-with-lighting-artist-ted-mebratu/ | num chunks: 2\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 1}\n",
      "Inserted 1 chunks\n",
      "https://developer.nvidia.com/blog/join-the-first-nvidia-llm-developer-day-elevate-your-app-building-skills/ | num chunks: 3\n",
      "200\n",
      "Deleted ids reponse: {'items_deleted': 1}\n",
      "Inserted 1 chunks\n"
     ]
    }
   ],
   "source": [
    "start = time.perf_counter()\n",
    "\n",
    "# If this were not in Jupyter we would run this\n",
    "# asyncio.run(main())\n",
    "\n",
    "# Since we are in a notebook, Jupyter is already running its own event loop\n",
    "# so we can just simply await main()\n",
    "await main()\n",
    "\n",
    "end = time.perf_counter()\n",
    "\n",
    "print(f\"Took {end - start} seconds\")\n",
    "\n",
    "# This should take around 2-3 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the summaries as a json file\n",
    "with open(\"data/techblogs_summaries/saved.json\", \"w\") as f:\n",
    "    json.dump(saved_summaries, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(saved_summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "saved_summaries[payloads[0]['additional_metadata']['document_url']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "techblogs_summaries_assettype = None\n",
    "\n",
    "for assettype in asset_types_json:\n",
    "    if assettype[\"name\"] ==\"summarize_techblogs\":\n",
    "        techblogs_summaries_assettype = assettype\n",
    "\n",
    "print(json.dumps(techblogs_summaries_assettype, indent=2))\n",
    "\n",
    "techblogs_summaries_assettype[\"chunking_params\"] = json.dumps(\n",
    "    {\n",
    "        \"strategy\": \"summarization\",\n",
    "        \"code_behavior\": \"remove_code_sections\",\n",
    "    }\n",
    ")\n",
    "\n",
    "print(json.dumps(techblogs_summaries_assettype, indent=2))\n",
    "\n",
    "\n",
    "update_asset_types_url = \"http://router:5006/asset-types/update\"\n",
    "response = httpx.post(update_asset_types_url, json={\"data\": techblogs_summaries_assettype})\n",
    "print(json.dumps(response.json(), indent=2))\n",
    "\n",
    "dump_response = httpx.post(\"http://router:5006/data/dump\")\n",
    "print(json.dumps(dump_response.json(), indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recap\n",
    "\n",
    "We now two indexes in Redis: `techblogs`, which contains chunks of roughly 250 words, and `summarize_techblogs` which contains the summaries written by ChatGPT.\n",
    "\n",
    "In the next notebook, we'll look at how we can evaluate the search results from these indexes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Lesson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Move to the next lesson by double-clicking *Lesson 03.ipynb* on the file-viewer on the left-hand side of your Jupyter Lab environment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
