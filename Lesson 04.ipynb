{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Techniques for Improving the Effectiveness of RAG Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the cell below to load the video presentation that accompanies this notebook, and watch it before working through the materials in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video controls width=\"640\" height=\"360\">\n",
       "    <source src=\"https://d36m44n9vdbmda.cloudfront.net/assets/s-fx-20-v1/lesson-04.mp4\" type=\"video/mp4\">\n",
       "    Your browser does not support the video tag.\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "video_url = \"https://d36m44n9vdbmda.cloudfront.net/assets/s-fx-20-v1/lesson-04.mp4\"\n",
    "\n",
    "video_html = f\"\"\"\n",
    "<video controls width=\"640\" height=\"360\">\n",
    "    <source src=\"{video_url}\" type=\"video/mp4\">\n",
    "    Your browser does not support the video tag.\n",
    "</video>\n",
    "\"\"\"\n",
    "\n",
    "display(HTML(video_html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 04: Better Generations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've covered data exploration and chunking strategies, loading vector databases for retrieval, and comparing and evaluating retrieval performance. We're now ready to create the final product of this course, a functioning RAG web application.\n",
    "\n",
    "We will build our application to be capable of multiple features:\n",
    "- **Asset Discovery and Summarization**: our app should take a given topic from a user's query and find NVIDIA resources that are relevant to that topic and summarize the search results.\n",
    "- **Question Answering**: our app should be able to answer a specific user question based on details it can extract from NVIDIA resources.\n",
    "- **Coding Assistant**: our app should also be able to detect if the user wants it to write some code for them.\n",
    "\n",
    "Each of these features should be handled differently for the system. In our case, we have two indices in our database: one is comprised of chunks of the actual contents of the blog posts, and the other is comprised of chunks of summaries of the blog posts. The Asset Discovery and Summarization task is better suited to the summary index, whereas the Question Answering and Coding Assistant tasks are better suited to the chunks pulled directly from the blogs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "<img src=\"img/blog-chunks.png\" width=\"600\" alt=\"Blog Chunks\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This notebook will focus on the app UI, search, and LLM.**\n",
    "\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "<img src=\"img/04_overview.png\" width=\"850\" alt=\"architecture diagram with app UI, search, and LLM components highlighted\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tailoring LLMs for RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we prompt our LLM to carry out these tasks, our prompts will vary depending on the task. The prompt for question answering will differ from the discovery/summarization prompt considerably.\n",
    "\n",
    "In fact, it might not be just the prompt that differs based on the task! We might have multiple different *models* that carry out different tasks. Models like GPT-4 and the recent variant of Mistral known as \"Mixtral\" are Mixture-of-Experts models: specialized sub-models take over for tasks that they are best suited for. Similarly, if we wanted to have a model tailored to summarize our domain's content, we'd want some way to route only summarization requests to this fine-tuned model, and not question-answering requests.\n",
    "\n",
    "We can accomplish that tailoring through parameter-efficient finetuning (PEFT), which requires much less data and compute than full finetuning of an LLM. See the session [Tailoring LLMs to Your Use Case](https://www.nvidia.com/en-us/on-demand/session/llmdevday23-02/#:~:text=Push%20LLMs%20beyond%20the%20quality,practical%2C%20real%2Dworld%20examples.) from NVIDIA's recent LLM Developer Day to learn more about customizing LLMs. During that recorded session, we showed how to p-tune a GPT model, and how to use low-rank adaptation (LoRA) with a Mistral 7B model; both are forms of PEFT. For this lesson, however, we'll stick with out-of-the-box (OOB), general-purpose GPT, just with different prompts for each task.\n",
    "\n",
    "NVIDIA Deep Learning Institute also has an enterprise workshop *[Efficient LLM Customization](https://courses.nvidia.com/courses/course-v1:DLI+C-FX-10+V2/)* which takes a deep dive into performing PEFT on several models for a variety of tasks, as well as several methods for synthetic data generation in service of PEFT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restart the Services"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make sure you're staring this lesson with all your services in the correct state, please restart them by running the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bringing containerized services down...\n",
      "Services down.\n",
      "Bringing containerized services back up...\n",
      "Services back up.\n"
     ]
    }
   ],
   "source": [
    "!./restart.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User Intent Classification"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to make our generation process more modular, we will need some sort of classification model that sits in front. This will classify the user's intent.\n",
    "\n",
    "Here we import a `ChatOpenAI` instance of our local NIM Mixtral 8x7B model configured and ready for use with LangChain from an [`llms` helper file](llms.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llms import llms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = llms.nim_mixtral_llm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional Remote LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optionally, instead of using our local model, you can also use either NVIDIA AI Foundation's Mixtral 8x7B model or OpenAI's gpt-3.5-turbo.\n",
    "\n",
    "For either of these 2 options you'll need an API key. For more details about NVIDIA AI Foundation and obtaining a free API key, see [the notebook *NVIDIA AI Foundation.ipynb*](./NVIDIA%20AI%20Foundation.ipynb).\n",
    "\n",
    "After obtaining an appropriate API key, uncomment the appropriate cell below, add your API key, and run the cell to set `llm` to the remote LLM you chose to work with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NVIDIA AI Foundation Mixtral 8x7B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from llms import set_api_key\n",
    "# set_api_key('NVIDIA_API_KEY', '<your_nvidia_api_key>')\n",
    "# llm = llms.nvai_mixtral_llm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OpenAI GPT-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from llms import set_api_key\n",
    "# set_api_key('OPENAI_API_KEY', '<your_openai_api_key>')\n",
    "# llm = llms.openai_gpt3_llm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to use an OOB LLM and a few-shot prompt to do our classification. This is not ideal, as mentioned earlier--rather than pay for all these tokens being sent in every few-shot prompt, consider modifying this notebook to use a fine-tuned model!\n",
    "\n",
    "While it may not be cost-effective to use a large language model on the scale of GPT or Mistral for classification, consider the tradeoff between speed-of-development/time-to-market and application speed/cost. Using a larger general-purpose LLM and combining it with a few-shot prompt is often the quickest way to get something up and running, generating training data for your embedder. Later, you can optimize to save costs and reduce latency. Plus, when you use an LLM, you can easily adjust the prompt on the fly if you want to add a new category, for instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from langchain_core.messages import AIMessage, SystemMessage, HumanMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, HumanMessagePromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_json(text):\n",
    "    stack = []\n",
    "    start_index = None\n",
    "\n",
    "    for i, char in enumerate(text):\n",
    "        if char == '{':\n",
    "            if not stack:\n",
    "                start_index = i\n",
    "            stack.append(char)\n",
    "        elif char == '}':\n",
    "            if stack:\n",
    "                stack.pop()\n",
    "                if not stack:\n",
    "                    end_index = i + 1\n",
    "                    json_str = text[start_index:end_index]\n",
    "                    try:\n",
    "                        json_obj = json.loads(json_str)\n",
    "                        return json_obj\n",
    "                    except json.JSONDecodeError:\n",
    "                        print(\"Error: JSON decoding failed.\")\n",
    "                        return None\n",
    "            else:\n",
    "                print(\"Error: Unmatched '}' character.\")\n",
    "                return None\n",
    "\n",
    "    print(\"No JSON object found in the text.\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'is_user_question': True, 'asks_for_code': False}\n"
     ]
    }
   ],
   "source": [
    "system_message = \"You are a helpful AI bot being used in a technical domain. Format your output as a JSON object.\"\n",
    "human_msg_pt = HumanMessagePromptTemplate.from_template(\n",
    "    'First, is the following text a user question that needs answering or just a topic to learn more about? Second, if the text is a user question that needs answering, is the question asking for code to be written?\\nText: {text}'\n",
    ")\n",
    "# three classification categories\n",
    "code_question = AIMessage(content=\"{\\n  \\\"is_user_question\\\": true,\\n  \\\"asks_for_code\\\": true\\n}\")\n",
    "regular_question = AIMessage(content=\"{\\n  \\\"is_user_question\\\": true,\\n  \\\"asks_for_code\\\": false\\n}\")\n",
    "not_question = AIMessage(content=\"{\\n  \\\"is_user_question\\\": false\\n}\")\n",
    "\n",
    "prompt = ChatPromptTemplate(\n",
    "    messages=[\n",
    "        SystemMessage(content=system_message),\n",
    "        human_msg_pt.format(text=\"how do I install cuda drivers\"),\n",
    "        code_question,\n",
    "        human_msg_pt.format(text=\"what is the right NVIDIA SDK to use for computer vision\"),\n",
    "        regular_question,\n",
    "        human_msg_pt.format(text=\"recommender systems for online shopping\"),\n",
    "        not_question,\n",
    "        human_msg_pt.format(text=\"How to import rapids cudf in python?\"),\n",
    "        code_question,\n",
    "        human_msg_pt.format(text=\"Generate code to make a Python web server.\"),\n",
    "        code_question,\n",
    "        human_msg_pt.format(text=\"biomedical devices\"),\n",
    "        not_question,\n",
    "        human_msg_pt.format(text=\"write some code that prints hello world\"),\n",
    "        code_question,\n",
    "        human_msg_pt.format(text=\"The leading cause of death in the 16th century was infection.\"),\n",
    "        not_question,\n",
    "        human_msg_pt.format(text=\"NVIDIA Merlin SDK for recommendation systems\"),\n",
    "        not_question,\n",
    "        human_msg_pt.format(text=\"who founded the company NVIDIA?\"),\n",
    "        regular_question,\n",
    "        human_msg_pt,\n",
    "    ]\n",
    ")\n",
    "chain = prompt | llm\n",
    "\n",
    "generation = chain.invoke({\"text\": \"what libraries should I learn in C++\"})\n",
    "print(extract_json(generation.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'is_user_question': True, 'asks_for_code': False}\n"
     ]
    }
   ],
   "source": [
    "generation = chain.invoke({\"text\": \"What is a major seventh chord?\"})\n",
    "print(extract_json(generation.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'is_user_question': False}\n"
     ]
    }
   ],
   "source": [
    "generation = chain.invoke({\"text\": \"omniverse scene lighting\"})\n",
    "print(extract_json(generation.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'is_user_question': False}\n"
     ]
    }
   ],
   "source": [
    "generation = chain.invoke({\"text\": \"Deep learning techniques for obstacle avoidance in autonomous mobile robots\"})\n",
    "print(extract_json(generation.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'is_user_question': True, 'asks_for_code': True}\n"
     ]
    }
   ],
   "source": [
    "generation = chain.invoke({\"text\": \"Generate code to write a simple Python web app.\"})\n",
    "print(extract_json(generation.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'is_user_question': True, 'asks_for_code': True}\n"
     ]
    }
   ],
   "source": [
    "generation = chain.invoke({\"text\": \"how would you write a print statement in C++\"})\n",
    "print(extract_json(generation.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like our classifier is doing its job."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Service"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're going to discuss our final service, which we're going to call `web` for Web App. You launched this service in Lesson 00."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36mweb-1  | \u001b[0m[2024-08-12 15:01:31 +0000] [7] [INFO] Starting gunicorn 21.2.0\n",
      "\u001b[36mweb-1  | \u001b[0m[2024-08-12 15:01:31 +0000] [7] [INFO] Listening at: http://0.0.0.0:5000 (7)\n",
      "\u001b[36mweb-1  | \u001b[0m[2024-08-12 15:01:31 +0000] [7] [INFO] Using worker: gevent\n",
      "\u001b[36mweb-1  | \u001b[0m[2024-08-12 15:01:31 +0000] [9] [INFO] Booting worker with pid: 9\n",
      "\u001b[36mweb-1  | \u001b[0m[2024-08-12 15:01:31 +0000] [10] [INFO] Booting worker with pid: 10\n",
      "\u001b[36mweb-1  | \u001b[0m[2024-08-12 15:01:31 +0000] [11] [INFO] Booting worker with pid: 11\n"
     ]
    }
   ],
   "source": [
    "!docker-compose logs web"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to have three different modes to our web app, based on the classified intent in the user's query. Each one will map to a separate prompt, with different styles of chunks being searched for as context depending on the classification.\n",
    "1. When a user's query is detected to not be in question form, we're going to search the summarize_techblogs` index to discover which assets might be relevant. We're then going to additionally summarize over all those assets.\n",
    "2. When a user's query is detected to be in question form, but not a code question, we're going to search the `techblogs` index and pull the non-code text to inject into the LLM prompt as context.\n",
    "3. When a user's query is detected to be a question that asks the LLM to write code, we're going to search the `techblogs` index and grab the code data we stored as metadata. That's why we did all that extra work back in the chunking stage!\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "<img src=\"img/end-to-end.png\" width=\"600\" alt=\"End-to-End\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Viewing the Web App"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final product `web` app is available on port 5000. Execute the following cell to generate a link to open it in a new browser tab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "var host = window.location.host;\n",
       "var url = 'http://'+host+':5000';\n",
       "element.innerHTML = '<a style=\"color:green;\" target=\"_blank\" href='+url+'>Click to open the final product web app.</a>';\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%js\n",
    "var host = window.location.host;\n",
    "var url = 'http://'+host+':5000';\n",
    "element.innerHTML = '<a style=\"color:green;\" target=\"_blank\" href='+url+'>Click to open the final product web app.</a>';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try the Web App"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try a few different examples of the three types of queries, such as the two built-in examples in the buttons. These should lead to summarization responses.\n",
    "\n",
    "Now try some QA questions. Here are some examples that are answerable based on the documents we indexed:\n",
    "- Which musician worked with the company Moment Factory on her world tour?\n",
    "- Write me some code using cgroups to isolate a GPU\n",
    "- What is NVIDIA Workbench?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Citing Sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One feature we added to this app is the ability to cite sources used for summarization and question answering. This is due to some instructions we added to our prompt, which you can check out at `web/src/chains.py`. With a bit of string replacement, we can turn those source numbers into links that cause the page to scroll to the proper source. \n",
    "\n",
    "You may have seen something similar in Bing search or Perplexity AI's search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Discussion Topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query Transformation\n",
    "We could expand short queries to be more detailed, adding context from what we know about the user, including their role or previous interests. Alternatively we could condense really long queries to a more manageable size. Query transformation could even enable keyword search when appropriate.\n",
    "\n",
    "### Quantitative Analysis of Generation\n",
    "We covered Precision and Recall as retrieval metrics, but it also makes sense to evaluate your LLM generation quantitatively. The primary metrics we typically consider are faithfulness (does the generation match the retrieved data) and answer relevancy (does the generation match the query intent). Refer to the \n",
    "[RAGAS framework](https://docs.ragas.io/en/stable/concepts/metrics/index.html) for further guidance on using an LLM to evaluate generation performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recap and Final Thoughts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the cell below to load and then watch the course conclusion video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video controls width=\"640\" height=\"360\">\n",
       "    <source src=\"https://d36m44n9vdbmda.cloudfront.net/assets/s-fx-20-v1/conclusion.mp4\" type=\"video/mp4\">\n",
       "    Your browser does not support the video tag.\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "video_url = \"https://d36m44n9vdbmda.cloudfront.net/assets/s-fx-20-v1/conclusion.mp4\"\n",
    "\n",
    "video_html = f\"\"\"\n",
    "<video controls width=\"640\" height=\"360\">\n",
    "    <source src=\"{video_url}\" type=\"video/mp4\">\n",
    "    Your browser does not support the video tag.\n",
    "</video>\n",
    "\"\"\"\n",
    "\n",
    "display(HTML(video_html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We hope this rapid-fire intro left you with some valuable techniques for improving RAG systems. We provided a lot of code you're free to use, but more importantly, we hope these ideas will help you think about how to design and evaluate a useful system that goes beyond naive RAG. \n",
    "\n",
    "To recap:\n",
    "- Build your RAG system in a modular way. Containerize each component so you can scale it as needed. Don't build monoliths.\n",
    "- Match your RAG system's task to the data you have at your disposal. Consider the features of your data, like the presence of code, or implicit structure from HTML.\n",
    "- Try various different chunking strategies, and consider looking into more advanced strategies as suggested in Lesson 01. Generate different indices corresponding to different chunking strategies so you can compare them. \n",
    "- Try both semantic and keyword search. Consider combining them for hybrid search. \n",
    "- Evaluate the precision/recall of your retrieval system using human-as-a-judge and LLM-as-a-judge frameworks.\n",
    "- If you have a variety of tasks your system can perform, use multiple prompts, multiple retrieval indexes for context (mapping to different chunking strategies), and potentially even multiple models. \n",
    "Consider our approach of using a classifier to route to the appropriate expert model.\n",
    "\n",
    "Thank you, and we'll see you in your next DLI!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
