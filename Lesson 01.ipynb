{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Techniques for Improving the Effectiveness of RAG Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the cell below to load the video presentation that accompanies this notebook, and watch it before working through the materials in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video controls width=\"640\" height=\"360\">\n",
       "    <source src=\"https://d36m44n9vdbmda.cloudfront.net/assets/s-fx-20-v1/lesson-01.mp4\" type=\"video/mp4\">\n",
       "    Your browser does not support the video tag.\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "video_url = \"https://d36m44n9vdbmda.cloudfront.net/assets/s-fx-20-v1/lesson-01.mp4\"\n",
    "\n",
    "video_html = f\"\"\"\n",
    "<video controls width=\"640\" height=\"360\">\n",
    "    <source src=\"{video_url}\" type=\"video/mp4\">\n",
    "    Your browser does not support the video tag.\n",
    "</video>\n",
    "\"\"\"\n",
    "\n",
    "display(HTML(video_html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 01: Exploring and Preparing your Dataset for Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Large language models (LLMs) are powerful tools for solving real-world problems--as long as they have the information necessary to help. Useful LLM-based systems need to be able to integrate new information reliably and quickly. Both of those goals can be achieved via retrieval-augmented generation (RAG): retrieving information from a data source and injecting it into the LLM's prompt.\n",
    "\n",
    "This greatly simplifies the problem that the LLM has to solve. Instead of requiring the information to have been available during training, the LLM can simply \"read\" the sources it receives as context and use them to find a solution. \n",
    "\n",
    "This course is a practical guide to building more effective RAG applications. RAG systems show great promise, but the simplest version of RAG--the one most often discussed in typical online tutorials--can struggle with anything other than basic queries. This course will show how your RAG system design can significantly improve performance at whatever task you choose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This notebook will focus on the chunker and LLM.**\n",
    "\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "<img src=\"img/01_overview.png\" width=\"850\" alt=\"architecture with router, chunker, and LLM highlighted\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Sources: NVIDIA Tech Blog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many of you will probably be looking to build a RAG system suited to your particular domain. We're no different, and so for this course, we'll be using NVIDIA materials as our data source. In particular, we'll be using HTML articles from the [NVIDIA Tech Blog](https://developer.nvidia.com/blog).\n",
    "\n",
    "This data source contains thousands of articles written by NVIDIAns on a number of different topics. Some articles are deep technical walkthroughs with lots of code samples. Other articles are more like news pieces that announce a new SDK release or feature. There is a pretty wide range of NVIDIA-related content, because our technology is used in so many different industries.\n",
    "\n",
    "We can do many different things with that data source--but first, we will explore it to figure out what kind of tasks we ultimately want our RAG application to accomplish."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TechBlogs are HTML pages from a Wordpress Blog. They have a leading title, and then are often further divided into sections, which can be one or more paragraphs. The blogs contain a mix of regular text, code, and images.\n",
    "\n",
    "Have a look at a few of these articles using these links: \n",
    "- https://developer.nvidia.com/blog/create-share-and-scale-enterprise-ai-workflows-with-nvidia-ai-workbench-now-in-beta/\n",
    "- https://developer.nvidia.com/blog/improving-cuda-initialization-times-using-cgroups-in-certain-scenarios/\n",
    "- https://developer.nvidia.com/blog/bringing-generative-ai-to-the-edge-with-nvidia-metropolis-microservices-for-jetson/\n",
    "\n",
    "If we \"embed\" the text of each post, we get a floating-point vector with hundreds or even thousands of dimensions quantifying the \"meaning\" of each blog post. To visualize this, we can reduce the dimensionality of these vectors so each blog is now represented by a 3D point which we can easily plot. Note how the embeddings naturally cluster the blogs, and we've color coded the clusters. Orange might represent blogs in the realm of healthcare and life-sciences. Magenta might capture blogs within the realm of robotics, etc.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "<img src=\"img/constellation.gif\" width=\"600\" alt=\"Constellation\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download NVIDIA Tech Blogs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To save time and make sure we're all using data that isn't date-dependent, we've already downloaded 200 blog posts from the Wordpress API. The response from that API has been saved in the directory `data/techblogs/`.\n",
    "\n",
    "If you'd like at a later time to download more, or more recent blog posts, you can use the cells in this section to do so."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we import the required Python libraries for downloading the NVIDIA Tech Blog data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import json\n",
    "import asyncio\n",
    "import httpx\n",
    "import time\n",
    "import shutil\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restart the Services"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make sure you're staring this lesson with all your services in the correct state, please restart them by running the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bringing containerized services down...\n",
      "Services down.\n",
      "Bringing containerized services back up...\n",
      "Services back up.\n"
     ]
    }
   ],
   "source": [
    "!./restart.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Download Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function will download the latest blogs from the Wordpress API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "POSTS_PER_PAGE = 25  # using 100 can cause HTML response to be too long so that the text gets terminated\n",
    "MAX_PAGE = 8  # setting to 8 so that we get 200 pages total. Increase to download more articles\n",
    "\n",
    "def download(session, headers, wp, data_dir):\n",
    "    current_page = 1\n",
    "    download_complete = False\n",
    "    now = datetime.now()\n",
    "    start_timecode = f\"{now.year}{now.month}{now.day}{now.hour}{now.minute}{now.second}\"\n",
    "    padding_width = math.ceil(math.log(MAX_PAGE, 10))\n",
    "\n",
    "    print(f\"Downloading up to {MAX_PAGE * POSTS_PER_PAGE} posts...\")\n",
    "    while (not download_complete) and (\n",
    "        current_page <= MAX_PAGE\n",
    "    ):  # <= because pages are 1-indexed\n",
    "        response = session.get(\n",
    "            f\"https://{wp}/wp-json/wp/v2/posts?page={current_page}&per_page={POSTS_PER_PAGE}\",\n",
    "            headers=headers,\n",
    "        )\n",
    "        if response.status_code == 200:\n",
    "            response_json = response.json()\n",
    "            with open(\n",
    "                os.path.join(\n",
    "                    data_dir,\n",
    "                    f\"{start_timecode}_{str(current_page).zfill(padding_width)}.json\",\n",
    "                ),\n",
    "                \"w\",\n",
    "            ) as dump_file:\n",
    "                json.dump(response_json, dump_file)\n",
    "\n",
    "            print(f\"Page {current_page}. Downloaded {len(response_json)} posts\")\n",
    "\n",
    "            if len(response_json) < POSTS_PER_PAGE:\n",
    "                download_complete = True\n",
    "                print(\n",
    "                    f\"Downloaded all ({POSTS_PER_PAGE * (current_page - 1) + len(response_json)} posts)\"\n",
    "                )\n",
    "            else:\n",
    "                current_page += 1\n",
    "\n",
    "        else:\n",
    "            print(\n",
    "                f\"Download of page {current_page} failed with status code {response.status_code}. {response.text}\"\n",
    "            )\n",
    "            download_complete = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Data Directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although we have already created it in this environment, the following is how we would create the `data/techblogs` directory to download the blog posts into."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.path.join(os.getcwd(), 'data', 'techblogs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download NVDIA Blog Posts"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the course is over, if you want to try downloading more articles, uncomment the code below and run the cells again. You can also, if you wish, change the `MAX_PAGE` constant above. If you do, be sure to rerun the cell that defines it before running the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment these lines below if you wanted to redownload\n",
    "# shutil.rmtree(data_dir)\n",
    "# os.makedirs(data_dir, exist_ok=True)\n",
    "# download(\n",
    "#     session=httpx.Client(),\n",
    "#     headers={\n",
    "#         \"user-agent\": \"Mozilla/5.0 (X11; Linux i686; rv:10.0) Gecko/20100101 Firefox/10.0\",\n",
    "#         \"Accept-Encoding\": \"gzip, deflate, br\",\n",
    "#     },\n",
    "#     wp=\"developer.nvidia.com/blog\",\n",
    "#     data_dir=data_dir,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Organize Tech Blogs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's organize all our downloaded data into a simple dictionary where the key is the URL, just for the purposes of easier access of a few examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list = [x for x in sorted(os.listdir(data_dir)) if '.json' in x]\n",
    "\n",
    "techblogs_dict = {}\n",
    "\n",
    "for i, filename in enumerate(file_list):\n",
    "    with open(os.path.join(data_dir, filename), 'r') as in_file:\n",
    "        data = json.load(in_file)\n",
    "    for item in data:\n",
    "        # skip items that do not link to developer.nvidia.com/blog or blogs.nvidia.com\n",
    "        if not item['link'].startswith(\"https://developer.nvidia.com/blog\"): # and not item['link'].startswith(\"https://blogs.nvidia.com\"):\n",
    "            continue\n",
    "        document_title = item['title']['rendered']\n",
    "        document_url = item['link']\n",
    "        document_html = item['content']['rendered']\n",
    "        document_date = item['date_gmt']\n",
    "        document_date_modified = item['modified_gmt']\n",
    "\n",
    "        techblogs_dict[document_url] = item\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are a few of the URLs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://developer.nvidia.com/blog/create-share-and-scale-enterprise-ai-workflows-with-nvidia-ai-workbench-now-in-beta/',\n",
       " 'https://developer.nvidia.com/blog/modernizing-the-data-center-with-accelerated-networking/',\n",
       " 'https://developer.nvidia.com/blog/emulating-the-attention-mechanism-in-transformer-models-with-a-fully-convolutional-network/',\n",
       " 'https://developer.nvidia.com/blog/announcing-metropolis-microservices-on-nvidia-jetson-orin-for-rapid-edge-ai-development/',\n",
       " 'https://developer.nvidia.com/blog/advancing-production-ai-with-nvidia-ai-enterprise/',\n",
       " 'https://developer.nvidia.com/blog/build-enterprise-grade-ai-with-nvidia-ai-software/',\n",
       " 'https://developer.nvidia.com/blog/delivering-efficient-high-performance-ai-clouds-with-nvidia-doca-2-5/',\n",
       " 'https://developer.nvidia.com/blog/using-the-power-of-ai-to-make-factories-safer/',\n",
       " 'https://developer.nvidia.com/blog/simplifying-network-operations-for-ai-with-nvidia-quantum-infiniband/',\n",
       " 'https://developer.nvidia.com/blog/how-to-build-vision-ai-applications-at-the-edge-with-nvidia-metropolis-microservices-and-apis/']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(techblogs_dict.keys())[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore Tech Blogs Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what information we got back from the Wordpress API, before looking later in this notebook at chunking it.\n",
    "\n",
    "We have the actual rendered HTML content, as well as some valuable metadata like when the article was published/modified, the author ID, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 75534,\n",
       " 'date': '2024-01-05T14:14:41',\n",
       " 'date_gmt': '2024-01-05T22:14:41',\n",
       " 'guid': {'rendered': 'https://developer.nvidia.com/blog/?p=75534'},\n",
       " 'modified': '2024-01-11T11:49:33',\n",
       " 'modified_gmt': '2024-01-11T19:49:33',\n",
       " 'slug': 'improving-cuda-initialization-times-using-cgroups-in-certain-scenarios',\n",
       " 'status': 'publish',\n",
       " 'type': 'post',\n",
       " 'link': 'https://developer.nvidia.com/blog/improving-cuda-initialization-times-using-cgroups-in-certain-scenarios/',\n",
       " 'title': {'rendered': 'Improving CUDA Initialization Times Using cgroups in Certain Scenarios'},\n",
       " 'content': {'rendered': '<div style=\"margin-top: 0px; margin-bottom: 0px;\" class=\"sharethis-inline-share-buttons\" ></div>\\n<p>Many CUDA applications running on multi-GPU platforms usually use a single GPU for their compute needs. In such scenarios, a performance penalty is paid by applications because CUDA has to enumerate/initialize all the GPUs on the system. If a CUDA application does not require other GPUs to be visible and accessible, you can launch such applications by isolating the unwanted GPUs from the CUDA process and eliminating unnecessary initialization steps.&nbsp;</p>\\n\\n\\n\\n<p>This post discusses the various methods to accomplish this and their performance benefits.</p>\\n\\n\\n\\n<h2 class=\"wp-block-heading\">GPU isolation</h2>\\n\\n\\n\\n<p>GPU isolation can be achieved on Linux systems by using Linux tools like <code>cgroups</code>. In this section, we first discuss a lower-level approach and then a higher-level possible approach.</p>\\n\\n\\n\\n<p>Another method exposed by CUDA to isolate devices is the use of <code>CUDA_VISIBLE_DEVICES</code>. Although functionally similar, this approach has limited initialization performance gains compared to the <code>cgroups</code> approach.</p>\\n\\n\\n\\n<h3 class=\"wp-block-heading\">Isolating GPUs using cgroups V1</h3>\\n\\n\\n\\n<p>Control groups provide a mechanism for aggregating or partitioning sets of tasks and all their future children into hierarchical groups with specialized behavior. You can use <code>cgroups</code> to control which GPUs are visible to a CUDA process. This ensures that only the GPUs that are needed by the CUDA process are made available to it.</p>\\n\\n\\n\\n<p>The following code provides a low-level example of how to employ <code>cgroups</code> and fully isolate a GPU to a single process. Be aware that you will likely have to run these commands in a root shell to work properly. We show a more convenient, higher-level utility later in this post.</p>\\n\\n\\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: cpp; title: ; notranslate\" title=\"\">\\n# Create a mountpoint for the cgroup hierarchy as root\\n$&gt; cd /mnt\\n$&gt; mkdir cgroupV1Device\\n\\n# Use mount command to mount the hierarchy and attach the device subsystem to it\\n$&gt; mount -t cgroup -o devices devices cgroupV1Device\\n$&gt; cd cgroupV1Device\\n# Now create a gpu subgroup directory to restrict/allow GPU access\\n$&gt; mkdir gpugroup\\n$&gt; cd gpugroup\\n# in the gpugroup, you will see many cgroupfs files, the ones that interest us are tasks, device.deny and device.allow\\n$&gt; ls gpugroup\\ntasks      devices.deny     devices.allow\\n\\n# Launch a shell from where the CUDA process will be executed. Gets the shells PID\\n$&gt; echo $$\\n\\n# Write this PID into the tasks files in the gpugroups folder\\n$&gt; echo &lt;PID&gt; tasks\\n\\n# List the device numbers of nvidia devices with the ls command\\n$&gt; ls -l /dev/nvidia*\\ncrw-rw-rw- 1 root root 195,   0 Jul 11 14:28 /dev/nvidia0\\ncrw-rw-rw- 1 root root 195,   0 Jul 11 14:28 /dev/nvidia1\\n\\n# Assuming that you only want to allow the CUDA process to access GPU0, you deny the CUDA process access to GPU1 by writing the following command to devices.deny\\n$&gt; echo \\'c 195:1 rmw\\' &gt; devices.deny\\n\\n# Now GPU1 will not be visible to The CUDA process that you launch from the second shell.\\n# To provide the CUDA process access to GPU1, we should write the following to devices.allow\\n\\n$&gt; echo \\'c 195:1 rmw\\' &gt; devices.allow\\n</pre></div>\\n\\n\\n<p>When you are done with the tasks, unmount the <code>/cgroupV1Device</code> folder with the umount command.</p>\\n\\n\\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: cpp; title: ; notranslate\" title=\"\">\\numount /mnt/cgroupV1Device\\n</pre></div>\\n\\n\\n<p>To allow or deny the user access to any other GPUs on the system, write those GPU numbers to the appropriate file. Here’s an example of denying access to only GPU5 and GPU6 on a multi-GPU system.</p>\\n\\n\\n\\n<p>In the <code>/gpugroup</code> folder created earlier, write the PID of the shell from where the CUDA process is to be launched into the <code>tasks</code> file:</p>\\n\\n\\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: cpp; title: ; notranslate\" title=\"\">\\n$&gt; echo &lt;PID&gt; tasks\\n</pre></div>\\n\\n\\n<p>Now add GPU5 and GPU6 to the denied list:</p>\\n\\n\\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: cpp; title: ; notranslate\" title=\"\">\\n$&gt; echo \\'c 195:5 rmw\\' &gt; devices.deny\\n$&gt; echo \\'c 195:6 rmw\\' &gt; devices.deny\\n</pre></div>\\n\\n\\n<p>At this point, the CUDA process can’t see or access the two GPUs. To enable only specific GPUs to a CUDA process, those GPUs should be added to the <code>devices.allow</code> file and the rest of the GPUs should be added to the <code>devices.deny</code> file.&nbsp;</p>\\n\\n\\n\\n<p>The access controls apply per process. Multiple processes can be added to the <code>tasks</code> file to propagate the same controls to more than one process.</p>\\n\\n\\n\\n<h3 class=\"wp-block-heading\">Isolating GPUs using the bubblewrap utility</h3>\\n\\n\\n\\n<p>The bubblewrap utility (bwrap) is a higher-level utility available for sandboxing and access control in Linux operating systems, which can be used to achieve the same effect as the solution presented earlier. You can use this to conveniently restrict or allow access to specific GPUs from a CUDA process:</p>\\n\\n\\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: cpp; title: ; notranslate\" title=\"\">\\n# install bubblewrap utility on Debian-like systems\\n$&gt;sudo apt-get install -y bubblewrap\\n\\n# create a simple shell script that uses bubblewap for binding the required GPU to the launched process\\n\\n#!/bin/sh\\n# bwrap.sh\\nGPU=$1;shift   # 0, 1, 2, 3, ..\\nif &#91; &quot;$GPU&quot; = &quot;&quot; ]; then echo &quot;missing arg: gpu id&quot;; exit 1; fi\\nbwrap \\\\\\n        --bind / / \\\\\\n        --dev /dev --dev-bind /dev/nvidiactl /dev/nvidiactl --dev-bind /dev/nvidia-uvm /dev/nvidia-uvm  \\\\\\n        --dev-bind /dev/nvidia$GPU /dev/nvidia$GPU \\\\\\n        &quot;$@&quot;\\n\\n\\n# Launch the CUDA process with the bubblewrap utility to only allow access to a specific GPU while running\\n$&gt; ./bwrap.sh 0 ./test_cuda_app &lt;args&gt;\\n</pre></div>\\n\\n\\n<p>More than one GPU can be exposed to a CUDA process by extending the <code>dev-bind</code> option in the code example.</p>\\n\\n\\n\\n<h2 class=\"wp-block-heading\">Performance benefits of GPU isolation&nbsp;</h2>\\n\\n\\n\\n<p>In this section, we compare the performance of the CUDA driver initialization API (cuInit) with and without GPU isolation, measured over 256 iterations. The APIs are being run on an x86-based machine with four A100 class GPUs.</p>\\n\\n\\n<div class=\"wp-block-image\">\\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"1200\" height=\"742\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/CUDA-initialisation-performance.png\" alt=\"Bar graph shows the performance of cuInit API running on an A104 system with and without GPU isolation using cgroups. The bar on the left shows the performance of cuInit when only a single GPU is exposed to the calling CUDA process via cgroups (~65 ms). The bar on the right shows the performance of cuInit when all four GPUs on the system are made available to the CUDA process (225 ms).\" class=\"wp-image-75547\" title=\"Chart\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/CUDA-initialisation-performance.png 1200w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/CUDA-initialisation-performance-300x186.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/CUDA-initialisation-performance-625x386.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/CUDA-initialisation-performance-179x111.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/CUDA-initialisation-performance-768x475.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/CUDA-initialisation-performance-645x399.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/CUDA-initialisation-performance-485x300.png 485w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/CUDA-initialisation-performance-146x90.png 146w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/CUDA-initialisation-performance-362x224.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/CUDA-initialisation-performance-178x110.png 178w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/CUDA-initialisation-performance-1024x633.png 1024w\" sizes=\"(max-width: 1200px) 100vw, 1200px\" /><figcaption class=\"wp-element-caption\"><em>Figure 1. CUDA initialisation performance comparison between a </em>cgroup<em>-constrained process and the default scenario on a four-GPU test system</em></figcaption></figure></div>\\n\\n\\n<h2 class=\"wp-block-heading\">Summary</h2>\\n\\n\\n\\n<p>GPU isolation using <code>cgroups</code> offers you the option of improving CUDA initialization times in a limited number of use cases where all the GPUs on the system are not required to be used by a given CUDA process.</p>\\n\\n\\n\\n<p>For more information, see the following resources:</p>\\n\\n\\n\\n<ul>\\n<li><a href=\"https://docs.kernel.org/admin-guide/cgroup-v1/index.html\">Control Groups version 1 — The Linux Kernel documentation</a></li>\\n\\n\\n\\n<li><a href=\"https://docs.nvidia.com/cuda/cuda-driver-api/group__CUDA__INITIALIZE.html\">cuInit</a></li>\\n</ul>\\n',\n",
       "  'protected': False},\n",
       " 'excerpt': {'rendered': '<p>Many CUDA applications running on multi-GPU platforms usually use a single GPU for their compute needs. In such scenarios, a performance penalty is paid by applications because CUDA has to enumerate/initialize all the GPUs on the system. If a CUDA application does not require other GPUs to be visible and accessible, you can launch such &hellip; <a href=\"https://developer.nvidia.com/blog/improving-cuda-initialization-times-using-cgroups-in-certain-scenarios/\">Continued</a></p>\\n',\n",
       "  'protected': False},\n",
       " 'author': 1968,\n",
       " 'featured_media': 75549,\n",
       " 'comment_status': 'open',\n",
       " 'ping_status': 'open',\n",
       " 'sticky': False,\n",
       " 'template': '',\n",
       " 'format': 'standard',\n",
       " 'meta': {'publish_to_discourse': '',\n",
       "  'publish_post_category': '318',\n",
       "  'wpdc_auto_publish_overridden': '',\n",
       "  'wpdc_topic_tags': '',\n",
       "  'wpdc_pin_topic': '',\n",
       "  'wpdc_pin_until': '',\n",
       "  'discourse_post_id': '1327681',\n",
       "  'discourse_permalink': 'https://forums.developer.nvidia.com/t/improving-cuda-initialization-times-using-cgroups-in-certain-scenarios/277990',\n",
       "  'wpdc_publishing_response': 'success',\n",
       "  'wpdc_publishing_error': '',\n",
       "  'footnotes': '',\n",
       "  '_links_to': '',\n",
       "  '_links_to_target': ''},\n",
       " 'categories': [852, 696, 503],\n",
       " 'tags': [453, 126, 1914, 796],\n",
       " 'acf': [],\n",
       " 'jetpack_featured_media_url': 'https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/hpc-featured.jpg',\n",
       " 'jetpack_shortlink': 'https://wp.me/pcCQAL-jEi',\n",
       " 'jetpack_likes_enabled': True,\n",
       " 'jetpack_sharing_enabled': True,\n",
       " '_links': {'self': [{'href': 'https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/75534'}],\n",
       "  'collection': [{'href': 'https://developer-blogs.nvidia.com/wp-json/wp/v2/posts'}],\n",
       "  'about': [{'href': 'https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post'}],\n",
       "  'author': [{'embeddable': True,\n",
       "    'href': 'https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1968'}],\n",
       "  'replies': [{'embeddable': True,\n",
       "    'href': 'https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=75534'}],\n",
       "  'version-history': [{'count': 9,\n",
       "    'href': 'https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/75534/revisions'}],\n",
       "  'predecessor-version': [{'id': 76259,\n",
       "    'href': 'https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/75534/revisions/76259'}],\n",
       "  'wp:featuredmedia': [{'embeddable': True,\n",
       "    'href': 'https://developer-blogs.nvidia.com/wp-json/wp/v2/media/75549'}],\n",
       "  'wp:attachment': [{'href': 'https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=75534'}],\n",
       "  'wp:term': [{'taxonomy': 'category',\n",
       "    'embeddable': True,\n",
       "    'href': 'https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=75534'},\n",
       "   {'taxonomy': 'post_tag',\n",
       "    'embeddable': True,\n",
       "    'href': 'https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=75534'}],\n",
       "  'curies': [{'name': 'wp',\n",
       "    'href': 'https://api.w.org/{rel}',\n",
       "    'templated': True}]}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example1 = techblogs_dict[\"https://developer.nvidia.com/blog/improving-cuda-initialization-times-using-cgroups-in-certain-scenarios/\"]\n",
    "example1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a second example article we'd like to index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 76663,\n",
       " 'date': '2024-01-23T09:00:00',\n",
       " 'date_gmt': '2024-01-23T17:00:00',\n",
       " 'guid': {'rendered': 'https://developer.nvidia.com/blog/?p=76663'},\n",
       " 'modified': '2024-01-25T10:17:29',\n",
       " 'modified_gmt': '2024-01-25T18:17:29',\n",
       " 'slug': 'bringing-generative-ai-to-the-edge-with-nvidia-metropolis-microservices-for-jetson',\n",
       " 'status': 'publish',\n",
       " 'type': 'post',\n",
       " 'link': 'https://developer.nvidia.com/blog/bringing-generative-ai-to-the-edge-with-nvidia-metropolis-microservices-for-jetson/',\n",
       " 'title': {'rendered': 'Bringing Generative AI to the Edge with NVIDIA Metropolis Microservices for Jetson'},\n",
       " 'content': {'rendered': '<div style=\"margin-top: 0px; margin-bottom: 0px;\" class=\"sharethis-inline-share-buttons\" ></div>\\n<p><a href=\"https://developer.nvidia.com/metropolis-microservices\">NVIDIA Metropolis Microservices for Jetson</a> provides a suite of easy-to-deploy services that enable you to quickly build production-quality vision AI applications while using the latest AI approaches.</p>\\n\\n\\n\\n<p>This post explains how to develop and deploy <a href=\"https://www.nvidia.com/en-us/glossary/data-science/generative-ai/\">generative AI</a>–powered applications with Metropolis Microservices on the NVIDIA Jetson edge AI platform by walking through a reference example that can be used as a general recipe for nearly any model.&nbsp;</p>\\n\\n\\n\\n<p>The reference example uses a stand-alone zero-shot detection <a href=\"https://github.com/NVIDIA-AI-IOT/nanoowl\">NanoOwl application</a> and integrates it with <a href=\"https://developer.nvidia.com/metropolis-microservices\">Metropolis Microservices for Jetson</a>, so that you can quickly prototype and deploy it in production.&nbsp;</p>\\n\\n\\n\\n<h2 class=\"wp-block-heading\">Transform your applications with generative AI</h2>\\n\\n\\n\\n<p>Generative AI is a new class of <a href=\"https://www.nvidia.com/en-us/glossary/data-science/machine-learning/\">machine learning</a> that enables models to understand the world in a more open way than previous methods.&nbsp;</p>\\n\\n\\n\\n<p>At the heart of most generative AI is a transformer-based model that has been trained on internet-scale data. These models have a much broader understanding across domains, enabling them to be used as a backbone for a variety of tasks. This flexibility enables models like CLIP, Owl, Llama, GPT, and Stable Diffusion to comprehend natural language inputs. They are capable of zero or few-shot learning.</p>\\n\\n\\n<div class=\"wp-block-image\">\\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"600\" height=\"566\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/zero-shot-detection-nanoowl.gif\" alt=\"GIF shows the NanoOwl model detecting a person, face, hands, and shoes on request.\\xa0\" class=\"wp-image-76965\"/><figcaption class=\"wp-element-caption\"><em>Figure 1. Zero-shot detection using NanoOwl</em></figcaption></figure></div>\\n\\n\\n<p>For more information about generative AI models for Jetson, see the <a href=\"http://www.jetson-ai-lab.com/\">NVIDIA Jetson Generative AI Lab</a> and <a href=\"https://developer.nvidia.com/blog/bringing-generative-ai-to-life-with-jetson/\">Bringing Generative AI to Life with NVIDIA Jetson</a>.&nbsp;</p>\\n\\n\\n\\n<h3 class=\"wp-block-heading\">Metropolis Microservices for Jetson</h3>\\n\\n\\n\\n<p>Metropolis Microservices can be used to rapidly build production-ready AI applications on Jetson. Metropolis Microservices are a set of modular and easily deployable Docker containers for camera management, system monitoring, IoT device integration, networking, storage, and more. These can be brought together to create powerful applications. Figure 2 shows the available microservices.</p>\\n\\n\\n<div class=\"wp-block-image\">\\n<figure class=\"aligncenter size-large\"><img decoding=\"async\" loading=\"lazy\" width=\"625\" height=\"403\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/metropolis-microservices-jetson-stack-b-625x403.png\" alt=\"The diagram shows reference AI workflows, app microservices, an AI stack, and platform software.\" class=\"wp-image-77116\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/metropolis-microservices-jetson-stack-b-625x403.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/metropolis-microservices-jetson-stack-b-300x193.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/metropolis-microservices-jetson-stack-b-179x115.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/metropolis-microservices-jetson-stack-b-768x495.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/metropolis-microservices-jetson-stack-b-1536x990.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/metropolis-microservices-jetson-stack-b-645x416.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/metropolis-microservices-jetson-stack-b-466x300.png 466w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/metropolis-microservices-jetson-stack-b-140x90.png 140w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/metropolis-microservices-jetson-stack-b-362x233.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/metropolis-microservices-jetson-stack-b-171x110.png 171w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/metropolis-microservices-jetson-stack-b-1024x660.png 1024w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/metropolis-microservices-jetson-stack-b.png 1999w\" sizes=\"(max-width: 625px) 100vw, 625px\" /><figcaption class=\"wp-element-caption\"><em>Figure 2. Metropolis Microservices for Jetson stack</em></figcaption></figure></div>\\n\\n\\n<p>For more information, see the <a href=\"https://resources.nvidia.com/en-us-metropolis-microservices-for-jetson/jetson-whitepaper\" data-type=\"link\" data-id=\"https://resources.nvidia.com/en-us-metropolis-microservices-for-jetson/jetson-whitepaper\">Metropolis Microservices for Jetson</a> whitepaper.</p>\\n\\n\\n\\n<h3 class=\"wp-block-heading\">Integrating generative AI apps with Metropolis Microservices</h3>\\n\\n\\n\\n<p>Metropolis Microservices and generative AI can be combined to take advantage of models that require little to no training. Figure 3 shows a diagram of the NanoOwl reference example that can be used as a general recipe to build generative AI–powered applications with Metropolis Microservices on Jetson.&nbsp;&nbsp;</p>\\n\\n\\n<div class=\"wp-block-image\">\\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"1384\" height=\"652\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/gen-ai-reference-app-jetson.png\" alt=\"The system diagram shows the user interfaces, live camera streams, and microservices on Jetson, including the generative AI application, Video Storage Toolkit, ingress, Redis, and monitoring.\" class=\"wp-image-76967\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/gen-ai-reference-app-jetson.png 1384w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/gen-ai-reference-app-jetson-300x141.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/gen-ai-reference-app-jetson-625x294.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/gen-ai-reference-app-jetson-179x84.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/gen-ai-reference-app-jetson-768x362.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/gen-ai-reference-app-jetson-645x304.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/gen-ai-reference-app-jetson-500x236.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/gen-ai-reference-app-jetson-160x75.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/gen-ai-reference-app-jetson-362x171.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/gen-ai-reference-app-jetson-233x110.png 233w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/gen-ai-reference-app-jetson-1024x482.png 1024w\" sizes=\"(max-width: 1384px) 100vw, 1384px\" /><figcaption class=\"wp-element-caption\"><em>Figure 3. Generative AI reference application using Metropolis Microservices for Jetson</em></figcaption></figure></div>\\n\\n\\n<h3 class=\"wp-block-heading\">Application customization with Metropolis Microservices</h3>\\n\\n\\n\\n<p>There are many open-source generative AI models available on GitHub and some have been optimized to run specifically on Jetson. You can find several of these models in the <a href=\"https://www.jetson-ai-lab.com/\">Jetson Generative AI Lab</a>.&nbsp;</p>\\n\\n\\n\\n<p>Most of these models have a lot in common. As inputs, they can typically accept text and an image. These models must first be loaded into memory with any configuration options. Then, the model can be called with an inference function where the image and text are passed in to produce an output.&nbsp;</p>\\n\\n\\n\\n<p>In the Python reference example, we used NanoOwl as the generative AI model. However, the general recipe of the reference example can be applied to nearly any generative AI model.&nbsp;</p>\\n\\n\\n<div class=\"wp-block-image\">\\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"1734\" height=\"824\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/overview-gen-ai-reference-app-metropolis-microservices-b-1.png\" alt=\"The diagram shows how a generative AI application can take in an RTSP stream and output detection information to RTSP and Redis.\" class=\"wp-image-77119\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/overview-gen-ai-reference-app-metropolis-microservices-b-1.png 1734w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/overview-gen-ai-reference-app-metropolis-microservices-b-1-300x143.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/overview-gen-ai-reference-app-metropolis-microservices-b-1-625x297.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/overview-gen-ai-reference-app-metropolis-microservices-b-1-179x85.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/overview-gen-ai-reference-app-metropolis-microservices-b-1-768x365.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/overview-gen-ai-reference-app-metropolis-microservices-b-1-1536x730.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/overview-gen-ai-reference-app-metropolis-microservices-b-1-645x307.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/overview-gen-ai-reference-app-metropolis-microservices-b-1-500x238.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/overview-gen-ai-reference-app-metropolis-microservices-b-1-160x76.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/overview-gen-ai-reference-app-metropolis-microservices-b-1-362x172.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/overview-gen-ai-reference-app-metropolis-microservices-b-1-231x110.png 231w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/overview-gen-ai-reference-app-metropolis-microservices-b-1-1024x487.png 1024w\" sizes=\"(max-width: 1734px) 100vw, 1734px\" /><figcaption class=\"wp-element-caption\"><em>Figure 4. Overview of a generative AI application using Metropolis Microservices</em></figcaption></figure></div>\\n\\n\\n<p>To run any generative AI model with Metropolis Microservices, you must first align the input and output from other microservices (Figure 4).&nbsp;</p>\\n\\n\\n\\n<p>For streaming video, the input and output uses the RTSP protocol. RTSP is streamed from Video Storage Toolkit (VST), a video ingestion and management microservice. The output is streamed over RTSP with the overlaid inference output. The output metadata is sent to a Redis stream where other applications can read the data. For more information, see the <a href=\"https://www.nvidia.com/en-us/on-demand/playlist/playList-238cd4a8-7f1d-42b4-9b0a-a949ade92845/\" data-type=\"link\" data-id=\"https://www.nvidia.com/en-us/on-demand/playlist/playList-238cd4a8-7f1d-42b4-9b0a-a949ade92845/\">Video Storage Toolkit with Metropolis Microservices demo videos</a>.</p>\\n\\n\\n\\n<p>Second, as a generative AI application requires some external interface such as prompts, you need the application to take REST API requests.&nbsp;</p>\\n\\n\\n\\n<p>Lastly, the application must be containerized to integrate seamlessly with other microservices. Figure 5 shows an example of NanoOwl object detection and metadata output on Redis.&nbsp;</p>\\n\\n\\n<div class=\"wp-block-image\">\\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"600\" height=\"181\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/gen-ai-app-running.gif\" alt=\"GIF shows the generative AI application detecting various objects like boxes, pallets, and people.\\xa0\" class=\"wp-image-76969\"/><figcaption class=\"wp-element-caption\"><em>Figure 5. Generative AI application running</em></figcaption></figure></div>\\n\\n\\n<h2 class=\"wp-block-heading\">Prepare the generative AI application</h2>\\n\\n\\n\\n<p>This reference example uses NanoOwl. However, you can follow these steps for any model that has a load and inference function callable from Python. There are several Python code examples shown in this post to highlight the main ideas of how to combine generative AI with Metropolis Microservices but some code is omitted to focus on the general recipe. For more information about the full implementation, see the reference example on the <a href=\"https://github.com/NVIDIA-AI-IOT/mmj_genai\">/NVIDIA-AI-IOT/mmj_genai</a> GitHub project.</p>\\n\\n\\n\\n<p>To prepare a generative AI model for integration with Metropolis Microservices, take the following steps:&nbsp;</p>\\n\\n\\n\\n<ol>\\n<li>Call the <code>predict</code> function for model inference</li>\\n\\n\\n\\n<li>Add RTSP I/O using the <code>jetson-utils</code> library.</li>\\n\\n\\n\\n<li>Add a REST endpoint for prompt updates with Flask.</li>\\n\\n\\n\\n<li>Use <code>mmj_utils</code> to generate overlays.</li>\\n\\n\\n\\n<li>Use <code>mmj_utils</code> to interact with VST to get streams.</li>\\n\\n\\n\\n<li>Use <code>mmj_utils</code> to output metadata to Redis.</li>\\n</ol>\\n\\n\\n\\n<h3 class=\"wp-block-heading\">Call the predict function for model inference</h3>\\n\\n\\n\\n<p>NanoOwl wraps the generative AI model in an <code>OwlPredictor</code> class. When this class is instantiated, it loads the model into memory. To make an inference on an image and text input, call the <code>predict</code> function to get the output.&nbsp;</p>\\n\\n\\n\\n<p>In this case, the output is a list of bounding boxes and labels for the detected objects.&nbsp;</p>\\n\\n\\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: python; title: ; notranslate\" title=\"\">\\nimport PIL.Image\\nimport time\\nimport torch\\nfrom nanoowl.owl_predictor import OwlPredictor\\n\\nimage = PIL.Image.open(&quot;my_image.png&quot;)\\nprompt = &#91;&quot;an owl&quot;, &quot;a person&quot;]\\n\\n#Load model\\n predictor = OwlPredictor(\\n       &quot;google/owlvit-base-patch32&quot;,\\n        image_encoder_engine=&quot;../data/owlvit_image_encoder_patch32.engine&quot;\\n    )\\n#Embed Text\\ntext_encodings = predictor.encode_text(text)\\n\\n#Inference\\noutput = predictor.predict(\\nimage=image, \\n        \\ttext=prompt, \\n        \\ttext_encodings=text_encodings,\\n       \\t threshold=0.1,\\n        \\tpad_square=False)\\n</pre></div>\\n\\n\\n<p>Most generative AI models have similar Python interfaces. There is image and text input, the model must be loaded, and then the model can infer from the prompt and image to get some output. To bring in your own generative AI model, you can wrap it in a class and implement an interface similar to the <code>OwlPredictor</code> class.&nbsp;</p>\\n\\n\\n\\n<h3 class=\"wp-block-heading\">Add RTSP I/O using the jetson-utils library</h3>\\n\\n\\n\\n<p>You can add RTSP video stream input using the <a href=\"https://github.com/dusty-nv/jetson-utils\">jetson-utils</a> library. This library provides <code>videoSource</code> and <code>videoOutput</code> classes that can be used to capture frames from an RTSP stream and output frames on a new RTSP stream.&nbsp;</p>\\n\\n\\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: python; title: ; notranslate\" title=\"\">\\nfrom jetson_utils import videoSource, videoOutput\\n\\nstream_input = &quot;rtsp://0.0.0.0:8554/input&quot;\\nstream_output = &quot;rtsp://0.0.0.0:8555/output&quot;\\n\\n#Create stream I/O\\nv_input = videoSource(stream_input)\\nv_output = videoOutput(stream_output)\\n\\nwhile(True):\\n\\timage = v_input.Capture() #get image from stream\\n\\toutput = predictor.predict(image=image, text=prompt, ...)\\n\\n\\tnew_image = postprocess(output)\\n\\n\\tv_output.Render(new_image) #write image to stream \\n</pre></div>\\n\\n\\n<p>This code example captures frames from an RTSP stream, which can then be passed to a model inference function. A new image is created from the model outputs and rendered to an output RTSP stream.&nbsp;</p>\\n\\n\\n\\n<h3 class=\"wp-block-heading\">Add a REST endpoint for prompt updates with Flask</h3>\\n\\n\\n\\n<p>Many generative AI models accept some kind of prompt or text input. To enable a user or another service to update the prompt dynamically, add a REST endpoint using <a href=\"https://flask.palletsprojects.com/en/3.0.x/\">Flask</a> that accepts prompt updates and passes them to the model.&nbsp;</p>\\n\\n\\n\\n<p>To make the Flask server integrate more easily with your model, create a wrapper class that can be called to launch a Flask server in its own thread. For more information, see the <a href=\"https://github.com/NVIDIA-AI-IOT/mmj_genai\">/NVIDIA-AI-IOT/mmj_genai</a> GitHub project.</p>\\n\\n\\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: python; title: ; notranslate\" title=\"\">\\nfrom flask_server import FlaskServer\\n\\n#Launch flask server and connect queue to receive prompt updates \\nflask_queue = Queue() #hold prompts from flask input \\nflask = FlaskServer(flask_queue)\\nflask.start_flask()\\n\\nwhile(True):\\n\\t...\\n\\n\\tif not flask_queue.empty(): #get prompt update\\n            prompt = flask_queue.get()\\n\\t\\n\\toutput = predictor.predict(image=image, text=prompt, ...)\\n\\t...\\n</pre></div>\\n\\n\\n<p>Connect your main script and the Flask endpoint through a queue that holds any incoming prompt updates. When a GET request is sent to the REST endpoint, the Flask server places the updated prompt in a queue. Your main loop can then check the queue for new prompts and pass it to the model for inference on the updated classes.</p>\\n\\n\\n\\n<h3 class=\"wp-block-heading\">Use mmj_utils to generate overlays</h3>\\n\\n\\n\\n<p>For computer vision tasks, it is nice to see a visual overlay of the model output (Figure 6). In the case of object detection models, you can overlay the bounding boxes and labels generated by the model on the input image to view where the model detected each object.&nbsp;</p>\\n\\n\\n<div class=\"wp-block-image\">\\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"1999\" height=\"1086\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/detection-overlay-mmj-utils.png\" alt=\"Image shows a factory floor with people, pallets, and equipment delineated by labeled bounding boxes.\\xa0\" class=\"wp-image-76970\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/detection-overlay-mmj-utils.png 1999w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/detection-overlay-mmj-utils-300x163.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/detection-overlay-mmj-utils-625x340.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/detection-overlay-mmj-utils-179x97.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/detection-overlay-mmj-utils-768x417.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/detection-overlay-mmj-utils-1536x834.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/detection-overlay-mmj-utils-645x350.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/detection-overlay-mmj-utils-500x272.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/detection-overlay-mmj-utils-160x87.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/detection-overlay-mmj-utils-362x197.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/detection-overlay-mmj-utils-202x110.png 202w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/detection-overlay-mmj-utils-1024x556.png 1024w\" sizes=\"(max-width: 1999px) 100vw, 1999px\" /><figcaption class=\"wp-element-caption\"><em>Figure 6. Detection overlay generated with mmj_utils</em></figcaption></figure></div>\\n\\n\\n<p>To do this, use the utility class called <code>DetectionGenerationCUDA</code> from the <a href=\"https://github.com/NVIDIA-AI-IOT/mmj_utils\">mmj_utils library</a>. This library depends on <code>jetson_utils</code>, which provides CUDA-accelerated functions used to generate the overlay.</p>\\n\\n\\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: python; title: ; notranslate\" title=\"\">\\nfrom mmj_utils.overlay_gen import DetectionOverlayCUDA\\n\\noverlay_gen = DetectionOverlayCUDA(draw_bbox=True, draw_text=True, text_size=45) #make overlay object\\n\\nwhile(True):\\n\\t...\\n\\n\\toutput = predictor.predict(image=image, text=prompt, ...)\\n\\t\\n\\t#Generate overlay and output\\n       text_labels = &#91;objects&#91;x] for x in output.labels]\\n       bboxes = output.boxes.tolist()\\n       image = overlay_gen(image, text_labels, bboxes)#generate overlay\\n       v_output.Render(image)\\n</pre></div>\\n\\n\\n<p>You can instantiate the <code>DetectionGenerationCUDA</code> object with several keyword arguments to adjust the text size, bounding box size, and colors to suit your needs. For more information about overlay generation with <code>mmj_utils</code>, see <a href=\"https://github.com/NVIDIA-AI-IOT/mmj_utils\">/NVIDIA-AI-IOT/mmj_utils</a> GitHub repo.</p>\\n\\n\\n\\n<p>To generate the overlay, call the object and pass the input image, list of labels, and bounding boxes generated by the model. It then draws the labels and bounding boxes on the input image and returns the modified image with the overlay. This modified image can then be rendered out on the RTSP stream.&nbsp;</p>\\n\\n\\n\\n<h3 class=\"wp-block-heading\">Use mmj_utils to interact with VST to get streams</h3>\\n\\n\\n\\n<p>VST can help manage your RTSP streams and provide a nice web UI to view the input and output streams. To integrate with VST, use the VST REST API directly or the VST class from mmj_utils, which wraps around the <a href=\"https://docs.nvidia.com/moj/vst/VST_API_Guide.html\">VST REST API</a>.&nbsp;</p>\\n\\n\\n\\n<p>Instead of hardcoding the RTSP input stream in the Python script, grab an RTSP stream link from VST. This link could come from IP cameras or other video stream sources managed through VST.&nbsp;</p>\\n\\n\\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: python; title: ; notranslate\" title=\"\">\\nfrom mmj_utils.vst import VST\\n\\nvst = VST(&quot;http://0.0.0.0:81&quot;)\\nvst_rtsp_streams = vst.get_rtsp_streams()\\nstream_input = vst_rtsp_streams&#91;0]\\n\\nv_input = videoSource(stream_input)\\n...\\n</pre></div>\\n\\n\\n<p>This connects to VST and grabs the first valid RTSP link. More complex logic could be added here to connect to a specific source or change the inputs dynamically.&nbsp;</p>\\n\\n\\n\\n<h3 class=\"wp-block-heading\">Use mmj_utils to output metadata to Redis</h3>\\n\\n\\n\\n<p>Generative AI models generate\\u200c metadata that can be used downstream by other services for generating analytics and insights. </p>\\n\\n\\n\\n<p>In this case, NanoOwl outputs bounding boxes on detected objects. You can output this information in <a href=\"https://docs.nvidia.com/moj/emdx/metadata.html\">Metropolis Schema</a> on a Redis stream, which can be captured by an analytic service. In the <code>mmj_utils</code> library, there is a class to help produce detection metadata on Redis.&nbsp;</p>\\n\\n\\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: python; title: ; notranslate\" title=\"\">\\nfrom mmj_utils.schema_gen import SchemaGenerator\\n\\n\\nschema_gen = SchemaGenerator(sensor_id=1, sensor_type=&quot;camera&quot;, sensor_loc=&#91;10,20,30])\\nschema_gen.connect_redis(aredis_host=0.0.0.0, redis_port=6379, redis_stream=&quot;owl&quot;)\\n\\nwhile True:\\n\\t...\\n\\toutput = predictor.predict(image=image, text=prompt, ...)\\n\\t\\n\\t#Output metadata\\n       text_labels = &#91;objects&#91;x] for x in output.labels]\\nschema_gen(text_labels, bboxes)\\n</pre></div>\\n\\n\\n<p>You can instantiate a <code>SchemaGenerator</code> object with information about the input camera stream and connect to Redis. The object can then be called by passing in text labels and bounding boxes produced by the model. The detection information gets converted to Metropolis Schema and output to Redis to be used by other microservices.&nbsp;</p>\\n\\n\\n<div class=\"wp-block-image\">\\n<figure class=\"aligncenter size-full\"><img decoding=\"async\" loading=\"lazy\" width=\"1309\" height=\"596\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/gen-ai-app.png\" alt=\"The diagram shows components such as VST Stream Discovery, NanoOwl Object Detection, a Flask REST Endpoint, Overlay Generation, Metadata Generation, and the incoming RTSP stream.\" class=\"wp-image-76971\" srcset=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/gen-ai-app.png 1309w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/gen-ai-app-300x137.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/gen-ai-app-625x285.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/gen-ai-app-179x82.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/gen-ai-app-768x350.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/gen-ai-app-645x294.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/gen-ai-app-500x228.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/gen-ai-app-160x73.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/gen-ai-app-362x165.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/gen-ai-app-242x110.png 242w, https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/gen-ai-app-1024x466.png 1024w\" sizes=\"(max-width: 1309px) 100vw, 1309px\" /><figcaption class=\"wp-element-caption\"><em>Figure 7. Generative AI application</em></figcaption></figure></div>\\n\\n\\n<h2 class=\"wp-block-heading\">Application deployment</h2>\\n\\n\\n\\n<p>To deploy the application, you can set up platform services like Ingress and Redis. Then, combine your custom generative AI container with the application services such as VST through <code>docker compose</code>.&nbsp;&nbsp;&nbsp;</p>\\n\\n\\n\\n<p>With the main application prepared with all the necessary I/O and microservice integration (Figure 7), you can deploy the application and connect with Metropolis Microservices.&nbsp;</p>\\n\\n\\n\\n<ol>\\n<li>Containerize the generative AI application.</li>\\n\\n\\n\\n<li>Set up the necessary platform services.</li>\\n\\n\\n\\n<li>Launch the application with <code>docker compose</code>.</li>\\n\\n\\n\\n<li>View outputs in real time.</li>\\n</ol>\\n\\n\\n\\n<h3 class=\"wp-block-heading\">Containerize the generative AI application</h3>\\n\\n\\n\\n<p>The first step for deployment is to containerize the generative AI application using Docker.&nbsp;</p>\\n\\n\\n\\n<p>An easy way to do this is to use the <a href=\"https://github.com/dusty-nv/jetson-containers\">jetson-containers</a> project. This project provides an easy way to build Docker containers for Jetson to support machine learning applications including generative AI models. Use jetson-containers to make a container with the necessary dependencies and then customize the container further to include the application code and any other packages needed to run your generative AI model.&nbsp;</p>\\n\\n\\n\\n<p>For more information about how to build the container for the NanoOwl example, see the <a href=\"https://github.com/NVIDIA-AI-IOT/mmj_genai/tree/main/src/README.md\" data-type=\"link\" data-id=\"https://github.com/NVIDIA-AI-IOT/mmj_genai/tree/main/src/README.md\">/src/readme</a> file in the GitHub project.</p>\\n\\n\\n\\n<h3 class=\"wp-block-heading\">Set up the necessary platform services</h3>\\n\\n\\n\\n<p>Next, set up the necessary platform services provided by Metropolis Microservices. These platform services provide many features needed to deploy an application with Metropolis Microservices.&nbsp;</p>\\n\\n\\n\\n<p>This reference generative AI application only requires the Ingress, Redis, and Monitoring platform services. Platform services can be quickly installed through APT and launched with <code>systemctl</code>.&nbsp;</p>\\n\\n\\n\\n<p>For more information about how to install and launch the necessary platform services, see the <a href=\"https://docs.nvidia.com/moj/index.html\">Metropolis Microservices for Jetson Quickstart Guide</a>.</p>\\n\\n\\n\\n<h3 class=\"wp-block-heading\">Launch the application with docker compose</h3>\\n\\n\\n\\n<p>With the application containerized and the necessary platform services set up, you can launch your application along with other application services like VST or Analytics using <code>docker compose</code>.&nbsp;</p>\\n\\n\\n\\n<p>To do this, create a <code>docker-compose.yaml</code> file that defines the containers to be launched along with any necessary launch options. After you define the <code>docker compose</code> file, you can start or stop your application using the <code>docker compose up</code> and<em> </em><code>docker compose down</code> commands.&nbsp;</p>\\n\\n\\n\\n<p>For more information about docker deployment, see the <a href=\"https://github.com/NVIDIA-AI-IOT/mmj_genai/tree/main/deploy/README.md\" data-type=\"link\" data-id=\"https://github.com/NVIDIA-AI-IOT/mmj_genai/tree/main/deploy/README.md\">/deploy/readme</a> file in the GitHub project.</p>\\n\\n\\n\\n<h3 class=\"wp-block-heading\">View outputs in real time</h3>\\n\\n\\n\\n<p>After the application is deployed, you can add an RTSP stream through VST and interact with the generative AI model through the REST API to send prompt updates and view the detections change in real time by watching the RTSP output. You can also see the metadata output on Redis.</p>\\n\\n\\n\\n<figure class=\"wp-block-embed aligncenter is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio\"><div class=\"wp-block-embed__wrapper\">\\n<span class=\"embed-youtube\" style=\"text-align:center; display: block;\"><iframe loading=\"lazy\" class=\"youtube-player\" width=\"640\" height=\"360\" src=\"https://www.youtube.com/embed/DTVYAND5X8Q?version=3&#038;rel=1&#038;showsearch=0&#038;showinfo=1&#038;iv_load_policy=1&#038;fs=1&#038;hl=en-US&#038;autohide=2&#038;wmode=transparent\" allowfullscreen=\"true\" style=\"border:0;\" sandbox=\"allow-scripts allow-same-origin allow-popups allow-presentation allow-popups-to-escape-sandbox\"></iframe></span>\\n</div><figcaption class=\"wp-element-caption\"><em>Video 1. Transform Edge AI Applications with Generative AI Using Metropolis Microservices for Jetson</em></figcaption></figure>\\n\\n\\n\\n<h1 class=\"wp-block-heading\">Conclusion</h1>\\n\\n\\n\\n<p>This post explained how to take a generative AI model and integrate it with Metropolis Microservices for Jetson. With generative AI and Metropolis Microservices, you can rapidly build intelligent video analytic applications that are both flexible and accurate.</p>\\n\\n\\n\\n<p>For more information about the provided services, see the <a href=\"https://developer.nvidia.com/metropolis-microservices\">Metropolis Microservices for Jetson</a> product page. To view the full reference application and more detailed steps about how to build and deploy it for yourself, see the <a href=\"https://github.com/NVIDIA-AI-IOT/mmj_genai\">/NVIDIA-AI-IOT/mmj_genai</a> GitHub project.</p>\\n',\n",
       "  'protected': False},\n",
       " 'excerpt': {'rendered': '<p>NVIDIA Metropolis Microservices for Jetson provides a suite of easy-to-deploy services that enable you to quickly build production-quality vision AI applications while using the latest AI approaches. This post explains how to develop and deploy generative AI–powered applications with Metropolis Microservices on the NVIDIA Jetson edge AI platform by walking through a reference example that &hellip; <a href=\"https://developer.nvidia.com/blog/bringing-generative-ai-to-the-edge-with-nvidia-metropolis-microservices-for-jetson/\">Continued</a></p>\\n',\n",
       "  'protected': False},\n",
       " 'author': 1925,\n",
       " 'featured_media': 77068,\n",
       " 'comment_status': 'open',\n",
       " 'ping_status': 'open',\n",
       " 'sticky': False,\n",
       " 'template': '',\n",
       " 'format': 'standard',\n",
       " 'meta': {'publish_to_discourse': '',\n",
       "  'publish_post_category': '318',\n",
       "  'wpdc_auto_publish_overridden': '',\n",
       "  'wpdc_topic_tags': '',\n",
       "  'wpdc_pin_topic': '',\n",
       "  'wpdc_pin_until': '',\n",
       "  'discourse_post_id': '1338055',\n",
       "  'discourse_permalink': 'https://forums.developer.nvidia.com/t/bringing-generative-ai-to-the-edge-with-nvidia-metropolis-microservices-for-jetson/279875',\n",
       "  'wpdc_publishing_response': 'success',\n",
       "  'wpdc_publishing_error': '',\n",
       "  'footnotes': '',\n",
       "  '_links_to': '',\n",
       "  '_links_to_target': ''},\n",
       " 'categories': [2724, 2758, 3110, 63],\n",
       " 'tags': [453, 1950],\n",
       " 'acf': [],\n",
       " 'jetpack_featured_media_url': 'https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/metropolis-iva-gen-ai-featured.gif',\n",
       " 'jetpack_shortlink': 'https://wp.me/pcCQAL-jWv',\n",
       " 'jetpack_likes_enabled': True,\n",
       " 'jetpack_sharing_enabled': True,\n",
       " '_links': {'self': [{'href': 'https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/76663'}],\n",
       "  'collection': [{'href': 'https://developer-blogs.nvidia.com/wp-json/wp/v2/posts'}],\n",
       "  'about': [{'href': 'https://developer-blogs.nvidia.com/wp-json/wp/v2/types/post'}],\n",
       "  'author': [{'embeddable': True,\n",
       "    'href': 'https://developer-blogs.nvidia.com/wp-json/wp/v2/users/1925'}],\n",
       "  'replies': [{'embeddable': True,\n",
       "    'href': 'https://developer-blogs.nvidia.com/wp-json/wp/v2/comments?post=76663'}],\n",
       "  'version-history': [{'count': 8,\n",
       "    'href': 'https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/76663/revisions'}],\n",
       "  'predecessor-version': [{'id': 77120,\n",
       "    'href': 'https://developer-blogs.nvidia.com/wp-json/wp/v2/posts/76663/revisions/77120'}],\n",
       "  'wp:featuredmedia': [{'embeddable': True,\n",
       "    'href': 'https://developer-blogs.nvidia.com/wp-json/wp/v2/media/77068'}],\n",
       "  'wp:attachment': [{'href': 'https://developer-blogs.nvidia.com/wp-json/wp/v2/media?parent=76663'}],\n",
       "  'wp:term': [{'taxonomy': 'category',\n",
       "    'embeddable': True,\n",
       "    'href': 'https://developer-blogs.nvidia.com/wp-json/wp/v2/categories?post=76663'},\n",
       "   {'taxonomy': 'post_tag',\n",
       "    'embeddable': True,\n",
       "    'href': 'https://developer-blogs.nvidia.com/wp-json/wp/v2/tags?post=76663'}],\n",
       "  'curies': [{'name': 'wp',\n",
       "    'href': 'https://api.w.org/{rel}',\n",
       "    'templated': True}]}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example2 = techblogs_dict[\"https://developer.nvidia.com/blog/bringing-generative-ai-to-the-edge-with-nvidia-metropolis-microservices-for-jetson/\"]\n",
    "example2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importance of Chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you haven't already, click on the links to those articles and get a sense visually of how much text there is and what style of text it is.\n",
    "\n",
    "These articles can get fairly long! We might not want to send entire posts as context to our downstream LLM--for any given prompt to the LLM, there might be hundreds of posts that have pieces relevant to the response! Using full posts makes it harder for the LLM to find the right information and could increase costs (since API-based LLMs charge per token).\n",
    "\n",
    "Instead, we can break these articles into chunks and index those chunks. Chunking is incredibly important for a RAG system, and chunking design can have a surprisingly large impact on RAG effectiveness.\n",
    "\n",
    "Unfortunately, there isn't a one-size-fits-all chunking strategy that will work for every dataset and every downstream task. Some use cases need a lot of context within each chunk; others do best when many small chunks are aggregated together. You'll need to experiment to ultimately arrive at a strategy that yields the best performance. \n",
    "\n",
    "Some chunking strategies can be quite clever (and also expensive computationally), and look at break points where the topic of the document is changing.\n",
    "\n",
    "For this lesson we'll go through some simpler ones via the chunking service we've prepared for you. This has a few basic strategies for chunking text and HTML based on a running counter of words--a convenient way to estimate how many chunks we can fit into a fixed-size LLM prompt. \n",
    "\n",
    "As a reminder, the source code for this can be found by navigating to `chunking/src`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducing the Chunking Service through API Docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You started the `chunking` microservice in *Lesson 00*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36mchunking-1  | \u001b[0mINFO:     Started server process [7]\n",
      "\u001b[36mchunking-1  | \u001b[0mINFO:     Waiting for application startup.\n",
      "\u001b[36mchunking-1  | \u001b[0mINFO:     Application startup complete.\n",
      "\u001b[36mchunking-1  | \u001b[0mINFO:     Uvicorn running on http://0.0.0.0:5005 (Press CTRL+C to quit)\n"
     ]
    }
   ],
   "source": [
    "!docker-compose logs chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see the chunking service is available on port 5005. Execute the following cell to generate a link to open it in a new browser tab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "var host = window.location.host;\n",
       "var url = 'http://'+host+':5005';\n",
       "element.innerHTML = '<a style=\"color:green;\" target=\"_blank\" href='+url+'>Click to open chunking service API docs.</a>';\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%js\n",
    "var host = window.location.host;\n",
    "var url = 'http://'+host+':5005';\n",
    "element.innerHTML = '<a style=\"color:green;\" target=\"_blank\" href='+url+'>Click to open chunking service API docs.</a>';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The chunking service is a FastAPI Python web app, running with Uvicorn. FastAPI has better performance than the more popular Flask framework and built-in asynchronous support for endpoints. This will let us chunk more than one document at a given time, to make the data upload process faster.\n",
    "\n",
    "FastAPI also has built in auto-generated API documentation (in a Swagger/OpenAPI format). If you visit the link you just generated in your browser, you should see an API docs page. If you click and expand the endpoint `/api/chunking` you will see examples of the Request Body you'd send to the endpoint in order to use different chunking strategies. Each example has an `Example Description` explaining what is happening.\n",
    "\n",
    "If you click \"Try It out\" button and then click \"Execute\" button, you can hit the chunking API entirely through the web browser. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimenting Programmatically with Chunking Service "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the API docs is nice for testing, but in order to send lots of requests at once, we want to hit the chunking API programmatically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create API Client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can accomplish this using the Python `httpx` library, which is very similar to the popular `requests` library but with better async support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = httpx.Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create API Request Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we create a `chunk_request` function for making requests to the chunking API. Note: the chunking service we are already running for you is running at the `chunking` hostname."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunking_url = \"http://chunking:5005/api/chunking\"\n",
    "\n",
    "def chunk_request(client, request_body):\n",
    "    chunking_resp = client.post(chunking_url, json=request_body, timeout=30)\n",
    "    chunks = chunking_resp.json()\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence-by-Sentence Chunking"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we do a standard sentence-by-sentence chunking to see what that looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': 'Many CUDA applications running on multi-GPU platforms usually use a single GPU for their compute needs.',\n",
       "  'text_components': ['Many CUDA applications running on multi-GPU platforms usually use a single GPU for their compute needs.'],\n",
       "  'paragraph_index': [0],\n",
       "  'word_count': [17],\n",
       "  'paragraph_sentence_index': [0],\n",
       "  'only_code': [False],\n",
       "  'contains_code': [False],\n",
       "  'document_title': 'Improving CUDA Initialization Times Using cgroups in Certain Scenarios',\n",
       "  'document_url': 'https://developer.nvidia.com/blog/improving-cuda-initialization-times-using-cgroups-in-certain-scenarios/',\n",
       "  'document_date': '2024-01-05T22:14:41',\n",
       "  'document_date_modified': '2024-01-11T19:49:33'},\n",
       " {'text': 'In such scenarios, a performance penalty is paid by applications because CUDA has to enumerate/initialize all the GPUs on the system.',\n",
       "  'text_components': ['In such scenarios, a performance penalty is paid by applications because CUDA has to enumerate/initialize all the GPUs on the system.'],\n",
       "  'paragraph_index': [0],\n",
       "  'word_count': [22],\n",
       "  'paragraph_sentence_index': [1],\n",
       "  'only_code': [False],\n",
       "  'contains_code': [False],\n",
       "  'document_title': 'Improving CUDA Initialization Times Using cgroups in Certain Scenarios',\n",
       "  'document_url': 'https://developer.nvidia.com/blog/improving-cuda-initialization-times-using-cgroups-in-certain-scenarios/',\n",
       "  'document_date': '2024-01-05T22:14:41',\n",
       "  'document_date_modified': '2024-01-11T19:49:33'},\n",
       " {'text': 'If a CUDA application does not require other GPUs to be visible and accessible, you can launch such applications by isolating the unwanted GPUs from the CUDA process and eliminating unnecessary initialization steps.',\n",
       "  'text_components': ['If a CUDA application does not require other GPUs to be visible and accessible, you can launch such applications by isolating the unwanted GPUs from the CUDA process and eliminating unnecessary initialization steps.'],\n",
       "  'paragraph_index': [0],\n",
       "  'word_count': [33],\n",
       "  'paragraph_sentence_index': [2],\n",
       "  'only_code': [False],\n",
       "  'contains_code': [False],\n",
       "  'document_title': 'Improving CUDA Initialization Times Using cgroups in Certain Scenarios',\n",
       "  'document_url': 'https://developer.nvidia.com/blog/improving-cuda-initialization-times-using-cgroups-in-certain-scenarios/',\n",
       "  'document_date': '2024-01-05T22:14:41',\n",
       "  'document_date_modified': '2024-01-11T19:49:33'},\n",
       " {'text': 'This post discusses the various methods to accomplish this and their performance benefits.',\n",
       "  'text_components': ['This post discusses the various methods to accomplish this and their performance benefits.'],\n",
       "  'paragraph_index': [1],\n",
       "  'word_count': [13],\n",
       "  'paragraph_sentence_index': [0],\n",
       "  'only_code': [False],\n",
       "  'contains_code': [False],\n",
       "  'document_title': 'Improving CUDA Initialization Times Using cgroups in Certain Scenarios',\n",
       "  'document_url': 'https://developer.nvidia.com/blog/improving-cuda-initialization-times-using-cgroups-in-certain-scenarios/',\n",
       "  'document_date': '2024-01-05T22:14:41',\n",
       "  'document_date_modified': '2024-01-11T19:49:33'},\n",
       " {'text': 'GPU isolation',\n",
       "  'text_components': ['GPU isolation'],\n",
       "  'paragraph_index': [2],\n",
       "  'word_count': [2],\n",
       "  'paragraph_sentence_index': [0],\n",
       "  'only_code': [False],\n",
       "  'contains_code': [False],\n",
       "  'document_title': 'Improving CUDA Initialization Times Using cgroups in Certain Scenarios',\n",
       "  'document_url': 'https://developer.nvidia.com/blog/improving-cuda-initialization-times-using-cgroups-in-certain-scenarios/',\n",
       "  'document_date': '2024-01-05T22:14:41',\n",
       "  'document_date_modified': '2024-01-11T19:49:33'},\n",
       " {'text': 'GPU isolation can be achieved on Linux systems by using Linux tools like ```cgroups```.',\n",
       "  'text_components': ['GPU isolation can be achieved on Linux systems by using Linux tools like ```cgroups```.'],\n",
       "  'paragraph_index': [3],\n",
       "  'word_count': [20],\n",
       "  'paragraph_sentence_index': [0],\n",
       "  'only_code': [False],\n",
       "  'contains_code': [True],\n",
       "  'document_title': 'Improving CUDA Initialization Times Using cgroups in Certain Scenarios',\n",
       "  'document_url': 'https://developer.nvidia.com/blog/improving-cuda-initialization-times-using-cgroups-in-certain-scenarios/',\n",
       "  'document_date': '2024-01-05T22:14:41',\n",
       "  'document_date_modified': '2024-01-11T19:49:33'},\n",
       " {'text': 'In this section, we first discuss a lower-level approach and then a higher-level possible approach.',\n",
       "  'text_components': ['In this section, we first discuss a lower-level approach and then a higher-level possible approach.'],\n",
       "  'paragraph_index': [3],\n",
       "  'word_count': [17],\n",
       "  'paragraph_sentence_index': [1],\n",
       "  'only_code': [False],\n",
       "  'contains_code': [True],\n",
       "  'document_title': 'Improving CUDA Initialization Times Using cgroups in Certain Scenarios',\n",
       "  'document_url': 'https://developer.nvidia.com/blog/improving-cuda-initialization-times-using-cgroups-in-certain-scenarios/',\n",
       "  'document_date': '2024-01-05T22:14:41',\n",
       "  'document_date_modified': '2024-01-11T19:49:33'},\n",
       " {'text': 'Another method exposed by CUDA to isolate devices is the use of ```CUDA_VISIBLE_DEVICES```.',\n",
       "  'text_components': ['Another method exposed by CUDA to isolate devices is the use of ```CUDA_VISIBLE_DEVICES```.'],\n",
       "  'paragraph_index': [4],\n",
       "  'word_count': [19],\n",
       "  'paragraph_sentence_index': [0],\n",
       "  'only_code': [False],\n",
       "  'contains_code': [True],\n",
       "  'document_title': 'Improving CUDA Initialization Times Using cgroups in Certain Scenarios',\n",
       "  'document_url': 'https://developer.nvidia.com/blog/improving-cuda-initialization-times-using-cgroups-in-certain-scenarios/',\n",
       "  'document_date': '2024-01-05T22:14:41',\n",
       "  'document_date_modified': '2024-01-11T19:49:33'},\n",
       " {'text': 'Although functionally similar, this approach has limited initialization performance gains compared to the ```cgroups``` approach.',\n",
       "  'text_components': ['Although functionally similar, this approach has limited initialization performance gains compared to the ```cgroups``` approach.'],\n",
       "  'paragraph_index': [4],\n",
       "  'word_count': [21],\n",
       "  'paragraph_sentence_index': [1],\n",
       "  'only_code': [False],\n",
       "  'contains_code': [True],\n",
       "  'document_title': 'Improving CUDA Initialization Times Using cgroups in Certain Scenarios',\n",
       "  'document_url': 'https://developer.nvidia.com/blog/improving-cuda-initialization-times-using-cgroups-in-certain-scenarios/',\n",
       "  'document_date': '2024-01-05T22:14:41',\n",
       "  'document_date_modified': '2024-01-11T19:49:33'},\n",
       " {'text': 'Isolating GPUs using cgroups V1',\n",
       "  'text_components': ['Isolating GPUs using cgroups V1'],\n",
       "  'paragraph_index': [5],\n",
       "  'word_count': [5],\n",
       "  'paragraph_sentence_index': [0],\n",
       "  'only_code': [False],\n",
       "  'contains_code': [False],\n",
       "  'document_title': 'Improving CUDA Initialization Times Using cgroups in Certain Scenarios',\n",
       "  'document_url': 'https://developer.nvidia.com/blog/improving-cuda-initialization-times-using-cgroups-in-certain-scenarios/',\n",
       "  'document_date': '2024-01-05T22:14:41',\n",
       "  'document_date_modified': '2024-01-11T19:49:33'},\n",
       " {'text': 'Control groups provide a mechanism for aggregating or partitioning sets of tasks and all their future children into hierarchical groups with specialized behavior.',\n",
       "  'text_components': ['Control groups provide a mechanism for aggregating or partitioning sets of tasks and all their future children into hierarchical groups with specialized behavior.'],\n",
       "  'paragraph_index': [6],\n",
       "  'word_count': [23],\n",
       "  'paragraph_sentence_index': [0],\n",
       "  'only_code': [False],\n",
       "  'contains_code': [True],\n",
       "  'document_title': 'Improving CUDA Initialization Times Using cgroups in Certain Scenarios',\n",
       "  'document_url': 'https://developer.nvidia.com/blog/improving-cuda-initialization-times-using-cgroups-in-certain-scenarios/',\n",
       "  'document_date': '2024-01-05T22:14:41',\n",
       "  'document_date_modified': '2024-01-11T19:49:33'},\n",
       " {'text': 'You can use ```cgroups``` to control which GPUs are visible to a CUDA process.',\n",
       "  'text_components': ['You can use ```cgroups``` to control which GPUs are visible to a CUDA process.'],\n",
       "  'paragraph_index': [6],\n",
       "  'word_count': [20],\n",
       "  'paragraph_sentence_index': [1],\n",
       "  'only_code': [False],\n",
       "  'contains_code': [True],\n",
       "  'document_title': 'Improving CUDA Initialization Times Using cgroups in Certain Scenarios',\n",
       "  'document_url': 'https://developer.nvidia.com/blog/improving-cuda-initialization-times-using-cgroups-in-certain-scenarios/',\n",
       "  'document_date': '2024-01-05T22:14:41',\n",
       "  'document_date_modified': '2024-01-11T19:49:33'},\n",
       " {'text': 'This ensures that only the GPUs that are needed by the CUDA process are made available to it.',\n",
       "  'text_components': ['This ensures that only the GPUs that are needed by the CUDA process are made available to it.'],\n",
       "  'paragraph_index': [6],\n",
       "  'word_count': [18],\n",
       "  'paragraph_sentence_index': [2],\n",
       "  'only_code': [False],\n",
       "  'contains_code': [True],\n",
       "  'document_title': 'Improving CUDA Initialization Times Using cgroups in Certain Scenarios',\n",
       "  'document_url': 'https://developer.nvidia.com/blog/improving-cuda-initialization-times-using-cgroups-in-certain-scenarios/',\n",
       "  'document_date': '2024-01-05T22:14:41',\n",
       "  'document_date_modified': '2024-01-11T19:49:33'},\n",
       " {'text': 'The following code provides a low-level example of how to employ ```cgroups``` and fully isolate a GPU to a single process.',\n",
       "  'text_components': ['The following code provides a low-level example of how to employ ```cgroups``` and fully isolate a GPU to a single process.'],\n",
       "  'paragraph_index': [7],\n",
       "  'word_count': [28],\n",
       "  'paragraph_sentence_index': [0],\n",
       "  'only_code': [False],\n",
       "  'contains_code': [True],\n",
       "  'document_title': 'Improving CUDA Initialization Times Using cgroups in Certain Scenarios',\n",
       "  'document_url': 'https://developer.nvidia.com/blog/improving-cuda-initialization-times-using-cgroups-in-certain-scenarios/',\n",
       "  'document_date': '2024-01-05T22:14:41',\n",
       "  'document_date_modified': '2024-01-11T19:49:33'},\n",
       " {'text': 'Be aware that you will likely have to run these commands in a root shell to work properly.',\n",
       "  'text_components': ['Be aware that you will likely have to run these commands in a root shell to work properly.'],\n",
       "  'paragraph_index': [7],\n",
       "  'word_count': [18],\n",
       "  'paragraph_sentence_index': [1],\n",
       "  'only_code': [False],\n",
       "  'contains_code': [True],\n",
       "  'document_title': 'Improving CUDA Initialization Times Using cgroups in Certain Scenarios',\n",
       "  'document_url': 'https://developer.nvidia.com/blog/improving-cuda-initialization-times-using-cgroups-in-certain-scenarios/',\n",
       "  'document_date': '2024-01-05T22:14:41',\n",
       "  'document_date_modified': '2024-01-11T19:49:33'},\n",
       " {'text': 'We show a more convenient, higher-level utility later in this post.',\n",
       "  'text_components': ['We show a more convenient, higher-level utility later in this post.'],\n",
       "  'paragraph_index': [7],\n",
       "  'word_count': [12],\n",
       "  'paragraph_sentence_index': [2],\n",
       "  'only_code': [False],\n",
       "  'contains_code': [True],\n",
       "  'document_title': 'Improving CUDA Initialization Times Using cgroups in Certain Scenarios',\n",
       "  'document_url': 'https://developer.nvidia.com/blog/improving-cuda-initialization-times-using-cgroups-in-certain-scenarios/',\n",
       "  'document_date': '2024-01-05T22:14:41',\n",
       "  'document_date_modified': '2024-01-11T19:49:33'},\n",
       " {'text': '```',\n",
       "  'text_components': ['```'],\n",
       "  'paragraph_index': [8],\n",
       "  'word_count': [0],\n",
       "  'paragraph_sentence_index': [0],\n",
       "  'only_code': [True],\n",
       "  'contains_code': [True],\n",
       "  'document_title': 'Improving CUDA Initialization Times Using cgroups in Certain Scenarios',\n",
       "  'document_url': 'https://developer.nvidia.com/blog/improving-cuda-initialization-times-using-cgroups-in-certain-scenarios/',\n",
       "  'document_date': '2024-01-05T22:14:41',\n",
       "  'document_date_modified': '2024-01-11T19:49:33'},\n",
       " {'text': '# Create a mountpoint for the cgroup hierarchy as root',\n",
       "  'text_components': ['# Create a mountpoint for the cgroup hierarchy as root'],\n",
       "  'paragraph_index': [8],\n",
       "  'word_count': [9],\n",
       "  'paragraph_sentence_index': [1],\n",
       "  'only_code': [True],\n",
       "  'contains_code': [True],\n",
       "  'document_title': 'Improving CUDA Initialization Times Using cgroups in Certain Scenarios',\n",
       "  'document_url': 'https://developer.nvidia.com/blog/improving-cuda-initialization-times-using-cgroups-in-certain-scenarios/',\n",
       "  'document_date': '2024-01-05T22:14:41',\n",
       "  'document_date_modified': '2024-01-11T19:49:33'},\n",
       " {'text': '$> cd /mnt',\n",
       "  'text_components': ['$> cd /mnt'],\n",
       "  'paragraph_index': [8],\n",
       "  'word_count': [2],\n",
       "  'paragraph_sentence_index': [2],\n",
       "  'only_code': [True],\n",
       "  'contains_code': [True],\n",
       "  'document_title': 'Improving CUDA Initialization Times Using cgroups in Certain Scenarios',\n",
       "  'document_url': 'https://developer.nvidia.com/blog/improving-cuda-initialization-times-using-cgroups-in-certain-scenarios/',\n",
       "  'document_date': '2024-01-05T22:14:41',\n",
       "  'document_date_modified': '2024-01-11T19:49:33'},\n",
       " {'text': '$> mkdir cgroupV1Device',\n",
       "  'text_components': ['$> mkdir cgroupV1Device'],\n",
       "  'paragraph_index': [8],\n",
       "  'word_count': [2],\n",
       "  'paragraph_sentence_index': [3],\n",
       "  'only_code': [True],\n",
       "  'contains_code': [True],\n",
       "  'document_title': 'Improving CUDA Initialization Times Using cgroups in Certain Scenarios',\n",
       "  'document_url': 'https://developer.nvidia.com/blog/improving-cuda-initialization-times-using-cgroups-in-certain-scenarios/',\n",
       "  'document_date': '2024-01-05T22:14:41',\n",
       "  'document_date_modified': '2024-01-11T19:49:33'},\n",
       " {'text': '# Use mount command to mount the hierarchy and attach the device subsystem to it',\n",
       "  'text_components': ['# Use mount command to mount the hierarchy and attach the device subsystem to it'],\n",
       "  'paragraph_index': [8],\n",
       "  'word_count': [14],\n",
       "  'paragraph_sentence_index': [4],\n",
       "  'only_code': [True],\n",
       "  'contains_code': [True],\n",
       "  'document_title': 'Improving CUDA Initialization Times Using cgroups in Certain Scenarios',\n",
       "  'document_url': 'https://developer.nvidia.com/blog/improving-cuda-initialization-times-using-cgroups-in-certain-scenarios/',\n",
       "  'document_date': '2024-01-05T22:14:41',\n",
       "  'document_date_modified': '2024-01-11T19:49:33'},\n",
       " {'text': '$> mount -t cgroup -o devices devices cgroupV1Device',\n",
       "  'text_components': ['$> mount -t cgroup -o devices devices cgroupV1Device'],\n",
       "  'paragraph_index': [8],\n",
       "  'word_count': [7],\n",
       "  'paragraph_sentence_index': [5],\n",
       "  'only_code': [True],\n",
       "  'contains_code': [True],\n",
       "  'document_title': 'Improving CUDA Initialization Times Using cgroups in Certain Scenarios',\n",
       "  'document_url': 'https://developer.nvidia.com/blog/improving-cuda-initialization-times-using-cgroups-in-certain-scenarios/',\n",
       "  'document_date': '2024-01-05T22:14:41',\n",
       "  'document_date_modified': '2024-01-11T19:49:33'},\n",
       " {'text': '$> cd cgroupV1Device',\n",
       "  'text_components': ['$> cd cgroupV1Device'],\n",
       "  'paragraph_index': [8],\n",
       "  'word_count': [2],\n",
       "  'paragraph_sentence_index': [6],\n",
       "  'only_code': [True],\n",
       "  'contains_code': [True],\n",
       "  'document_title': 'Improving CUDA Initialization Times Using cgroups in Certain Scenarios',\n",
       "  'document_url': 'https://developer.nvidia.com/blog/improving-cuda-initialization-times-using-cgroups-in-certain-scenarios/',\n",
       "  'document_date': '2024-01-05T22:14:41',\n",
       "  'document_date_modified': '2024-01-11T19:49:33'},\n",
       " {'text': '# Now create a gpu subgroup directory to restrict/allow GPU access',\n",
       "  'text_components': ['# Now create a gpu subgroup directory to restrict/allow GPU access'],\n",
       "  'paragraph_index': [8],\n",
       "  'word_count': [11],\n",
       "  'paragraph_sentence_index': [7],\n",
       "  'only_code': [True],\n",
       "  'contains_code': [True],\n",
       "  'document_title': 'Improving CUDA Initialization Times Using cgroups in Certain Scenarios',\n",
       "  'document_url': 'https://developer.nvidia.com/blog/improving-cuda-initialization-times-using-cgroups-in-certain-scenarios/',\n",
       "  'document_date': '2024-01-05T22:14:41',\n",
       "  'document_date_modified': '2024-01-11T19:49:33'},\n",
       " {'text': '$> mkdir gpugroup',\n",
       "  'text_components': ['$> mkdir gpugroup'],\n",
       "  'paragraph_index': [8],\n",
       "  'word_count': [2],\n",
       "  'paragraph_sentence_index': [8],\n",
       "  'only_code': [True],\n",
       "  'contains_code': [True],\n",
       "  'document_title': 'Improving CUDA Initialization Times Using cgroups in Certain Scenarios',\n",
       "  'document_url': 'https://developer.nvidia.com/blog/improving-cuda-initialization-times-using-cgroups-in-certain-scenarios/',\n",
       "  'document_date': '2024-01-05T22:14:41',\n",
       "  'document_date_modified': '2024-01-11T19:49:33'},\n",
       " {'text': '$> cd gpugroup',\n",
       "  'text_components': ['$> cd gpugroup'],\n",
       "  'paragraph_index': [8],\n",
       "  'word_count': [2],\n",
       "  'paragraph_sentence_index': [9],\n",
       "  'only_code': [True],\n",
       "  'contains_code': [True],\n",
       "  'document_title': 'Improving CUDA Initialization Times Using cgroups in Certain Scenarios',\n",
       "  'document_url': 'https://developer.nvidia.com/blog/improving-cuda-initialization-times-using-cgroups-in-certain-scenarios/',\n",
       "  'document_date': '2024-01-05T22:14:41',\n",
       "  'document_date_modified': '2024-01-11T19:49:33'},\n",
       " {'text': '# in the gpugroup, you will see many cgroupfs files, the ones that interest us are tasks, device.deny and device.allow',\n",
       "  'text_components': ['# in the gpugroup, you will see many cgroupfs files, the ones that interest us are tasks, device.deny and device.allow'],\n",
       "  'paragraph_index': [8],\n",
       "  'word_count': [21],\n",
       "  'paragraph_sentence_index': [10],\n",
       "  'only_code': [True],\n",
       "  'contains_code': [True],\n",
       "  'document_title': 'Improving CUDA Initialization Times Using cgroups in Certain Scenarios',\n",
       "  'document_url': 'https://developer.nvidia.com/blog/improving-cuda-initialization-times-using-cgroups-in-certain-scenarios/',\n",
       "  'document_date': '2024-01-05T22:14:41',\n",
       "  'document_date_modified': '2024-01-11T19:49:33'},\n",
       " {'text': '$> ls gpugroup',\n",
       "  'text_components': ['$> ls gpugroup'],\n",
       "  'paragraph_index': [8],\n",
       "  'word_count': [2],\n",
       "  'paragraph_sentence_index': [11],\n",
       "  'only_code': [True],\n",
       "  'contains_code': [True],\n",
       "  'document_title': 'Improving CUDA Initialization Times Using cgroups in Certain Scenarios',\n",
       "  'document_url': 'https://developer.nvidia.com/blog/improving-cuda-initialization-times-using-cgroups-in-certain-scenarios/',\n",
       "  'document_date': '2024-01-05T22:14:41',\n",
       "  'document_date_modified': '2024-01-11T19:49:33'},\n",
       " {'text': 'tasks      devices.deny     devices.allow',\n",
       "  'text_components': ['tasks      devices.deny     devices.allow'],\n",
       "  'paragraph_index': [8],\n",
       "  'word_count': [5],\n",
       "  'paragraph_sentence_index': [12],\n",
       "  'only_code': [True],\n",
       "  'contains_code': [True],\n",
       "  'document_title': 'Improving CUDA Initialization Times Using cgroups in Certain Scenarios',\n",
       "  'document_url': 'https://developer.nvidia.com/blog/improving-cuda-initialization-times-using-cgroups-in-certain-scenarios/',\n",
       "  'document_date': '2024-01-05T22:14:41',\n",
       "  'document_date_modified': '2024-01-11T19:49:33'},\n",
       " {'text': '# Launch a shell from where the CUDA process will be executed. Gets the shells PID',\n",
       "  'text_components': ['# Launch a shell from where the CUDA process will be executed. Gets the shells PID'],\n",
       "  'paragraph_index': [8],\n",
       "  'word_count': [15],\n",
       "  'paragraph_sentence_index': [13],\n",
       "  'only_code': [True],\n",
       "  'contains_code': [True],\n",
       "  'document_title': 'Improving CUDA Initialization Times Using cgroups in Certain Scenarios',\n",
       "  'document_url': 'https://developer.nvidia.com/blog/improving-cuda-initialization-times-using-cgroups-in-certain-scenarios/',\n",
       "  'document_date': '2024-01-05T22:14:41',\n",
       "  'document_date_modified': '2024-01-11T19:49:33'},\n",
       " {'text': '$> echo $$',\n",
       "  'text_components': ['$> echo $$'],\n",
       "  'paragraph_index': [8],\n",
       "  'word_count': [1],\n",
       "  'paragraph_sentence_index': [14],\n",
       "  'only_code': [True],\n",
       "  'contains_code': [True],\n",
       "  'document_title': 'Improving CUDA Initialization Times Using cgroups in Certain Scenarios',\n",
       "  'document_url': 'https://developer.nvidia.com/blog/improving-cuda-initialization-times-using-cgroups-in-certain-scenarios/',\n",
       "  'document_date': '2024-01-05T22:14:41',\n",
       "  'document_date_modified': '2024-01-11T19:49:33'},\n",
       " {'text': '# Write this PID into the tasks files in the gpugroups folder',\n",
       "  'text_components': ['# Write this PID into the tasks files in the gpugroups folder'],\n",
       "  'paragraph_index': [8],\n",
       "  'word_count': [11],\n",
       "  'paragraph_sentence_index': [15],\n",
       "  'only_code': [True],\n",
       "  'contains_code': [True],\n",
       "  'document_title': 'Improving CUDA Initialization Times Using cgroups in Certain Scenarios',\n",
       "  'document_url': 'https://developer.nvidia.com/blog/improving-cuda-initialization-times-using-cgroups-in-certain-scenarios/',\n",
       "  'document_date': '2024-01-05T22:14:41',\n",
       "  'document_date_modified': '2024-01-11T19:49:33'},\n",
       " {'text': '$> echo <PID> tasks',\n",
       "  'text_components': ['$> echo <PID> tasks'],\n",
       "  'paragraph_index': [8],\n",
       "  'word_count': [3],\n",
       "  'paragraph_sentence_index': [16],\n",
       "  'only_code': [True],\n",
       "  'contains_code': [True],\n",
       "  'document_title': 'Improving CUDA Initialization Times Using cgroups in Certain Scenarios',\n",
       "  'document_url': 'https://developer.nvidia.com/blog/improving-cuda-initialization-times-using-cgroups-in-certain-scenarios/',\n",
       "  'document_date': '2024-01-05T22:14:41',\n",
       "  'document_date_modified': '2024-01-11T19:49:33'},\n",
       " {'text': '# List the device numbers of nvidia devices with the ls command',\n",
       "  'text_components': ['# List the device numbers of nvidia devices with the ls command'],\n",
       "  'paragraph_index': [8],\n",
       "  'word_count': [11],\n",
       "  'paragraph_sentence_index': [17],\n",
       "  'only_code': [True],\n",
       "  'contains_code': [True],\n",
       "  'document_title': 'Improving CUDA Initialization Times Using cgroups in Certain Scenarios',\n",
       "  'document_url': 'https://developer.nvidia.com/blog/improving-cuda-initialization-times-using-cgroups-in-certain-scenarios/',\n",
       "  'document_date': '2024-01-05T22:14:41',\n",
       "  'document_date_modified': '2024-01-11T19:49:33'},\n",
       " {'text': '$> ls -l /dev/nvidia*',\n",
       "  'text_components': ['$> ls -l /dev/nvidia*'],\n",
       "  'paragraph_index': [8],\n",
       "  'word_count': [4],\n",
       "  'paragraph_sentence_index': [18],\n",
       "  'only_code': [True],\n",
       "  'contains_code': [True],\n",
       "  'document_title': 'Improving CUDA Initialization Times Using cgroups in Certain Scenarios',\n",
       "  'document_url': 'https://developer.nvidia.com/blog/improving-cuda-initialization-times-using-cgroups-in-certain-scenarios/',\n",
       "  'document_date': '2024-01-05T22:14:41',\n",
       "  'document_date_modified': '2024-01-11T19:49:33'},\n",
       " {'text': 'crw-rw-rw- 1 root root 195,   0 Jul 11 14:28 /dev/nvidia0',\n",
       "  'text_components': ['crw-rw-rw- 1 root root 195,   0 Jul 11 14:28 /dev/nvidia0'],\n",
       "  'paragraph_index': [8],\n",
       "  'word_count': [14],\n",
       "  'paragraph_sentence_index': [19],\n",
       "  'only_code': [True],\n",
       "  'contains_code': [True],\n",
       "  'document_title': 'Improving CUDA Initialization Times Using cgroups in Certain Scenarios',\n",
       "  'document_url': 'https://developer.nvidia.com/blog/improving-cuda-initialization-times-using-cgroups-in-certain-scenarios/',\n",
       "  'document_date': '2024-01-05T22:14:41',\n",
       "  'document_date_modified': '2024-01-11T19:49:33'},\n",
       " {'text': 'crw-rw-rw- 1 root root 195,   0 Jul 11 14:28 /dev/nvidia1',\n",
       "  'text_components': ['crw-rw-rw- 1 root root 195,   0 Jul 11 14:28 /dev/nvidia1'],\n",
       "  'paragraph_index': [8],\n",
       "  'word_count': [14],\n",
       "  'paragraph_sentence_index': [20],\n",
       "  'only_code': [True],\n",
       "  'contains_code': [True],\n",
       "  'document_title': 'Improving CUDA Initialization Times Using cgroups in Certain Scenarios',\n",
       "  'document_url': 'https://developer.nvidia.com/blog/improving-cuda-initialization-times-using-cgroups-in-certain-scenarios/',\n",
       "  'document_date': '2024-01-05T22:14:41',\n",
       "  'document_date_modified': '2024-01-11T19:49:33'},\n",
       " {'text': '# Assuming that you only want to allow the CUDA process to access GPU0, you deny the CUDA process access to GPU1 by writing the following command to devices.deny',\n",
       "  'text_components': ['# Assuming that you only want to allow the CUDA process to access GPU0, you deny the CUDA process access to GPU1 by writing the following command to devices.deny'],\n",
       "  'paragraph_index': [8],\n",
       "  'word_count': [29],\n",
       "  'paragraph_sentence_index': [21],\n",
       "  'only_code': [True],\n",
       "  'contains_code': [True],\n",
       "  'document_title': 'Improving CUDA Initialization Times Using cgroups in Certain Scenarios',\n",
       "  'document_url': 'https://developer.nvidia.com/blog/improving-cuda-initialization-times-using-cgroups-in-certain-scenarios/',\n",
       "  'document_date': '2024-01-05T22:14:41',\n",
       "  'document_date_modified': '2024-01-11T19:49:33'},\n",
       " {'text': \"$> echo 'c 195:1 rmw' > devices.deny\",\n",
       "  'text_components': [\"$> echo 'c 195:1 rmw' > devices.deny\"],\n",
       "  'paragraph_index': [8],\n",
       "  'word_count': [7],\n",
       "  'paragraph_sentence_index': [22],\n",
       "  'only_code': [True],\n",
       "  'contains_code': [True],\n",
       "  'document_title': 'Improving CUDA Initialization Times Using cgroups in Certain Scenarios',\n",
       "  'document_url': 'https://developer.nvidia.com/blog/improving-cuda-initialization-times-using-cgroups-in-certain-scenarios/',\n",
       "  'document_date': '2024-01-05T22:14:41',\n",
       "  'document_date_modified': '2024-01-11T19:49:33'},\n",
       " {'text': '# Now GPU1 will not be visible to The CUDA process that you launch from the second shell.',\n",
       "  'text_components': ['# Now GPU1 will not be visible to The CUDA process that you launch from the second shell.'],\n",
       "  'paragraph_index': [8],\n",
       "  'word_count': [17],\n",
       "  'paragraph_sentence_index': [23],\n",
       "  'only_code': [True],\n",
       "  'contains_code': [True],\n",
       "  'document_title': 'Improving CUDA Initialization Times Using cgroups in Certain Scenarios',\n",
       "  'document_url': 'https://developer.nvidia.com/blog/improving-cuda-initialization-times-using-cgroups-in-certain-scenarios/',\n",
       "  'document_date': '2024-01-05T22:14:41',\n",
       "  'document_date_modified': '2024-01-11T19:49:33'},\n",
       " {'text': '# To provide the CUDA process access to GPU1, we should write the following to devices.allow',\n",
       "  'text_components': ['# To provide the CUDA process access to GPU1, we should write the following to devices.allow'],\n",
       "  'paragraph_index': [8],\n",
       "  'word_count': [16],\n",
       "  'paragraph_sentence_index': [24],\n",
       "  'only_code': [True],\n",
       "  'contains_code': [True],\n",
       "  'document_title': 'Improving CUDA Initialization Times Using cgroups in Certain Scenarios',\n",
       "  'document_url': 'https://developer.nvidia.com/blog/improving-cuda-initialization-times-using-cgroups-in-certain-scenarios/',\n",
       "  'document_date': '2024-01-05T22:14:41',\n",
       "  'document_date_modified': '2024-01-11T19:49:33'},\n",
       " {'text': \"$> echo 'c 195:1 rmw' > devices.allow\",\n",
       "  'text_components': [\"$> echo 'c 195:1 rmw' > devices.allow\"],\n",
       "  'paragraph_index': [8],\n",
       "  'word_count': [7],\n",
       "  'paragraph_sentence_index': [25],\n",
       "  'only_code': [True],\n",
       "  'contains_code': [True],\n",
       "  'document_title': 'Improving CUDA Initialization Times Using cgroups in Certain Scenarios',\n",
       "  'document_url': 'https://developer.nvidia.com/blog/improving-cuda-initialization-times-using-cgroups-in-certain-scenarios/',\n",
       "  'document_date': '2024-01-05T22:14:41',\n",
       "  'document_date_modified': '2024-01-11T19:49:33'},\n",
       " {'text': '```',\n",
       "  'text_components': ['```'],\n",
       "  'paragraph_index': [8],\n",
       "  'word_count': [0],\n",
       "  'paragraph_sentence_index': [26],\n",
       "  'only_code': [True],\n",
       "  'contains_code': [True],\n",
       "  'document_title': 'Improving CUDA Initialization Times Using cgroups in Certain Scenarios',\n",
       "  'document_url': 'https://developer.nvidia.com/blog/improving-cuda-initialization-times-using-cgroups-in-certain-scenarios/',\n",
       "  'document_date': '2024-01-05T22:14:41',\n",
       "  'document_date_modified': '2024-01-11T19:49:33'},\n",
       " {'text': 'When you are done with the tasks, unmount the ```/cgroupV1Device``` folder with the umount command.',\n",
       "  'text_components': ['When you are done with the tasks, unmount the ```/cgroupV1Device``` folder with the umount command.'],\n",
       "  'paragraph_index': [9],\n",
       "  'word_count': [21],\n",
       "  'paragraph_sentence_index': [0],\n",
       "  'only_code': [False],\n",
       "  'contains_code': [True],\n",
       "  'document_title': 'Improving CUDA Initialization Times Using cgroups in Certain Scenarios',\n",
       "  'document_url': 'https://developer.nvidia.com/blog/improving-cuda-initialization-times-using-cgroups-in-certain-scenarios/',\n",
       "  'document_date': '2024-01-05T22:14:41',\n",
       "  'document_date_modified': '2024-01-11T19:49:33'},\n",
       " {'text': '```',\n",
       "  'text_components': ['```'],\n",
       "  'paragraph_index': [10],\n",
       "  'word_count': [0],\n",
       "  'paragraph_sentence_index': [0],\n",
       "  'only_code': [True],\n",
       "  'contains_code': [True],\n",
       "  'document_title': 'Improving CUDA Initialization Times Using cgroups in Certain Scenarios',\n",
       "  'document_url': 'https://developer.nvidia.com/blog/improving-cuda-initialization-times-using-cgroups-in-certain-scenarios/',\n",
       "  'document_date': '2024-01-05T22:14:41',\n",
       "  'document_date_modified': '2024-01-11T19:49:33'},\n",
       " {'text': 'umount /mnt/cgroupV1Device',\n",
       "  'text_components': ['umount /mnt/cgroupV1Device'],\n",
       "  'paragraph_index': [10],\n",
       "  'word_count': [3],\n",
       "  'paragraph_sentence_index': [1],\n",
       "  'only_code': [True],\n",
       "  'contains_code': [True],\n",
       "  'document_title': 'Improving CUDA Initialization Times Using cgroups in Certain Scenarios',\n",
       "  'document_url': 'https://developer.nvidia.com/blog/improving-cuda-initialization-times-using-cgroups-in-certain-scenarios/',\n",
       "  'document_date': '2024-01-05T22:14:41',\n",
       "  'document_date_modified': '2024-01-11T19:49:33'},\n",
       " {'text': '```',\n",
       "  'text_components': ['```'],\n",
       "  'paragraph_index': [10],\n",
       "  'word_count': [0],\n",
       "  'paragraph_sentence_index': [2],\n",
       "  'only_code': [True],\n",
       "  'contains_code': [True],\n",
       "  'document_title': 'Improving CUDA Initialization Times Using cgroups in Certain Scenarios',\n",
       "  'document_url': 'https://developer.nvidia.com/blog/improving-cuda-initialization-times-using-cgroups-in-certain-scenarios/',\n",
       "  'document_date': '2024-01-05T22:14:41',\n",
       "  'document_date_modified': '2024-01-11T19:49:33'},\n",
       " {'text': 'To allow or deny the user access to any other GPUs on the system, write those GPU numbers to the appropriate file.',\n",
       "  'text_components': ['To allow or deny the user access to any other GPUs on the system, write those GPU numbers to the appropriate file.'],\n",
       "  'paragraph_index': [11],\n",
       "  'word_count': [22],\n",
       "  'paragraph_sentence_index': [0],\n",
       "  'only_code': [False],\n",
       "  'contains_code': [False],\n",
       "  'document_title': 'Improving CUDA Initialization Times Using cgroups in Certain Scenarios',\n",
       "  'document_url': 'https://developer.nvidia.com/blog/improving-cuda-initialization-times-using-cgroups-in-certain-scenarios/',\n",
       "  'document_date': '2024-01-05T22:14:41',\n",
       "  'document_date_modified': '2024-01-11T19:49:33'},\n",
       " {'text': 'Here’s an example of denying access to only GPU5 and GPU6 on a multi-GPU system.',\n",
       "  'text_components': ['Here’s an example of denying access to only GPU5 and GPU6 on a multi-GPU system.'],\n",
       "  'paragraph_index': [11],\n",
       "  'word_count': [17],\n",
       "  'paragraph_sentence_index': [1],\n",
       "  'only_code': [False],\n",
       "  'contains_code': [False],\n",
       "  'document_title': 'Improving CUDA Initialization Times Using cgroups in Certain Scenarios',\n",
       "  'document_url': 'https://developer.nvidia.com/blog/improving-cuda-initialization-times-using-cgroups-in-certain-scenarios/',\n",
       "  'document_date': '2024-01-05T22:14:41',\n",
       "  'document_date_modified': '2024-01-11T19:49:33'},\n",
       " {'text': 'In the ```/gpugroup``` folder created earlier, write the PID of the shell from where the CUDA process is to be launched into the ```tasks``` file:',\n",
       "  'text_components': ['In the ```/gpugroup``` folder created earlier, write the PID of the shell from where the CUDA process is to be launched into the ```tasks``` file:'],\n",
       "  'paragraph_index': [12],\n",
       "  'word_count': [37],\n",
       "  'paragraph_sentence_index': [0],\n",
       "  'only_code': [False],\n",
       "  'contains_code': [True],\n",
       "  'document_title': 'Improving CUDA Initialization Times Using cgroups in Certain Scenarios',\n",
       "  'document_url': 'https://developer.nvidia.com/blog/improving-cuda-initialization-times-using-cgroups-in-certain-scenarios/',\n",
       "  'document_date': '2024-01-05T22:14:41',\n",
       "  'document_date_modified': '2024-01-11T19:49:33'},\n",
       " {'text': '```',\n",
       "  'text_components': ['```'],\n",
       "  'paragraph_index': [13],\n",
       "  'word_count': [0],\n",
       "  'paragraph_sentence_index': [0],\n",
       "  'only_code': [True],\n",
       "  'contains_code': [True],\n",
       "  'document_title': 'Improving CUDA Initialization Times Using cgroups in Certain Scenarios',\n",
       "  'document_url': 'https://developer.nvidia.com/blog/improving-cuda-initialization-times-using-cgroups-in-certain-scenarios/',\n",
       "  'document_date': '2024-01-05T22:14:41',\n",
       "  'document_date_modified': '2024-01-11T19:49:33'},\n",
       " {'text': '$> echo <PID> tasks',\n",
       "  'text_components': ['$> echo <PID> tasks'],\n",
       "  'paragraph_index': [13],\n",
       "  'word_count': [3],\n",
       "  'paragraph_sentence_index': [1],\n",
       "  'only_code': [True],\n",
       "  'contains_code': [True],\n",
       "  'document_title': 'Improving CUDA Initialization Times Using cgroups in Certain Scenarios',\n",
       "  'document_url': 'https://developer.nvidia.com/blog/improving-cuda-initialization-times-using-cgroups-in-certain-scenarios/',\n",
       "  'document_date': '2024-01-05T22:14:41',\n",
       "  'document_date_modified': '2024-01-11T19:49:33'},\n",
       " {'text': '```',\n",
       "  'text_components': ['```'],\n",
       "  'paragraph_index': [13],\n",
       "  'word_count': [0],\n",
       "  'paragraph_sentence_index': [2],\n",
       "  'only_code': [True],\n",
       "  'contains_code': [True],\n",
       "  'document_title': 'Improving CUDA Initialization Times Using cgroups in Certain Scenarios',\n",
       "  'document_url': 'https://developer.nvidia.com/blog/improving-cuda-initialization-times-using-cgroups-in-certain-scenarios/',\n",
       "  'document_date': '2024-01-05T22:14:41',\n",
       "  'document_date_modified': '2024-01-11T19:49:33'},\n",
       " {'text': 'Now add GPU5 and GPU6 to the denied list:',\n",
       "  'text_components': ['Now add GPU5 and GPU6 to the denied list:'],\n",
       "  'paragraph_index': [14],\n",
       "  'word_count': [9],\n",
       "  'paragraph_sentence_index': [0],\n",
       "  'only_code': [False],\n",
       "  'contains_code': [False],\n",
       "  'document_title': 'Improving CUDA Initialization Times Using cgroups in Certain Scenarios',\n",
       "  'document_url': 'https://developer.nvidia.com/blog/improving-cuda-initialization-times-using-cgroups-in-certain-scenarios/',\n",
       "  'document_date': '2024-01-05T22:14:41',\n",
       "  'document_date_modified': '2024-01-11T19:49:33'},\n",
       " {'text': '```',\n",
       "  'text_components': ['```'],\n",
       "  'paragraph_index': [15],\n",
       "  'word_count': [0],\n",
       "  'paragraph_sentence_index': [0],\n",
       "  'only_code': [True],\n",
       "  'contains_code': [True],\n",
       "  'document_title': 'Improving CUDA Initialization Times Using cgroups in Certain Scenarios',\n",
       "  'document_url': 'https://developer.nvidia.com/blog/improving-cuda-initialization-times-using-cgroups-in-certain-scenarios/',\n",
       "  'document_date': '2024-01-05T22:14:41',\n",
       "  'document_date_modified': '2024-01-11T19:49:33'},\n",
       " {'text': \"$> echo 'c 195:5 rmw' > devices.deny\",\n",
       "  'text_components': [\"$> echo 'c 195:5 rmw' > devices.deny\"],\n",
       "  'paragraph_index': [15],\n",
       "  'word_count': [7],\n",
       "  'paragraph_sentence_index': [1],\n",
       "  'only_code': [True],\n",
       "  'contains_code': [True],\n",
       "  'document_title': 'Improving CUDA Initialization Times Using cgroups in Certain Scenarios',\n",
       "  'document_url': 'https://developer.nvidia.com/blog/improving-cuda-initialization-times-using-cgroups-in-certain-scenarios/',\n",
       "  'document_date': '2024-01-05T22:14:41',\n",
       "  'document_date_modified': '2024-01-11T19:49:33'},\n",
       " {'text': \"$> echo 'c 195:6 rmw' > devices.deny\",\n",
       "  'text_components': [\"$> echo 'c 195:6 rmw' > devices.deny\"],\n",
       "  'paragraph_index': [15],\n",
       "  'word_count': [7],\n",
       "  'paragraph_sentence_index': [2],\n",
       "  'only_code': [True],\n",
       "  'contains_code': [True],\n",
       "  'document_title': 'Improving CUDA Initialization Times Using cgroups in Certain Scenarios',\n",
       "  'document_url': 'https://developer.nvidia.com/blog/improving-cuda-initialization-times-using-cgroups-in-certain-scenarios/',\n",
       "  'document_date': '2024-01-05T22:14:41',\n",
       "  'document_date_modified': '2024-01-11T19:49:33'},\n",
       " {'text': '```',\n",
       "  'text_components': ['```'],\n",
       "  'paragraph_index': [15],\n",
       "  'word_count': [0],\n",
       "  'paragraph_sentence_index': [3],\n",
       "  'only_code': [True],\n",
       "  'contains_code': [True],\n",
       "  'document_title': 'Improving CUDA Initialization Times Using cgroups in Certain Scenarios',\n",
       "  'document_url': 'https://developer.nvidia.com/blog/improving-cuda-initialization-times-using-cgroups-in-certain-scenarios/',\n",
       "  'document_date': '2024-01-05T22:14:41',\n",
       "  'document_date_modified': '2024-01-11T19:49:33'},\n",
       " {'text': 'At this point, the CUDA process can’t see or access the two GPUs.',\n",
       "  'text_components': ['At this point, the CUDA process can’t see or access the two GPUs.'],\n",
       "  'paragraph_index': [16],\n",
       "  'word_count': [14],\n",
       "  'paragraph_sentence_index': [0],\n",
       "  'only_code': [False],\n",
       "  'contains_code': [True],\n",
       "  'document_title': 'Improving CUDA Initialization Times Using cgroups in Certain Scenarios',\n",
       "  'document_url': 'https://developer.nvidia.com/blog/improving-cuda-initialization-times-using-cgroups-in-certain-scenarios/',\n",
       "  'document_date': '2024-01-05T22:14:41',\n",
       "  'document_date_modified': '2024-01-11T19:49:33'},\n",
       " {'text': 'To enable only specific GPUs to a CUDA process, those GPUs should be added to the ```devices.allow``` file and the rest of the GPUs should be added to the ```devices.deny``` file.',\n",
       "  'text_components': ['To enable only specific GPUs to a CUDA process, those GPUs should be added to the ```devices.allow``` file and the rest of the GPUs should be added to the ```devices.deny``` file.'],\n",
       "  'paragraph_index': [16],\n",
       "  'word_count': [43],\n",
       "  'paragraph_sentence_index': [1],\n",
       "  'only_code': [False],\n",
       "  'contains_code': [True],\n",
       "  'document_title': 'Improving CUDA Initialization Times Using cgroups in Certain Scenarios',\n",
       "  'document_url': 'https://developer.nvidia.com/blog/improving-cuda-initialization-times-using-cgroups-in-certain-scenarios/',\n",
       "  'document_date': '2024-01-05T22:14:41',\n",
       "  'document_date_modified': '2024-01-11T19:49:33'},\n",
       " {'text': 'The access controls apply per process.',\n",
       "  'text_components': ['The access controls apply per process.'],\n",
       "  'paragraph_index': [17],\n",
       "  'word_count': [6],\n",
       "  'paragraph_sentence_index': [0],\n",
       "  'only_code': [False],\n",
       "  'contains_code': [True],\n",
       "  'document_title': 'Improving CUDA Initialization Times Using cgroups in Certain Scenarios',\n",
       "  'document_url': 'https://developer.nvidia.com/blog/improving-cuda-initialization-times-using-cgroups-in-certain-scenarios/',\n",
       "  'document_date': '2024-01-05T22:14:41',\n",
       "  'document_date_modified': '2024-01-11T19:49:33'},\n",
       " {'text': 'Multiple processes can be added to the ```tasks``` file to propagate the same controls to more than one process.',\n",
       "  'text_components': ['Multiple processes can be added to the ```tasks``` file to propagate the same controls to more than one process.'],\n",
       "  'paragraph_index': [17],\n",
       "  'word_count': [25],\n",
       "  'paragraph_sentence_index': [1],\n",
       "  'only_code': [False],\n",
       "  'contains_code': [True],\n",
       "  'document_title': 'Improving CUDA Initialization Times Using cgroups in Certain Scenarios',\n",
       "  'document_url': 'https://developer.nvidia.com/blog/improving-cuda-initialization-times-using-cgroups-in-certain-scenarios/',\n",
       "  'document_date': '2024-01-05T22:14:41',\n",
       "  'document_date_modified': '2024-01-11T19:49:33'},\n",
       " {'text': 'Isolating GPUs using the bubblewrap utility',\n",
       "  'text_components': ['Isolating GPUs using the bubblewrap utility'],\n",
       "  'paragraph_index': [18],\n",
       "  'word_count': [6],\n",
       "  'paragraph_sentence_index': [0],\n",
       "  'only_code': [False],\n",
       "  'contains_code': [False],\n",
       "  'document_title': 'Improving CUDA Initialization Times Using cgroups in Certain Scenarios',\n",
       "  'document_url': 'https://developer.nvidia.com/blog/improving-cuda-initialization-times-using-cgroups-in-certain-scenarios/',\n",
       "  'document_date': '2024-01-05T22:14:41',\n",
       "  'document_date_modified': '2024-01-11T19:49:33'},\n",
       " {'text': 'The bubblewrap utility (bwrap) is a higher-level utility available for sandboxing and access control in Linux operating systems, which can be used to achieve the same effect as the solution presented earlier.',\n",
       "  'text_components': ['The bubblewrap utility (bwrap) is a higher-level utility available for sandboxing and access control in Linux operating systems, which can be used to achieve the same effect as the solution presented earlier.'],\n",
       "  'paragraph_index': [19],\n",
       "  'word_count': [33],\n",
       "  'paragraph_sentence_index': [0],\n",
       "  'only_code': [False],\n",
       "  'contains_code': [False],\n",
       "  'document_title': 'Improving CUDA Initialization Times Using cgroups in Certain Scenarios',\n",
       "  'document_url': 'https://developer.nvidia.com/blog/improving-cuda-initialization-times-using-cgroups-in-certain-scenarios/',\n",
       "  'document_date': '2024-01-05T22:14:41',\n",
       "  'document_date_modified': '2024-01-11T19:49:33'},\n",
       " {'text': 'You can use this to conveniently restrict or allow access to specific GPUs from a CUDA process:',\n",
       "  'text_components': ['You can use this to conveniently restrict or allow access to specific GPUs from a CUDA process:'],\n",
       "  'paragraph_index': [19],\n",
       "  'word_count': [17],\n",
       "  'paragraph_sentence_index': [1],\n",
       "  'only_code': [False],\n",
       "  'contains_code': [False],\n",
       "  'document_title': 'Improving CUDA Initialization Times Using cgroups in Certain Scenarios',\n",
       "  'document_url': 'https://developer.nvidia.com/blog/improving-cuda-initialization-times-using-cgroups-in-certain-scenarios/',\n",
       "  'document_date': '2024-01-05T22:14:41',\n",
       "  'document_date_modified': '2024-01-11T19:49:33'},\n",
       " {'text': '```',\n",
       "  'text_components': ['```'],\n",
       "  'paragraph_index': [20],\n",
       "  'word_count': [0],\n",
       "  'paragraph_sentence_index': [0],\n",
       "  'only_code': [True],\n",
       "  'contains_code': [True],\n",
       "  'document_title': 'Improving CUDA Initialization Times Using cgroups in Certain Scenarios',\n",
       "  'document_url': 'https://developer.nvidia.com/blog/improving-cuda-initialization-times-using-cgroups-in-certain-scenarios/',\n",
       "  'document_date': '2024-01-05T22:14:41',\n",
       "  'document_date_modified': '2024-01-11T19:49:33'},\n",
       " {'text': '# install bubblewrap utility on Debian-like systems',\n",
       "  'text_components': ['# install bubblewrap utility on Debian-like systems'],\n",
       "  'paragraph_index': [20],\n",
       "  'word_count': [7],\n",
       "  'paragraph_sentence_index': [1],\n",
       "  'only_code': [True],\n",
       "  'contains_code': [True],\n",
       "  'document_title': 'Improving CUDA Initialization Times Using cgroups in Certain Scenarios',\n",
       "  'document_url': 'https://developer.nvidia.com/blog/improving-cuda-initialization-times-using-cgroups-in-certain-scenarios/',\n",
       "  'document_date': '2024-01-05T22:14:41',\n",
       "  'document_date_modified': '2024-01-11T19:49:33'},\n",
       " {'text': '$>sudo apt-get install -y bubblewrap',\n",
       "  'text_components': ['$>sudo apt-get install -y bubblewrap'],\n",
       "  'paragraph_index': [20],\n",
       "  'word_count': [6],\n",
       "  'paragraph_sentence_index': [2],\n",
       "  'only_code': [True],\n",
       "  'contains_code': [True],\n",
       "  'document_title': 'Improving CUDA Initialization Times Using cgroups in Certain Scenarios',\n",
       "  'document_url': 'https://developer.nvidia.com/blog/improving-cuda-initialization-times-using-cgroups-in-certain-scenarios/',\n",
       "  'document_date': '2024-01-05T22:14:41',\n",
       "  'document_date_modified': '2024-01-11T19:49:33'},\n",
       " {'text': '# create a simple shell script that uses bubblewap for binding the required GPU to the launched process',\n",
       "  'text_components': ['# create a simple shell script that uses bubblewap for binding the required GPU to the launched process'],\n",
       "  'paragraph_index': [20],\n",
       "  'word_count': [17],\n",
       "  'paragraph_sentence_index': [3],\n",
       "  'only_code': [True],\n",
       "  'contains_code': [True],\n",
       "  'document_title': 'Improving CUDA Initialization Times Using cgroups in Certain Scenarios',\n",
       "  'document_url': 'https://developer.nvidia.com/blog/improving-cuda-initialization-times-using-cgroups-in-certain-scenarios/',\n",
       "  'document_date': '2024-01-05T22:14:41',\n",
       "  'document_date_modified': '2024-01-11T19:49:33'},\n",
       " {'text': '#!/bin/sh',\n",
       "  'text_components': ['#!/bin/sh'],\n",
       "  'paragraph_index': [20],\n",
       "  'word_count': [2],\n",
       "  'paragraph_sentence_index': [4],\n",
       "  'only_code': [True],\n",
       "  'contains_code': [True],\n",
       "  'document_title': 'Improving CUDA Initialization Times Using cgroups in Certain Scenarios',\n",
       "  'document_url': 'https://developer.nvidia.com/blog/improving-cuda-initialization-times-using-cgroups-in-certain-scenarios/',\n",
       "  'document_date': '2024-01-05T22:14:41',\n",
       "  'document_date_modified': '2024-01-11T19:49:33'},\n",
       " {'text': '# bwrap.sh',\n",
       "  'text_components': ['# bwrap.sh'],\n",
       "  'paragraph_index': [20],\n",
       "  'word_count': [2],\n",
       "  'paragraph_sentence_index': [5],\n",
       "  'only_code': [True],\n",
       "  'contains_code': [True],\n",
       "  'document_title': 'Improving CUDA Initialization Times Using cgroups in Certain Scenarios',\n",
       "  'document_url': 'https://developer.nvidia.com/blog/improving-cuda-initialization-times-using-cgroups-in-certain-scenarios/',\n",
       "  'document_date': '2024-01-05T22:14:41',\n",
       "  'document_date_modified': '2024-01-11T19:49:33'},\n",
       " {'text': 'GPU=$1;shift   # 0, 1, 2, 3, ..',\n",
       "  'text_components': ['GPU=$1;shift   # 0, 1, 2, 3, ..'],\n",
       "  'paragraph_index': [20],\n",
       "  'word_count': [7],\n",
       "  'paragraph_sentence_index': [6],\n",
       "  'only_code': [True],\n",
       "  'contains_code': [True],\n",
       "  'document_title': 'Improving CUDA Initialization Times Using cgroups in Certain Scenarios',\n",
       "  'document_url': 'https://developer.nvidia.com/blog/improving-cuda-initialization-times-using-cgroups-in-certain-scenarios/',\n",
       "  'document_date': '2024-01-05T22:14:41',\n",
       "  'document_date_modified': '2024-01-11T19:49:33'},\n",
       " {'text': 'if [ \"$GPU\" = \"\" ]; then echo \"missing arg: gpu id\"; exit 1; fi',\n",
       "  'text_components': ['if [ \"$GPU\" = \"\" ]; then echo \"missing arg: gpu id\"; exit 1; fi'],\n",
       "  'paragraph_index': [20],\n",
       "  'word_count': [11],\n",
       "  'paragraph_sentence_index': [7],\n",
       "  'only_code': [True],\n",
       "  'contains_code': [True],\n",
       "  'document_title': 'Improving CUDA Initialization Times Using cgroups in Certain Scenarios',\n",
       "  'document_url': 'https://developer.nvidia.com/blog/improving-cuda-initialization-times-using-cgroups-in-certain-scenarios/',\n",
       "  'document_date': '2024-01-05T22:14:41',\n",
       "  'document_date_modified': '2024-01-11T19:49:33'},\n",
       " {'text': 'bwrap \\\\',\n",
       "  'text_components': ['bwrap \\\\'],\n",
       "  'paragraph_index': [20],\n",
       "  'word_count': [1],\n",
       "  'paragraph_sentence_index': [8],\n",
       "  'only_code': [True],\n",
       "  'contains_code': [True],\n",
       "  'document_title': 'Improving CUDA Initialization Times Using cgroups in Certain Scenarios',\n",
       "  'document_url': 'https://developer.nvidia.com/blog/improving-cuda-initialization-times-using-cgroups-in-certain-scenarios/',\n",
       "  'document_date': '2024-01-05T22:14:41',\n",
       "  'document_date_modified': '2024-01-11T19:49:33'},\n",
       " {'text': '        --bind / / \\\\',\n",
       "  'text_components': ['        --bind / / \\\\'],\n",
       "  'paragraph_index': [20],\n",
       "  'word_count': [1],\n",
       "  'paragraph_sentence_index': [9],\n",
       "  'only_code': [True],\n",
       "  'contains_code': [True],\n",
       "  'document_title': 'Improving CUDA Initialization Times Using cgroups in Certain Scenarios',\n",
       "  'document_url': 'https://developer.nvidia.com/blog/improving-cuda-initialization-times-using-cgroups-in-certain-scenarios/',\n",
       "  'document_date': '2024-01-05T22:14:41',\n",
       "  'document_date_modified': '2024-01-11T19:49:33'},\n",
       " {'text': '        --dev /dev --dev-bind /dev/nvidiactl /dev/nvidiactl --dev-bind /dev/nvidia-uvm /dev/nvidia-uvm  \\\\',\n",
       "  'text_components': ['        --dev /dev --dev-bind /dev/nvidiactl /dev/nvidiactl --dev-bind /dev/nvidia-uvm /dev/nvidia-uvm  \\\\'],\n",
       "  'paragraph_index': [20],\n",
       "  'word_count': [16],\n",
       "  'paragraph_sentence_index': [10],\n",
       "  'only_code': [True],\n",
       "  'contains_code': [True],\n",
       "  'document_title': 'Improving CUDA Initialization Times Using cgroups in Certain Scenarios',\n",
       "  'document_url': 'https://developer.nvidia.com/blog/improving-cuda-initialization-times-using-cgroups-in-certain-scenarios/',\n",
       "  'document_date': '2024-01-05T22:14:41',\n",
       "  'document_date_modified': '2024-01-11T19:49:33'},\n",
       " {'text': '        --dev-bind /dev/nvidia$GPU /dev/nvidia$GPU \\\\',\n",
       "  'text_components': ['        --dev-bind /dev/nvidia$GPU /dev/nvidia$GPU \\\\'],\n",
       "  'paragraph_index': [20],\n",
       "  'word_count': [8],\n",
       "  'paragraph_sentence_index': [11],\n",
       "  'only_code': [True],\n",
       "  'contains_code': [True],\n",
       "  'document_title': 'Improving CUDA Initialization Times Using cgroups in Certain Scenarios',\n",
       "  'document_url': 'https://developer.nvidia.com/blog/improving-cuda-initialization-times-using-cgroups-in-certain-scenarios/',\n",
       "  'document_date': '2024-01-05T22:14:41',\n",
       "  'document_date_modified': '2024-01-11T19:49:33'},\n",
       " {'text': '        \"$@\"',\n",
       "  'text_components': ['        \"$@\"'],\n",
       "  'paragraph_index': [20],\n",
       "  'word_count': [0],\n",
       "  'paragraph_sentence_index': [12],\n",
       "  'only_code': [True],\n",
       "  'contains_code': [True],\n",
       "  'document_title': 'Improving CUDA Initialization Times Using cgroups in Certain Scenarios',\n",
       "  'document_url': 'https://developer.nvidia.com/blog/improving-cuda-initialization-times-using-cgroups-in-certain-scenarios/',\n",
       "  'document_date': '2024-01-05T22:14:41',\n",
       "  'document_date_modified': '2024-01-11T19:49:33'},\n",
       " {'text': '# Launch the CUDA process with the bubblewrap utility to only allow access to a specific GPU while running',\n",
       "  'text_components': ['# Launch the CUDA process with the bubblewrap utility to only allow access to a specific GPU while running'],\n",
       "  'paragraph_index': [20],\n",
       "  'word_count': [18],\n",
       "  'paragraph_sentence_index': [13],\n",
       "  'only_code': [True],\n",
       "  'contains_code': [True],\n",
       "  'document_title': 'Improving CUDA Initialization Times Using cgroups in Certain Scenarios',\n",
       "  'document_url': 'https://developer.nvidia.com/blog/improving-cuda-initialization-times-using-cgroups-in-certain-scenarios/',\n",
       "  'document_date': '2024-01-05T22:14:41',\n",
       "  'document_date_modified': '2024-01-11T19:49:33'},\n",
       " {'text': '$> ./bwrap.sh 0 ./test_cuda_app <args>',\n",
       "  'text_components': ['$> ./bwrap.sh 0 ./test_cuda_app <args>'],\n",
       "  'paragraph_index': [20],\n",
       "  'word_count': [5],\n",
       "  'paragraph_sentence_index': [14],\n",
       "  'only_code': [True],\n",
       "  'contains_code': [True],\n",
       "  'document_title': 'Improving CUDA Initialization Times Using cgroups in Certain Scenarios',\n",
       "  'document_url': 'https://developer.nvidia.com/blog/improving-cuda-initialization-times-using-cgroups-in-certain-scenarios/',\n",
       "  'document_date': '2024-01-05T22:14:41',\n",
       "  'document_date_modified': '2024-01-11T19:49:33'},\n",
       " {'text': '```',\n",
       "  'text_components': ['```'],\n",
       "  'paragraph_index': [20],\n",
       "  'word_count': [0],\n",
       "  'paragraph_sentence_index': [15],\n",
       "  'only_code': [True],\n",
       "  'contains_code': [True],\n",
       "  'document_title': 'Improving CUDA Initialization Times Using cgroups in Certain Scenarios',\n",
       "  'document_url': 'https://developer.nvidia.com/blog/improving-cuda-initialization-times-using-cgroups-in-certain-scenarios/',\n",
       "  'document_date': '2024-01-05T22:14:41',\n",
       "  'document_date_modified': '2024-01-11T19:49:33'},\n",
       " {'text': 'More than one GPU can be exposed to a CUDA process by extending the ```dev-bind``` option in the code example.',\n",
       "  'text_components': ['More than one GPU can be exposed to a CUDA process by extending the ```dev-bind``` option in the code example.'],\n",
       "  'paragraph_index': [21],\n",
       "  'word_count': [27],\n",
       "  'paragraph_sentence_index': [0],\n",
       "  'only_code': [False],\n",
       "  'contains_code': [True],\n",
       "  'document_title': 'Improving CUDA Initialization Times Using cgroups in Certain Scenarios',\n",
       "  'document_url': 'https://developer.nvidia.com/blog/improving-cuda-initialization-times-using-cgroups-in-certain-scenarios/',\n",
       "  'document_date': '2024-01-05T22:14:41',\n",
       "  'document_date_modified': '2024-01-11T19:49:33'},\n",
       " {'text': 'Performance benefits of GPU isolation',\n",
       "  'text_components': ['Performance benefits of GPU isolation'],\n",
       "  'paragraph_index': [22],\n",
       "  'word_count': [5],\n",
       "  'paragraph_sentence_index': [0],\n",
       "  'only_code': [False],\n",
       "  'contains_code': [False],\n",
       "  'document_title': 'Improving CUDA Initialization Times Using cgroups in Certain Scenarios',\n",
       "  'document_url': 'https://developer.nvidia.com/blog/improving-cuda-initialization-times-using-cgroups-in-certain-scenarios/',\n",
       "  'document_date': '2024-01-05T22:14:41',\n",
       "  'document_date_modified': '2024-01-11T19:49:33'},\n",
       " {'text': 'In this section, we compare the performance of the CUDA driver initialization API (cuInit) with and without GPU isolation, measured over 256 iterations.',\n",
       "  'text_components': ['In this section, we compare the performance of the CUDA driver initialization API (cuInit) with and without GPU isolation, measured over 256 iterations.'],\n",
       "  'paragraph_index': [23],\n",
       "  'word_count': [23],\n",
       "  'paragraph_sentence_index': [0],\n",
       "  'only_code': [False],\n",
       "  'contains_code': [False],\n",
       "  'document_title': 'Improving CUDA Initialization Times Using cgroups in Certain Scenarios',\n",
       "  'document_url': 'https://developer.nvidia.com/blog/improving-cuda-initialization-times-using-cgroups-in-certain-scenarios/',\n",
       "  'document_date': '2024-01-05T22:14:41',\n",
       "  'document_date_modified': '2024-01-11T19:49:33'},\n",
       " {'text': 'The APIs are being run on an x86-based machine with four A100 class GPUs.',\n",
       "  'text_components': ['The APIs are being run on an x86-based machine with four A100 class GPUs.'],\n",
       "  'paragraph_index': [23],\n",
       "  'word_count': [15],\n",
       "  'paragraph_sentence_index': [1],\n",
       "  'only_code': [False],\n",
       "  'contains_code': [False],\n",
       "  'document_title': 'Improving CUDA Initialization Times Using cgroups in Certain Scenarios',\n",
       "  'document_url': 'https://developer.nvidia.com/blog/improving-cuda-initialization-times-using-cgroups-in-certain-scenarios/',\n",
       "  'document_date': '2024-01-05T22:14:41',\n",
       "  'document_date_modified': '2024-01-11T19:49:33'},\n",
       " {'text': 'Bar graph shows the performance of cuInit API running on an A104 system with and without GPU isolation using cgroups.',\n",
       "  'text_components': ['Bar graph shows the performance of cuInit API running on an A104 system with and without GPU isolation using cgroups.'],\n",
       "  'paragraph_index': [24],\n",
       "  'word_count': [20],\n",
       "  'paragraph_sentence_index': [0],\n",
       "  'only_code': [False],\n",
       "  'contains_code': [False],\n",
       "  'document_title': 'Improving CUDA Initialization Times Using cgroups in Certain Scenarios',\n",
       "  'document_url': 'https://developer.nvidia.com/blog/improving-cuda-initialization-times-using-cgroups-in-certain-scenarios/',\n",
       "  'document_date': '2024-01-05T22:14:41',\n",
       "  'document_date_modified': '2024-01-11T19:49:33'},\n",
       " {'text': 'The bar on the left shows the performance of cuInit when only a single GPU is exposed to the calling CUDA process via cgroups (~65 ms).',\n",
       "  'text_components': ['The bar on the left shows the performance of cuInit when only a single GPU is exposed to the calling CUDA process via cgroups (~65 ms).'],\n",
       "  'paragraph_index': [24],\n",
       "  'word_count': [26],\n",
       "  'paragraph_sentence_index': [1],\n",
       "  'only_code': [False],\n",
       "  'contains_code': [False],\n",
       "  'document_title': 'Improving CUDA Initialization Times Using cgroups in Certain Scenarios',\n",
       "  'document_url': 'https://developer.nvidia.com/blog/improving-cuda-initialization-times-using-cgroups-in-certain-scenarios/',\n",
       "  'document_date': '2024-01-05T22:14:41',\n",
       "  'document_date_modified': '2024-01-11T19:49:33'},\n",
       " {'text': 'The bar on the right shows the performance of cuInit when all four GPUs on the system are made available to the CUDA process (225 ms).',\n",
       "  'text_components': ['The bar on the right shows the performance of cuInit when all four GPUs on the system are made available to the CUDA process (225 ms).'],\n",
       "  'paragraph_index': [24],\n",
       "  'word_count': [26],\n",
       "  'paragraph_sentence_index': [2],\n",
       "  'only_code': [False],\n",
       "  'contains_code': [False],\n",
       "  'document_title': 'Improving CUDA Initialization Times Using cgroups in Certain Scenarios',\n",
       "  'document_url': 'https://developer.nvidia.com/blog/improving-cuda-initialization-times-using-cgroups-in-certain-scenarios/',\n",
       "  'document_date': '2024-01-05T22:14:41',\n",
       "  'document_date_modified': '2024-01-11T19:49:33'},\n",
       " {'text': 'Figure 1.',\n",
       "  'text_components': ['Figure 1.'],\n",
       "  'paragraph_index': [25],\n",
       "  'word_count': [2],\n",
       "  'paragraph_sentence_index': [0],\n",
       "  'only_code': [False],\n",
       "  'contains_code': [False],\n",
       "  'document_title': 'Improving CUDA Initialization Times Using cgroups in Certain Scenarios',\n",
       "  'document_url': 'https://developer.nvidia.com/blog/improving-cuda-initialization-times-using-cgroups-in-certain-scenarios/',\n",
       "  'document_date': '2024-01-05T22:14:41',\n",
       "  'document_date_modified': '2024-01-11T19:49:33'},\n",
       " {'text': 'CUDA initialisation performance comparison between a cgroup-constrained process and the default scenario on a four-GPU test system',\n",
       "  'text_components': ['CUDA initialisation performance comparison between a cgroup-constrained process and the default scenario on a four-GPU test system'],\n",
       "  'paragraph_index': [25],\n",
       "  'word_count': [19],\n",
       "  'paragraph_sentence_index': [1],\n",
       "  'only_code': [False],\n",
       "  'contains_code': [False],\n",
       "  'document_title': 'Improving CUDA Initialization Times Using cgroups in Certain Scenarios',\n",
       "  'document_url': 'https://developer.nvidia.com/blog/improving-cuda-initialization-times-using-cgroups-in-certain-scenarios/',\n",
       "  'document_date': '2024-01-05T22:14:41',\n",
       "  'document_date_modified': '2024-01-11T19:49:33'},\n",
       " {'text': 'Summary',\n",
       "  'text_components': ['Summary'],\n",
       "  'paragraph_index': [26],\n",
       "  'word_count': [1],\n",
       "  'paragraph_sentence_index': [0],\n",
       "  'only_code': [False],\n",
       "  'contains_code': [False],\n",
       "  'document_title': 'Improving CUDA Initialization Times Using cgroups in Certain Scenarios',\n",
       "  'document_url': 'https://developer.nvidia.com/blog/improving-cuda-initialization-times-using-cgroups-in-certain-scenarios/',\n",
       "  'document_date': '2024-01-05T22:14:41',\n",
       "  'document_date_modified': '2024-01-11T19:49:33'},\n",
       " {'text': 'GPU isolation using ```cgroups``` offers you the option of improving CUDA initialization times in a limited number of use cases where all the GPUs on the system are not required to be used by a given CUDA process.',\n",
       "  'text_components': ['GPU isolation using ```cgroups``` offers you the option of improving CUDA initialization times in a limited number of use cases where all the GPUs on the system are not required to be used by a given CUDA process.'],\n",
       "  'paragraph_index': [27],\n",
       "  'word_count': [44],\n",
       "  'paragraph_sentence_index': [0],\n",
       "  'only_code': [False],\n",
       "  'contains_code': [True],\n",
       "  'document_title': 'Improving CUDA Initialization Times Using cgroups in Certain Scenarios',\n",
       "  'document_url': 'https://developer.nvidia.com/blog/improving-cuda-initialization-times-using-cgroups-in-certain-scenarios/',\n",
       "  'document_date': '2024-01-05T22:14:41',\n",
       "  'document_date_modified': '2024-01-11T19:49:33'},\n",
       " {'text': 'For more information, see the following resources:',\n",
       "  'text_components': ['For more information, see the following resources:'],\n",
       "  'paragraph_index': [28],\n",
       "  'word_count': [7],\n",
       "  'paragraph_sentence_index': [0],\n",
       "  'only_code': [False],\n",
       "  'contains_code': [False],\n",
       "  'document_title': 'Improving CUDA Initialization Times Using cgroups in Certain Scenarios',\n",
       "  'document_url': 'https://developer.nvidia.com/blog/improving-cuda-initialization-times-using-cgroups-in-certain-scenarios/',\n",
       "  'document_date': '2024-01-05T22:14:41',\n",
       "  'document_date_modified': '2024-01-11T19:49:33'},\n",
       " {'text': 'Control Groups version 1 — The Linux Kernel documentation',\n",
       "  'text_components': ['Control Groups version 1 — The Linux Kernel documentation'],\n",
       "  'paragraph_index': [29],\n",
       "  'word_count': [8],\n",
       "  'paragraph_sentence_index': [0],\n",
       "  'only_code': [False],\n",
       "  'contains_code': [False],\n",
       "  'document_title': 'Improving CUDA Initialization Times Using cgroups in Certain Scenarios',\n",
       "  'document_url': 'https://developer.nvidia.com/blog/improving-cuda-initialization-times-using-cgroups-in-certain-scenarios/',\n",
       "  'document_date': '2024-01-05T22:14:41',\n",
       "  'document_date_modified': '2024-01-11T19:49:33'},\n",
       " {'text': 'cuInit',\n",
       "  'text_components': ['cuInit'],\n",
       "  'paragraph_index': [30],\n",
       "  'word_count': [1],\n",
       "  'paragraph_sentence_index': [0],\n",
       "  'only_code': [False],\n",
       "  'contains_code': [False],\n",
       "  'document_title': 'Improving CUDA Initialization Times Using cgroups in Certain Scenarios',\n",
       "  'document_url': 'https://developer.nvidia.com/blog/improving-cuda-initialization-times-using-cgroups-in-certain-scenarios/',\n",
       "  'document_date': '2024-01-05T22:14:41',\n",
       "  'document_date_modified': '2024-01-11T19:49:33'}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item = example1\n",
    "\n",
    "document_title = item[\"title\"][\"rendered\"]\n",
    "document_url = item[\"link\"]\n",
    "document_html = item[\"content\"][\"rendered\"]\n",
    "document_date = item[\"date_gmt\"]\n",
    "document_date_modified = item[\"modified_gmt\"]\n",
    "\n",
    "\n",
    "chunk_request(\n",
    "    client,\n",
    "    {\n",
    "        \"strategy\": \"sentence\",\n",
    "        \"input_type\": \"html\",\n",
    "        \"input_str\": document_html,\n",
    "        \"additional_metadata\": {\n",
    "            \"document_title\": document_title,\n",
    "            \"document_url\": document_url,\n",
    "            \"document_date\": document_date,\n",
    "            \"document_date_modified\": document_date_modified,\n",
    "        },\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These chunks are probably too small to be useful--they often refer to information outside of the chunk that's important to understanding it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Increase Minimum Chunk Size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's increase the minimum size of the chunks (measured in number of words) to 250. The chunking service will add sentences to each chunk until we exceed the minimum word count.\n",
    "\n",
    "But what if that border we picked breaks up a meaningful segment of text? We can let chunks overlap--making it more likely that sentences are chunked with any important context in at least one of the chunks on either side of a boundary. Let's set overlap at least 50 words. \n",
    "\n",
    "The chunking microservice uses the Python `spacy` package internally to count words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': 'Many CUDA applications running on multi-GPU platforms usually use a single GPU for their compute needs. In such scenarios, a performance penalty is paid by applications because CUDA has to enumerate/initialize all the GPUs on the system. If a CUDA application does not require other GPUs to be visible and accessible, you can launch such applications by isolating the unwanted GPUs from the CUDA process and eliminating unnecessary initialization steps. This post discusses the various methods to accomplish this and their performance benefits. GPU isolation GPU isolation can be achieved on Linux systems by using Linux tools like ```cgroups```. In this section, we first discuss a lower-level approach and then a higher-level possible approach. Another method exposed by CUDA to isolate devices is the use of ```CUDA_VISIBLE_DEVICES```. Although functionally similar, this approach has limited initialization performance gains compared to the ```cgroups``` approach. Isolating GPUs using cgroups V1 Control groups provide a mechanism for aggregating or partitioning sets of tasks and all their future children into hierarchical groups with specialized behavior. You can use ```cgroups``` to control which GPUs are visible to a CUDA process. This ensures that only the GPUs that are needed by the CUDA process are made available to it. The following code provides a low-level example of how to employ ```cgroups``` and fully isolate a GPU to a single process.',\n",
       "  'text_components': ['Many CUDA applications running on multi-GPU platforms usually use a single GPU for their compute needs.',\n",
       "   'In such scenarios, a performance penalty is paid by applications because CUDA has to enumerate/initialize all the GPUs on the system.',\n",
       "   'If a CUDA application does not require other GPUs to be visible and accessible, you can launch such applications by isolating the unwanted GPUs from the CUDA process and eliminating unnecessary initialization steps.',\n",
       "   'This post discusses the various methods to accomplish this and their performance benefits.',\n",
       "   'GPU isolation',\n",
       "   'GPU isolation can be achieved on Linux systems by using Linux tools like ```cgroups```.',\n",
       "   'In this section, we first discuss a lower-level approach and then a higher-level possible approach.',\n",
       "   'Another method exposed by CUDA to isolate devices is the use of ```CUDA_VISIBLE_DEVICES```.',\n",
       "   'Although functionally similar, this approach has limited initialization performance gains compared to the ```cgroups``` approach.',\n",
       "   'Isolating GPUs using cgroups V1',\n",
       "   'Control groups provide a mechanism for aggregating or partitioning sets of tasks and all their future children into hierarchical groups with specialized behavior.',\n",
       "   'You can use ```cgroups``` to control which GPUs are visible to a CUDA process.',\n",
       "   'This ensures that only the GPUs that are needed by the CUDA process are made available to it.',\n",
       "   'The following code provides a low-level example of how to employ ```cgroups``` and fully isolate a GPU to a single process.'],\n",
       "  'paragraph_index': [0, 0, 0, 1, 2, 3, 3, 4, 4, 5, 6, 6, 6, 7],\n",
       "  'word_count': [17, 22, 33, 13, 2, 20, 17, 19, 21, 5, 23, 20, 18, 28],\n",
       "  'paragraph_sentence_index': [0, 1, 2, 0, 0, 0, 1, 0, 1, 0, 0, 1, 2, 0],\n",
       "  'only_code': [False,\n",
       "   False,\n",
       "   False,\n",
       "   False,\n",
       "   False,\n",
       "   False,\n",
       "   False,\n",
       "   False,\n",
       "   False,\n",
       "   False,\n",
       "   False,\n",
       "   False,\n",
       "   False,\n",
       "   False],\n",
       "  'contains_code': [False,\n",
       "   False,\n",
       "   False,\n",
       "   False,\n",
       "   False,\n",
       "   True,\n",
       "   True,\n",
       "   True,\n",
       "   True,\n",
       "   False,\n",
       "   True,\n",
       "   True,\n",
       "   True,\n",
       "   True],\n",
       "  'document_title': 'Improving CUDA Initialization Times Using cgroups in Certain Scenarios',\n",
       "  'document_url': 'https://developer.nvidia.com/blog/improving-cuda-initialization-times-using-cgroups-in-certain-scenarios/',\n",
       "  'document_date': '2024-01-05T22:14:41',\n",
       "  'document_date_modified': '2024-01-11T19:49:33'},\n",
       " {'text': 'You can use ```cgroups``` to control which GPUs are visible to a CUDA process. This ensures that only the GPUs that are needed by the CUDA process are made available to it. The following code provides a low-level example of how to employ ```cgroups``` and fully isolate a GPU to a single process. Be aware that you will likely have to run these commands in a root shell to work properly. We show a more convenient, higher-level utility later in this post.',\n",
       "  'text_components': ['You can use ```cgroups``` to control which GPUs are visible to a CUDA process.',\n",
       "   'This ensures that only the GPUs that are needed by the CUDA process are made available to it.',\n",
       "   'The following code provides a low-level example of how to employ ```cgroups``` and fully isolate a GPU to a single process.',\n",
       "   'Be aware that you will likely have to run these commands in a root shell to work properly.',\n",
       "   'We show a more convenient, higher-level utility later in this post.'],\n",
       "  'paragraph_index': [6, 6, 7, 7, 7],\n",
       "  'word_count': [20, 18, 28, 18, 12],\n",
       "  'paragraph_sentence_index': [1, 2, 0, 1, 2],\n",
       "  'only_code': [False, False, False, False, False],\n",
       "  'contains_code': [True, True, True, True, True],\n",
       "  'document_title': 'Improving CUDA Initialization Times Using cgroups in Certain Scenarios',\n",
       "  'document_url': 'https://developer.nvidia.com/blog/improving-cuda-initialization-times-using-cgroups-in-certain-scenarios/',\n",
       "  'document_date': '2024-01-05T22:14:41',\n",
       "  'document_date_modified': '2024-01-11T19:49:33'},\n",
       " {'text': \"```\\n# Create a mountpoint for the cgroup hierarchy as root\\n$> cd /mnt\\n$> mkdir cgroupV1Device\\n# Use mount command to mount the hierarchy and attach the device subsystem to it\\n$> mount -t cgroup -o devices devices cgroupV1Device\\n$> cd cgroupV1Device\\n# Now create a gpu subgroup directory to restrict/allow GPU access\\n$> mkdir gpugroup\\n$> cd gpugroup\\n# in the gpugroup, you will see many cgroupfs files, the ones that interest us are tasks, device.deny and device.allow\\n$> ls gpugroup\\ntasks      devices.deny     devices.allow\\n# Launch a shell from where the CUDA process will be executed. Gets the shells PID\\n$> echo $$\\n# Write this PID into the tasks files in the gpugroups folder\\n$> echo <PID> tasks\\n# List the device numbers of nvidia devices with the ls command\\n$> ls -l /dev/nvidia*\\ncrw-rw-rw- 1 root root 195,   0 Jul 11 14:28 /dev/nvidia0\\ncrw-rw-rw- 1 root root 195,   0 Jul 11 14:28 /dev/nvidia1\\n# Assuming that you only want to allow the CUDA process to access GPU0, you deny the CUDA process access to GPU1 by writing the following command to devices.deny\\n$> echo 'c 195:1 rmw' > devices.deny\\n# Now GPU1 will not be visible to The CUDA process that you launch from the second shell.\\n# To provide the CUDA process access to GPU1, we should write the following to devices.allow\\n$> echo 'c 195:1 rmw' > devices.allow\\n```\",\n",
       "  'text_components': ['```',\n",
       "   '# Create a mountpoint for the cgroup hierarchy as root',\n",
       "   '$> cd /mnt',\n",
       "   '$> mkdir cgroupV1Device',\n",
       "   '# Use mount command to mount the hierarchy and attach the device subsystem to it',\n",
       "   '$> mount -t cgroup -o devices devices cgroupV1Device',\n",
       "   '$> cd cgroupV1Device',\n",
       "   '# Now create a gpu subgroup directory to restrict/allow GPU access',\n",
       "   '$> mkdir gpugroup',\n",
       "   '$> cd gpugroup',\n",
       "   '# in the gpugroup, you will see many cgroupfs files, the ones that interest us are tasks, device.deny and device.allow',\n",
       "   '$> ls gpugroup',\n",
       "   'tasks      devices.deny     devices.allow',\n",
       "   '# Launch a shell from where the CUDA process will be executed. Gets the shells PID',\n",
       "   '$> echo $$',\n",
       "   '# Write this PID into the tasks files in the gpugroups folder',\n",
       "   '$> echo <PID> tasks',\n",
       "   '# List the device numbers of nvidia devices with the ls command',\n",
       "   '$> ls -l /dev/nvidia*',\n",
       "   'crw-rw-rw- 1 root root 195,   0 Jul 11 14:28 /dev/nvidia0',\n",
       "   'crw-rw-rw- 1 root root 195,   0 Jul 11 14:28 /dev/nvidia1',\n",
       "   '# Assuming that you only want to allow the CUDA process to access GPU0, you deny the CUDA process access to GPU1 by writing the following command to devices.deny',\n",
       "   \"$> echo 'c 195:1 rmw' > devices.deny\",\n",
       "   '# Now GPU1 will not be visible to The CUDA process that you launch from the second shell.',\n",
       "   '# To provide the CUDA process access to GPU1, we should write the following to devices.allow',\n",
       "   \"$> echo 'c 195:1 rmw' > devices.allow\",\n",
       "   '```'],\n",
       "  'paragraph_index': [8,\n",
       "   8,\n",
       "   8,\n",
       "   8,\n",
       "   8,\n",
       "   8,\n",
       "   8,\n",
       "   8,\n",
       "   8,\n",
       "   8,\n",
       "   8,\n",
       "   8,\n",
       "   8,\n",
       "   8,\n",
       "   8,\n",
       "   8,\n",
       "   8,\n",
       "   8,\n",
       "   8,\n",
       "   8,\n",
       "   8,\n",
       "   8,\n",
       "   8,\n",
       "   8,\n",
       "   8,\n",
       "   8,\n",
       "   8],\n",
       "  'word_count': [0,\n",
       "   9,\n",
       "   2,\n",
       "   2,\n",
       "   14,\n",
       "   7,\n",
       "   2,\n",
       "   11,\n",
       "   2,\n",
       "   2,\n",
       "   21,\n",
       "   2,\n",
       "   5,\n",
       "   15,\n",
       "   1,\n",
       "   11,\n",
       "   3,\n",
       "   11,\n",
       "   4,\n",
       "   14,\n",
       "   14,\n",
       "   29,\n",
       "   7,\n",
       "   17,\n",
       "   16,\n",
       "   7,\n",
       "   0],\n",
       "  'paragraph_sentence_index': [0,\n",
       "   1,\n",
       "   2,\n",
       "   3,\n",
       "   4,\n",
       "   5,\n",
       "   6,\n",
       "   7,\n",
       "   8,\n",
       "   9,\n",
       "   10,\n",
       "   11,\n",
       "   12,\n",
       "   13,\n",
       "   14,\n",
       "   15,\n",
       "   16,\n",
       "   17,\n",
       "   18,\n",
       "   19,\n",
       "   20,\n",
       "   21,\n",
       "   22,\n",
       "   23,\n",
       "   24,\n",
       "   25,\n",
       "   26],\n",
       "  'only_code': [True,\n",
       "   True,\n",
       "   True,\n",
       "   True,\n",
       "   True,\n",
       "   True,\n",
       "   True,\n",
       "   True,\n",
       "   True,\n",
       "   True,\n",
       "   True,\n",
       "   True,\n",
       "   True,\n",
       "   True,\n",
       "   True,\n",
       "   True,\n",
       "   True,\n",
       "   True,\n",
       "   True,\n",
       "   True,\n",
       "   True,\n",
       "   True,\n",
       "   True,\n",
       "   True,\n",
       "   True,\n",
       "   True,\n",
       "   True],\n",
       "  'contains_code': [True,\n",
       "   True,\n",
       "   True,\n",
       "   True,\n",
       "   True,\n",
       "   True,\n",
       "   True,\n",
       "   True,\n",
       "   True,\n",
       "   True,\n",
       "   True,\n",
       "   True,\n",
       "   True,\n",
       "   True,\n",
       "   True,\n",
       "   True,\n",
       "   True,\n",
       "   True,\n",
       "   True,\n",
       "   True,\n",
       "   True,\n",
       "   True,\n",
       "   True,\n",
       "   True,\n",
       "   True,\n",
       "   True,\n",
       "   True],\n",
       "  'document_title': 'Improving CUDA Initialization Times Using cgroups in Certain Scenarios',\n",
       "  'document_url': 'https://developer.nvidia.com/blog/improving-cuda-initialization-times-using-cgroups-in-certain-scenarios/',\n",
       "  'document_date': '2024-01-05T22:14:41',\n",
       "  'document_date_modified': '2024-01-11T19:49:33'},\n",
       " {'text': 'When you are done with the tasks, unmount the ```/cgroupV1Device``` folder with the umount command.',\n",
       "  'text_components': ['When you are done with the tasks, unmount the ```/cgroupV1Device``` folder with the umount command.'],\n",
       "  'paragraph_index': [9],\n",
       "  'word_count': [21],\n",
       "  'paragraph_sentence_index': [0],\n",
       "  'only_code': [False],\n",
       "  'contains_code': [True],\n",
       "  'document_title': 'Improving CUDA Initialization Times Using cgroups in Certain Scenarios',\n",
       "  'document_url': 'https://developer.nvidia.com/blog/improving-cuda-initialization-times-using-cgroups-in-certain-scenarios/',\n",
       "  'document_date': '2024-01-05T22:14:41',\n",
       "  'document_date_modified': '2024-01-11T19:49:33'},\n",
       " {'text': '```\\numount /mnt/cgroupV1Device\\n```',\n",
       "  'text_components': ['```', 'umount /mnt/cgroupV1Device', '```'],\n",
       "  'paragraph_index': [10, 10, 10],\n",
       "  'word_count': [0, 3, 0],\n",
       "  'paragraph_sentence_index': [0, 1, 2],\n",
       "  'only_code': [True, True, True],\n",
       "  'contains_code': [True, True, True],\n",
       "  'document_title': 'Improving CUDA Initialization Times Using cgroups in Certain Scenarios',\n",
       "  'document_url': 'https://developer.nvidia.com/blog/improving-cuda-initialization-times-using-cgroups-in-certain-scenarios/',\n",
       "  'document_date': '2024-01-05T22:14:41',\n",
       "  'document_date_modified': '2024-01-11T19:49:33'},\n",
       " {'text': 'To allow or deny the user access to any other GPUs on the system, write those GPU numbers to the appropriate file. Here’s an example of denying access to only GPU5 and GPU6 on a multi-GPU system. In the ```/gpugroup``` folder created earlier, write the PID of the shell from where the CUDA process is to be launched into the ```tasks``` file:',\n",
       "  'text_components': ['To allow or deny the user access to any other GPUs on the system, write those GPU numbers to the appropriate file.',\n",
       "   'Here’s an example of denying access to only GPU5 and GPU6 on a multi-GPU system.',\n",
       "   'In the ```/gpugroup``` folder created earlier, write the PID of the shell from where the CUDA process is to be launched into the ```tasks``` file:'],\n",
       "  'paragraph_index': [11, 11, 12],\n",
       "  'word_count': [22, 17, 37],\n",
       "  'paragraph_sentence_index': [0, 1, 0],\n",
       "  'only_code': [False, False, False],\n",
       "  'contains_code': [False, False, True],\n",
       "  'document_title': 'Improving CUDA Initialization Times Using cgroups in Certain Scenarios',\n",
       "  'document_url': 'https://developer.nvidia.com/blog/improving-cuda-initialization-times-using-cgroups-in-certain-scenarios/',\n",
       "  'document_date': '2024-01-05T22:14:41',\n",
       "  'document_date_modified': '2024-01-11T19:49:33'},\n",
       " {'text': '```\\n$> echo <PID> tasks\\n```',\n",
       "  'text_components': ['```', '$> echo <PID> tasks', '```'],\n",
       "  'paragraph_index': [13, 13, 13],\n",
       "  'word_count': [0, 3, 0],\n",
       "  'paragraph_sentence_index': [0, 1, 2],\n",
       "  'only_code': [True, True, True],\n",
       "  'contains_code': [True, True, True],\n",
       "  'document_title': 'Improving CUDA Initialization Times Using cgroups in Certain Scenarios',\n",
       "  'document_url': 'https://developer.nvidia.com/blog/improving-cuda-initialization-times-using-cgroups-in-certain-scenarios/',\n",
       "  'document_date': '2024-01-05T22:14:41',\n",
       "  'document_date_modified': '2024-01-11T19:49:33'},\n",
       " {'text': 'Now add GPU5 and GPU6 to the denied list:',\n",
       "  'text_components': ['Now add GPU5 and GPU6 to the denied list:'],\n",
       "  'paragraph_index': [14],\n",
       "  'word_count': [9],\n",
       "  'paragraph_sentence_index': [0],\n",
       "  'only_code': [False],\n",
       "  'contains_code': [False],\n",
       "  'document_title': 'Improving CUDA Initialization Times Using cgroups in Certain Scenarios',\n",
       "  'document_url': 'https://developer.nvidia.com/blog/improving-cuda-initialization-times-using-cgroups-in-certain-scenarios/',\n",
       "  'document_date': '2024-01-05T22:14:41',\n",
       "  'document_date_modified': '2024-01-11T19:49:33'},\n",
       " {'text': \"```\\n$> echo 'c 195:5 rmw' > devices.deny\\n$> echo 'c 195:6 rmw' > devices.deny\\n```\",\n",
       "  'text_components': ['```',\n",
       "   \"$> echo 'c 195:5 rmw' > devices.deny\",\n",
       "   \"$> echo 'c 195:6 rmw' > devices.deny\",\n",
       "   '```'],\n",
       "  'paragraph_index': [15, 15, 15, 15],\n",
       "  'word_count': [0, 7, 7, 0],\n",
       "  'paragraph_sentence_index': [0, 1, 2, 3],\n",
       "  'only_code': [True, True, True, True],\n",
       "  'contains_code': [True, True, True, True],\n",
       "  'document_title': 'Improving CUDA Initialization Times Using cgroups in Certain Scenarios',\n",
       "  'document_url': 'https://developer.nvidia.com/blog/improving-cuda-initialization-times-using-cgroups-in-certain-scenarios/',\n",
       "  'document_date': '2024-01-05T22:14:41',\n",
       "  'document_date_modified': '2024-01-11T19:49:33'},\n",
       " {'text': 'At this point, the CUDA process can’t see or access the two GPUs. To enable only specific GPUs to a CUDA process, those GPUs should be added to the ```devices.allow``` file and the rest of the GPUs should be added to the ```devices.deny``` file. The access controls apply per process. Multiple processes can be added to the ```tasks``` file to propagate the same controls to more than one process. Isolating GPUs using the bubblewrap utility The bubblewrap utility (bwrap) is a higher-level utility available for sandboxing and access control in Linux operating systems, which can be used to achieve the same effect as the solution presented earlier. You can use this to conveniently restrict or allow access to specific GPUs from a CUDA process:',\n",
       "  'text_components': ['At this point, the CUDA process can’t see or access the two GPUs.',\n",
       "   'To enable only specific GPUs to a CUDA process, those GPUs should be added to the ```devices.allow``` file and the rest of the GPUs should be added to the ```devices.deny``` file.',\n",
       "   'The access controls apply per process.',\n",
       "   'Multiple processes can be added to the ```tasks``` file to propagate the same controls to more than one process.',\n",
       "   'Isolating GPUs using the bubblewrap utility',\n",
       "   'The bubblewrap utility (bwrap) is a higher-level utility available for sandboxing and access control in Linux operating systems, which can be used to achieve the same effect as the solution presented earlier.',\n",
       "   'You can use this to conveniently restrict or allow access to specific GPUs from a CUDA process:'],\n",
       "  'paragraph_index': [16, 16, 17, 17, 18, 19, 19],\n",
       "  'word_count': [14, 43, 6, 25, 6, 33, 17],\n",
       "  'paragraph_sentence_index': [0, 1, 0, 1, 0, 0, 1],\n",
       "  'only_code': [False, False, False, False, False, False, False],\n",
       "  'contains_code': [True, True, True, True, False, False, False],\n",
       "  'document_title': 'Improving CUDA Initialization Times Using cgroups in Certain Scenarios',\n",
       "  'document_url': 'https://developer.nvidia.com/blog/improving-cuda-initialization-times-using-cgroups-in-certain-scenarios/',\n",
       "  'document_date': '2024-01-05T22:14:41',\n",
       "  'document_date_modified': '2024-01-11T19:49:33'},\n",
       " {'text': '```\\n# install bubblewrap utility on Debian-like systems\\n$>sudo apt-get install -y bubblewrap\\n# create a simple shell script that uses bubblewap for binding the required GPU to the launched process\\n#!/bin/sh\\n# bwrap.sh\\nGPU=$1;shift   # 0, 1, 2, 3, ..\\nif [ \"$GPU\" = \"\" ]; then echo \"missing arg: gpu id\"; exit 1; fi\\nbwrap \\\\\\n        --bind / / \\\\\\n        --dev /dev --dev-bind /dev/nvidiactl /dev/nvidiactl --dev-bind /dev/nvidia-uvm /dev/nvidia-uvm  \\\\\\n        --dev-bind /dev/nvidia$GPU /dev/nvidia$GPU \\\\\\n        \"$@\"\\n# Launch the CUDA process with the bubblewrap utility to only allow access to a specific GPU while running\\n$> ./bwrap.sh 0 ./test_cuda_app <args>\\n```',\n",
       "  'text_components': ['```',\n",
       "   '# install bubblewrap utility on Debian-like systems',\n",
       "   '$>sudo apt-get install -y bubblewrap',\n",
       "   '# create a simple shell script that uses bubblewap for binding the required GPU to the launched process',\n",
       "   '#!/bin/sh',\n",
       "   '# bwrap.sh',\n",
       "   'GPU=$1;shift   # 0, 1, 2, 3, ..',\n",
       "   'if [ \"$GPU\" = \"\" ]; then echo \"missing arg: gpu id\"; exit 1; fi',\n",
       "   'bwrap \\\\',\n",
       "   '        --bind / / \\\\',\n",
       "   '        --dev /dev --dev-bind /dev/nvidiactl /dev/nvidiactl --dev-bind /dev/nvidia-uvm /dev/nvidia-uvm  \\\\',\n",
       "   '        --dev-bind /dev/nvidia$GPU /dev/nvidia$GPU \\\\',\n",
       "   '        \"$@\"',\n",
       "   '# Launch the CUDA process with the bubblewrap utility to only allow access to a specific GPU while running',\n",
       "   '$> ./bwrap.sh 0 ./test_cuda_app <args>',\n",
       "   '```'],\n",
       "  'paragraph_index': [20,\n",
       "   20,\n",
       "   20,\n",
       "   20,\n",
       "   20,\n",
       "   20,\n",
       "   20,\n",
       "   20,\n",
       "   20,\n",
       "   20,\n",
       "   20,\n",
       "   20,\n",
       "   20,\n",
       "   20,\n",
       "   20,\n",
       "   20],\n",
       "  'word_count': [0, 7, 6, 17, 2, 2, 7, 11, 1, 1, 16, 8, 0, 18, 5, 0],\n",
       "  'paragraph_sentence_index': [0,\n",
       "   1,\n",
       "   2,\n",
       "   3,\n",
       "   4,\n",
       "   5,\n",
       "   6,\n",
       "   7,\n",
       "   8,\n",
       "   9,\n",
       "   10,\n",
       "   11,\n",
       "   12,\n",
       "   13,\n",
       "   14,\n",
       "   15],\n",
       "  'only_code': [True,\n",
       "   True,\n",
       "   True,\n",
       "   True,\n",
       "   True,\n",
       "   True,\n",
       "   True,\n",
       "   True,\n",
       "   True,\n",
       "   True,\n",
       "   True,\n",
       "   True,\n",
       "   True,\n",
       "   True,\n",
       "   True,\n",
       "   True],\n",
       "  'contains_code': [True,\n",
       "   True,\n",
       "   True,\n",
       "   True,\n",
       "   True,\n",
       "   True,\n",
       "   True,\n",
       "   True,\n",
       "   True,\n",
       "   True,\n",
       "   True,\n",
       "   True,\n",
       "   True,\n",
       "   True,\n",
       "   True,\n",
       "   True],\n",
       "  'document_title': 'Improving CUDA Initialization Times Using cgroups in Certain Scenarios',\n",
       "  'document_url': 'https://developer.nvidia.com/blog/improving-cuda-initialization-times-using-cgroups-in-certain-scenarios/',\n",
       "  'document_date': '2024-01-05T22:14:41',\n",
       "  'document_date_modified': '2024-01-11T19:49:33'},\n",
       " {'text': 'More than one GPU can be exposed to a CUDA process by extending the ```dev-bind``` option in the code example. Performance benefits of GPU isolation In this section, we compare the performance of the CUDA driver initialization API (cuInit) with and without GPU isolation, measured over 256 iterations. The APIs are being run on an x86-based machine with four A100 class GPUs. Bar graph shows the performance of cuInit API running on an A104 system with and without GPU isolation using cgroups. The bar on the left shows the performance of cuInit when only a single GPU is exposed to the calling CUDA process via cgroups (~65 ms). The bar on the right shows the performance of cuInit when all four GPUs on the system are made available to the CUDA process (225 ms). Figure 1. CUDA initialisation performance comparison between a cgroup-constrained process and the default scenario on a four-GPU test system Summary GPU isolation using ```cgroups``` offers you the option of improving CUDA initialization times in a limited number of use cases where all the GPUs on the system are not required to be used by a given CUDA process. For more information, see the following resources: Control Groups version 1 — The Linux Kernel documentation cuInit',\n",
       "  'text_components': ['More than one GPU can be exposed to a CUDA process by extending the ```dev-bind``` option in the code example.',\n",
       "   'Performance benefits of GPU isolation',\n",
       "   'In this section, we compare the performance of the CUDA driver initialization API (cuInit) with and without GPU isolation, measured over 256 iterations.',\n",
       "   'The APIs are being run on an x86-based machine with four A100 class GPUs.',\n",
       "   'Bar graph shows the performance of cuInit API running on an A104 system with and without GPU isolation using cgroups.',\n",
       "   'The bar on the left shows the performance of cuInit when only a single GPU is exposed to the calling CUDA process via cgroups (~65 ms).',\n",
       "   'The bar on the right shows the performance of cuInit when all four GPUs on the system are made available to the CUDA process (225 ms).',\n",
       "   'Figure 1.',\n",
       "   'CUDA initialisation performance comparison between a cgroup-constrained process and the default scenario on a four-GPU test system',\n",
       "   'Summary',\n",
       "   'GPU isolation using ```cgroups``` offers you the option of improving CUDA initialization times in a limited number of use cases where all the GPUs on the system are not required to be used by a given CUDA process.',\n",
       "   'For more information, see the following resources:',\n",
       "   'Control Groups version 1 — The Linux Kernel documentation',\n",
       "   'cuInit'],\n",
       "  'paragraph_index': [21, 22, 23, 23, 24, 24, 24, 25, 25, 26, 27, 28, 29, 30],\n",
       "  'word_count': [27, 5, 23, 15, 20, 26, 26, 2, 19, 1, 44, 7, 8, 1],\n",
       "  'paragraph_sentence_index': [0, 0, 0, 1, 0, 1, 2, 0, 1, 0, 0, 0, 0, 0],\n",
       "  'only_code': [False,\n",
       "   False,\n",
       "   False,\n",
       "   False,\n",
       "   False,\n",
       "   False,\n",
       "   False,\n",
       "   False,\n",
       "   False,\n",
       "   False,\n",
       "   False,\n",
       "   False,\n",
       "   False,\n",
       "   False],\n",
       "  'contains_code': [True,\n",
       "   False,\n",
       "   False,\n",
       "   False,\n",
       "   False,\n",
       "   False,\n",
       "   False,\n",
       "   False,\n",
       "   False,\n",
       "   False,\n",
       "   True,\n",
       "   False,\n",
       "   False,\n",
       "   False],\n",
       "  'document_title': 'Improving CUDA Initialization Times Using cgroups in Certain Scenarios',\n",
       "  'document_url': 'https://developer.nvidia.com/blog/improving-cuda-initialization-times-using-cgroups-in-certain-scenarios/',\n",
       "  'document_date': '2024-01-05T22:14:41',\n",
       "  'document_date_modified': '2024-01-11T19:49:33'}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks = chunk_request(\n",
    "    client,\n",
    "    {\n",
    "        \"strategy\": \"sentence\",\n",
    "        \"chunk_min_words\": 250,\n",
    "        \"chunk_overlap_words\": 50,\n",
    "        \"input_type\": \"html\",\n",
    "        \"input_str\": document_html,\n",
    "        \"additional_metadata\": {\n",
    "            \"document_title\": document_title,\n",
    "            \"document_url\": document_url,\n",
    "            \"document_date\": document_date,\n",
    "            \"document_date_modified\": document_date_modified,\n",
    "        },\n",
    "    },\n",
    ")\n",
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "print(len(chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk #0\n",
      "Word Count: 258\n",
      "==========\n",
      "Chunk #1\n",
      "Word Count: 96\n",
      "==========\n",
      "Chunk #2\n",
      "Word Count: 228\n",
      "==========\n",
      "Chunk #3\n",
      "Word Count: 21\n",
      "==========\n",
      "Chunk #4\n",
      "Word Count: 3\n",
      "==========\n",
      "Chunk #5\n",
      "Word Count: 76\n",
      "==========\n",
      "Chunk #6\n",
      "Word Count: 3\n",
      "==========\n",
      "Chunk #7\n",
      "Word Count: 9\n",
      "==========\n",
      "Chunk #8\n",
      "Word Count: 14\n",
      "==========\n",
      "Chunk #9\n",
      "Word Count: 144\n",
      "==========\n",
      "Chunk #10\n",
      "Word Count: 101\n",
      "==========\n",
      "Chunk #11\n",
      "Word Count: 224\n",
      "==========\n"
     ]
    }
   ],
   "source": [
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"Chunk #{i}\")\n",
    "    print(f\"Word Count: {sum(chunk['word_count'])}\")\n",
    "    # print(chunk[\"text\"])\n",
    "    print(\"==========\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we inspect the word count in each chunk, we see that we don't always hit the minimum number of words we set as a parameter (250).\n",
    "\n",
    "Check out Chunk #3 for example, which only has 21 words. Too many small chunks like this will \"clog\" our retrieval system with bad data. This forces us to retrieve a larger \n",
    "batch of results (top K has to be set to a larger number) and then an LLM (or person, if they're looking at the LLM's sources) would need to sift through too many bad matches. \n",
    "\n",
    "Even if a small chunk is not helpful, it can still come up in a search if the document title or other metadata cause a close semantic/keyword search match. Small chunks are especially prone to this problem because they don't have enough content of their own to \"dilute\" the effect of metadata matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'When you are done with the tasks, unmount the ```/cgroupV1Device``` folder with the umount command.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[3][\"text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with Code in Your Documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This chunk is this small because the default behavior of the chunking microservice is to enforce boundaries between code and non-code sections in HTML. It will not combine a section of the article written in natural language with another section of only code. This little sentence comes right before a large code section in the article, so the chunking service ends the chunk right there.\n",
    "\n",
    "Our use case therefore requires us to handle the presence of large code sections within our HTML. \n",
    "\n",
    "Lots of text documents at NVIDIA contain a mix of code and natural language, whether it's blog posts like these, SDK documentation, Git repository README markdown files, etc.\n",
    "\n",
    "These sections of code are very different syntactically and grammatically from regular natural language text, and so an embedding model that has not been trained on code may not perform well with code present. For embedding models that are trained on code and natural language, it's also going to be important to delimit the code with the characters the embedding model was trained on. The chunking service uses triple backticks (```) to indicate a section of code.\n",
    "\n",
    "The chunking service as written supports three strategies to deal with code. \n",
    "1. The default (`\"code_behavior\": \"enforce_code_boundaries\"`) is to enforce hard boundaries between code and non-code. This has the benefit of separation, but has the drawback that sometimes you will end up with awkward small chunks because of these boundaries.\n",
    "2. The second option (`\"code_behavior\": \"ignore_code_boundaries\"`) is to just ignore the boundaries and lump code and non-code together, while still keeping the backticks as delimiters. This is a good option if your embedding model supports both code and non-code.\n",
    "3. The third option (`\"code_behavior\": \"remove_code_sections\"`) is to remove the long only-code sections from the actual text that will be embedded, but store the code as metadata which can later be used. For example, the code can be supplied to an LLM that is generating a response based on the retrieval results it found by matching on the accompanying natural language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Only Code Sections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try option 3, since for this lesson we will be using the `SentenceTransformers` embedding model [`e5-large-unsupervised`](https://huggingface.co/intfloat/e5-large-unsupervised), which was not trained on code. Additionally, this model has a maximum token limit of 512 tokens, or roughly ~380 typical words. To be on the safe side we'll use a minimum of 250 words.\n",
    "\n",
    "*Note: `e5-large-unsupervised` should only be used for English language text. There is a multilingual version of the e5 model on HuggingFace if you're interested.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "chunks = chunk_request(\n",
    "    client,\n",
    "    {\n",
    "        \"strategy\": \"sentence\",\n",
    "        \"code_behavior\": \"remove_code_sections\",\n",
    "        \"chunk_min_words\": 250,\n",
    "        \"chunk_overlap_words\": 50,\n",
    "        \"input_type\": \"html\",\n",
    "        \"input_str\": document_html,\n",
    "        \"additional_metadata\": {\n",
    "            \"document_title\": document_title,\n",
    "            \"document_url\": document_url,\n",
    "            \"document_date\": document_date,\n",
    "            \"document_date_modified\": document_date_modified,\n",
    "        },\n",
    "    },\n",
    ")\n",
    "\n",
    "print(len(chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk #0\n",
      "Word Count: 258\n",
      "Many CUDA applications running on multi-GPU platforms usually use a single GPU for their compute needs. In such scenarios, a performance penalty is paid by applications because CUDA has to enumerate/initialize all the GPUs on the system. If a CUDA application does not require other GPUs to be visible and accessible, you can launch such applications by isolating the unwanted GPUs from the CUDA process and eliminating unnecessary initialization steps. This post discusses the various methods to accomplish this and their performance benefits. GPU isolation GPU isolation can be achieved on Linux systems by using Linux tools like ```cgroups```. In this section, we first discuss a lower-level approach and then a higher-level possible approach. Another method exposed by CUDA to isolate devices is the use of ```CUDA_VISIBLE_DEVICES```. Although functionally similar, this approach has limited initialization performance gains compared to the ```cgroups``` approach. Isolating GPUs using cgroups V1 Control groups provide a mechanism for aggregating or partitioning sets of tasks and all their future children into hierarchical groups with specialized behavior. You can use ```cgroups``` to control which GPUs are visible to a CUDA process. This ensures that only the GPUs that are needed by the CUDA process are made available to it. The following code provides a low-level example of how to employ ```cgroups``` and fully isolate a GPU to a single process.\n",
      "==========\n",
      "Chunk #1\n",
      "Word Count: 329\n",
      "You can use ```cgroups``` to control which GPUs are visible to a CUDA process. This ensures that only the GPUs that are needed by the CUDA process are made available to it. The following code provides a low-level example of how to employ ```cgroups``` and fully isolate a GPU to a single process. Be aware that you will likely have to run these commands in a root shell to work properly. We show a more convenient, higher-level utility later in this post. When you are done with the tasks, unmount the ```/cgroupV1Device``` folder with the umount command. To allow or deny the user access to any other GPUs on the system, write those GPU numbers to the appropriate file. Here’s an example of denying access to only GPU5 and GPU6 on a multi-GPU system. In the ```/gpugroup``` folder created earlier, write the PID of the shell from where the CUDA process is to be launched into the ```tasks``` file: Now add GPU5 and GPU6 to the denied list: At this point, the CUDA process can’t see or access the two GPUs. To enable only specific GPUs to a CUDA process, those GPUs should be added to the ```devices.allow``` file and the rest of the GPUs should be added to the ```devices.deny``` file. The access controls apply per process. Multiple processes can be added to the ```tasks``` file to propagate the same controls to more than one process. Isolating GPUs using the bubblewrap utility The bubblewrap utility (bwrap) is a higher-level utility available for sandboxing and access control in Linux operating systems, which can be used to achieve the same effect as the solution presented earlier.\n",
      "==========\n",
      "Chunk #2\n",
      "Word Count: 305\n",
      "Multiple processes can be added to the ```tasks``` file to propagate the same controls to more than one process. Isolating GPUs using the bubblewrap utility The bubblewrap utility (bwrap) is a higher-level utility available for sandboxing and access control in Linux operating systems, which can be used to achieve the same effect as the solution presented earlier. You can use this to conveniently restrict or allow access to specific GPUs from a CUDA process: More than one GPU can be exposed to a CUDA process by extending the ```dev-bind``` option in the code example. Performance benefits of GPU isolation In this section, we compare the performance of the CUDA driver initialization API (cuInit) with and without GPU isolation, measured over 256 iterations. The APIs are being run on an x86-based machine with four A100 class GPUs. Bar graph shows the performance of cuInit API running on an A104 system with and without GPU isolation using cgroups. The bar on the left shows the performance of cuInit when only a single GPU is exposed to the calling CUDA process via cgroups (~65 ms). The bar on the right shows the performance of cuInit when all four GPUs on the system are made available to the CUDA process (225 ms). Figure 1. CUDA initialisation performance comparison between a cgroup-constrained process and the default scenario on a four-GPU test system Summary GPU isolation using ```cgroups``` offers you the option of improving CUDA initialization times in a limited number of use cases where all the GPUs on the system are not required to be used by a given CUDA process. For more information, see the following resources: Control Groups version 1 — The Linux Kernel documentation cuInit\n",
      "==========\n"
     ]
    }
   ],
   "source": [
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"Chunk #{i}\")\n",
    "    # note that word_count is a list which contains word components of all text components, including ones that are\n",
    "    # only code. These get removed from the final text though\n",
    "    print(f\"Word Count: {sum(wc for wc, only_code in zip(chunk['word_count'], chunk['only_code']) if not only_code)}\")\n",
    "    print(chunk[\"text\"])\n",
    "    print(\"==========\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is looking pretty good! Let's try our second example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk #0\n",
      "Word Count: 263\n",
      "NVIDIA Metropolis Microservices for Jetson provides a suite of easy-to-deploy services that enable you to quickly build production-quality vision AI applications while using the latest AI approaches. This post explains how to develop and deploy generative AI –powered applications with Metropolis Microservices on the NVIDIA Jetson edge AI platform by walking through a reference example that can be used as a general recipe for nearly any model. The reference example uses a stand-alone zero-shot detection NanoOwl application and integrates it with Metropolis Microservices for Jetson, so that you can quickly prototype and deploy it in production. Transform your applications with generative AI Generative AI is a new class of machine learning that enables models to understand the world in a more open way than previous methods. At the heart of most generative AI is a transformer-based model that has been trained on internet-scale data. These models have a much broader understanding across domains, enabling them to be used as a backbone for a variety of tasks. This flexibility enables models like CLIP, Owl, Llama, GPT, and Stable Diffusion to comprehend natural language inputs. They are capable of zero or few-shot learning. GIF shows the NanoOwl model detecting a person, face, hands, and shoes on request. Figure 1. Zero-shot detection using NanoOwl For more information about generative AI models for Jetson, see the NVIDIA Jetson Generative AI Lab and Bringing Generative AI to Life with NVIDIA Jetson. Metropolis Microservices for Jetson Metropolis Microservices can be used to rapidly build production-ready AI applications on Jetson.\n",
      "==========\n",
      "Chunk #1\n",
      "Word Count: 304\n",
      "Figure 1. Zero-shot detection using NanoOwl For more information about generative AI models for Jetson, see the NVIDIA Jetson Generative AI Lab and Bringing Generative AI to Life with NVIDIA Jetson. Metropolis Microservices for Jetson Metropolis Microservices can be used to rapidly build production-ready AI applications on Jetson. Metropolis Microservices are a set of modular and easily deployable Docker containers for camera management, system monitoring, IoT device integration, networking, storage, and more. These can be brought together to create powerful applications. Figure 2 shows the available microservices. The diagram shows reference AI workflows, app microservices, an AI stack, and platform software. Figure 2. Metropolis Microservices for Jetson stack For more information, see the Metropolis Microservices for Jetson whitepaper. Integrating generative AI apps with Metropolis Microservices Metropolis Microservices and generative AI can be combined to take advantage of models that require little to no training. Figure 3 shows a diagram of the NanoOwl reference example that can be used as a general recipe to build generative AI–powered applications with Metropolis Microservices on Jetson. The system diagram shows the user interfaces, live camera streams, and microservices on Jetson, including the generative AI application, Video Storage Toolkit, ingress, Redis, and monitoring. Figure 3. Generative AI reference application using Metropolis Microservices for Jetson Application customization with Metropolis Microservices There are many open-source generative AI models available on GitHub and some have been optimized to run specifically on Jetson. You can find several of these models in the Jetson Generative AI Lab. Most of these models have a lot in common. As inputs, they can typically accept text and an image. These models must first be loaded into memory with any configuration options. Then, the model can be called with an inference function where the image and text are passed in to produce an output.\n",
      "==========\n",
      "Chunk #2\n",
      "Word Count: 335\n",
      "Most of these models have a lot in common. As inputs, they can typically accept text and an image. These models must first be loaded into memory with any configuration options. Then, the model can be called with an inference function where the image and text are passed in to produce an output. In the Python reference example, we used NanoOwl as the generative AI model. However, the general recipe of the reference example can be applied to nearly any generative AI model. The diagram shows how a generative AI application can take in an RTSP stream and output detection information to RTSP and Redis. Figure 4. Overview of a generative AI application using Metropolis Microservices To run any generative AI model with Metropolis Microservices, you must first align the input and output from other microservices (Figure 4). For streaming video, the input and output uses the RTSP protocol. RTSP is streamed from Video Storage Toolkit (VST), a video ingestion and management microservice. The output is streamed over RTSP with the overlaid inference output. The output metadata is sent to a Redis stream where other applications can read the data. For more information, see the Video Storage Toolkit with Metropolis Microservices demo videos. Second, as a generative AI application requires some external interface such as prompts, you need the application to take REST API requests. Lastly, the application must be containerized to integrate seamlessly with other microservices. Figure 5 shows an example of NanoOwl object detection and metadata output on Redis. GIF shows the generative AI application detecting various objects like boxes, pallets, and people. Figure 5. Generative AI application running Prepare the generative AI application This reference example uses NanoOwl. However, you can follow these steps for any model that has a load and inference function callable from Python. There are several Python code examples shown in this post to highlight the main ideas of how to combine generative AI with Metropolis Microservices but some code is omitted to focus on the general recipe.\n",
      "==========\n",
      "Chunk #3\n",
      "Word Count: 312\n",
      "However, you can follow these steps for any model that has a load and inference function callable from Python. There are several Python code examples shown in this post to highlight the main ideas of how to combine generative AI with Metropolis Microservices but some code is omitted to focus on the general recipe. For more information about the full implementation, see the reference example on the /NVIDIA-AI-IOT/mmj_genai GitHub project. To prepare a generative AI model for integration with Metropolis Microservices, take the following steps: Call the ```predict``` function for model inference Add RTSP I/O using the ```jetson-utils``` library. Add a REST endpoint for prompt updates with Flask. Use ```mmj_utils``` to generate overlays. Use ```mmj_utils``` to interact with VST to get streams. Use ```mmj_utils``` to output metadata to Redis. Call the predict function for model inference NanoOwl wraps the generative AI model in an ```OwlPredictor``` class. When this class is instantiated, it loads the model into memory. To make an inference on an image and text input, call the ```predict``` function to get the output. In this case, the output is a list of bounding boxes and labels for the detected objects. Most generative AI models have similar Python interfaces. There is image and text input, the model must be loaded, and then the model can infer from the prompt and image to get some output. To bring in your own generative AI model, you can wrap it in a class and implement an interface similar to the ```OwlPredictor``` class. Add RTSP I/O using the jetson-utils library\n",
      "==========\n",
      "Chunk #4\n",
      "Word Count: 321\n",
      "There is image and text input, the model must be loaded, and then the model can infer from the prompt and image to get some output. To bring in your own generative AI model, you can wrap it in a class and implement an interface similar to the ```OwlPredictor``` class. Add RTSP I/O using the jetson-utils library You can add RTSP video stream input using the jetson-utils library. This library provides ```videoSource``` and ```videoOutput``` classes that can be used to capture frames from an RTSP stream and output frames on a new RTSP stream. This code example captures frames from an RTSP stream, which can then be passed to a model inference function. A new image is created from the model outputs and rendered to an output RTSP stream. Add a REST endpoint for prompt updates with Flask Many generative AI models accept some kind of prompt or text input. To enable a user or another service to update the prompt dynamically, add a REST endpoint using Flask that accepts prompt updates and passes them to the model. To make the Flask server integrate more easily with your model, create a wrapper class that can be called to launch a Flask server in its own thread. For more information, see the /NVIDIA-AI-IOT/mmj_genai GitHub project. Connect your main script and the Flask endpoint through a queue that holds any incoming prompt updates. When a GET request is sent to the REST endpoint, the Flask server places the updated prompt in a queue. Your main loop can then check the queue for new prompts and pass it to the model for inference on the updated classes. Use mmj_utils to generate overlays For computer vision tasks, it is nice to see a visual overlay of the model output (Figure 6).\n",
      "==========\n",
      "Chunk #5\n",
      "Word Count: 316\n",
      "When a GET request is sent to the REST endpoint, the Flask server places the updated prompt in a queue. Your main loop can then check the queue for new prompts and pass it to the model for inference on the updated classes. Use mmj_utils to generate overlays For computer vision tasks, it is nice to see a visual overlay of the model output (Figure 6). In the case of object detection models, you can overlay the bounding boxes and labels generated by the model on the input image to view where the model detected each object. Image shows a factory floor with people, pallets, and equipment delineated by labeled bounding boxes. Figure 6. Detection overlay generated with mmj_utils To do this, use the utility class called ```DetectionGenerationCUDA``` from the mmj_utils library. This library depends on ```jetson_utils```, which provides CUDA-accelerated functions used to generate the overlay. You can instantiate the ```DetectionGenerationCUDA``` object with several keyword arguments to adjust the text size, bounding box size, and colors to suit your needs. For more information about overlay generation with ```mmj_utils```, see /NVIDIA-AI-IOT/mmj_utils GitHub repo. To generate the overlay, call the object and pass the input image, list of labels, and bounding boxes generated by the model. It then draws the labels and bounding boxes on the input image and returns the modified image with the overlay. This modified image can then be rendered out on the RTSP stream. Use mmj_utils to interact with VST to get streams VST can help manage your RTSP streams and provide a nice web UI to view the input and output streams. To integrate with VST, use the VST REST API directly or the VST class from mmj_utils, which wraps around the VST REST API.\n",
      "==========\n",
      "Chunk #6\n",
      "Word Count: 318\n",
      "Use mmj_utils to interact with VST to get streams VST can help manage your RTSP streams and provide a nice web UI to view the input and output streams. To integrate with VST, use the VST REST API directly or the VST class from mmj_utils, which wraps around the VST REST API. Instead of hardcoding the RTSP input stream in the Python script, grab an RTSP stream link from VST. This link could come from IP cameras or other video stream sources managed through VST. This connects to VST and grabs the first valid RTSP link. More complex logic could be added here to connect to a specific source or change the inputs dynamically. Use mmj_utils to output metadata to Redis Generative AI models generate‌ metadata that can be used downstream by other services for generating analytics and insights. In this case, NanoOwl outputs bounding boxes on detected objects. You can output this information in Metropolis Schema on a Redis stream, which can be captured by an analytic service. In the ```mmj_utils``` library, there is a class to help produce detection metadata on Redis. You can instantiate a ```SchemaGenerator``` object with information about the input camera stream and connect to Redis. The object can then be called by passing in text labels and bounding boxes produced by the model. The detection information gets converted to Metropolis Schema and output to Redis to be used by other microservices. The diagram shows components such as VST Stream Discovery, NanoOwl Object Detection, a Flask REST Endpoint, Overlay Generation, Metadata Generation, and the incoming RTSP stream. Figure 7. Generative AI application Application deployment To deploy the application, you can set up platform services like Ingress and Redis. Then, combine your custom generative AI container with the application services such as VST through ```docker compose```.\n",
      "==========\n",
      "Chunk #7\n",
      "Word Count: 321\n",
      "The diagram shows components such as VST Stream Discovery, NanoOwl Object Detection, a Flask REST Endpoint, Overlay Generation, Metadata Generation, and the incoming RTSP stream. Figure 7. Generative AI application Application deployment To deploy the application, you can set up platform services like Ingress and Redis. Then, combine your custom generative AI container with the application services such as VST through ```docker compose```. With the main application prepared with all the necessary I/O and microservice integration (Figure 7), you can deploy the application and connect with Metropolis Microservices. Containerize the generative AI application. Set up the necessary platform services. Launch the application with ```docker compose```. View outputs in real time. Containerize the generative AI application The first step for deployment is to containerize the generative AI application using Docker. An easy way to do this is to use the jetson-containers project. This project provides an easy way to build Docker containers for Jetson to support machine learning applications including generative AI models. Use jetson-containers to make a container with the necessary dependencies and then customize the container further to include the application code and any other packages needed to run your generative AI model. For more information about how to build the container for the NanoOwl example, see the /src/readme file in the GitHub project. Set up the necessary platform services Next, set up the necessary platform services provided by Metropolis Microservices. These platform services provide many features needed to deploy an application with Metropolis Microservices. This reference generative AI application only requires the Ingress, Redis, and Monitoring platform services. Platform services can be quickly installed through APT and launched with ```systemctl```. For more information about how to install and launch the necessary platform services, see the Metropolis Microservices for Jetson Quickstart Guide. Launch the application with docker compose\n",
      "==========\n",
      "Chunk #8\n",
      "Word Count: 326\n",
      "This reference generative AI application only requires the Ingress, Redis, and Monitoring platform services. Platform services can be quickly installed through APT and launched with ```systemctl```. For more information about how to install and launch the necessary platform services, see the Metropolis Microservices for Jetson Quickstart Guide. Launch the application with docker compose With the application containerized and the necessary platform services set up, you can launch your application along with other application services like VST or Analytics using ```docker compose```. To do this, create a ```docker-compose.yaml``` file that defines the containers to be launched along with any necessary launch options. After you define the ```docker compose``` file, you can start or stop your application using the ```docker compose up``` and ```docker compose down``` commands. For more information about docker deployment, see the /deploy/readme file in the GitHub project. View outputs in real time After the application is deployed, you can add an RTSP stream through VST and interact with the generative AI model through the REST API to send prompt updates and view the detections change in real time by watching the RTSP output. You can also see the metadata output on Redis. Video 1. Transform Edge AI Applications with Generative AI Using Metropolis Microservices for Jetson Conclusion This post explained how to take a generative AI model and integrate it with Metropolis Microservices for Jetson. With generative AI and Metropolis Microservices, you can rapidly build intelligent video analytic applications that are both flexible and accurate. For more information about the provided services, see the Metropolis Microservices for Jetson product page. To view the full reference application and more detailed steps about how to build and deploy it for yourself, see the /NVIDIA-AI-IOT/mmj_genai GitHub project.\n",
      "==========\n"
     ]
    }
   ],
   "source": [
    "item = example2\n",
    "\n",
    "document_title = item[\"title\"][\"rendered\"]\n",
    "document_url = item[\"link\"]\n",
    "document_html = item[\"content\"][\"rendered\"]\n",
    "document_date = item[\"date_gmt\"]\n",
    "document_date_modified = item[\"modified_gmt\"]\n",
    "\n",
    "\n",
    "chunks = chunk_request(\n",
    "    client,\n",
    "    {\n",
    "        \"strategy\": \"sentence\",\n",
    "        \"code_behavior\": \"remove_code_sections\",\n",
    "        \"chunk_min_words\": 250,\n",
    "        \"chunk_overlap_words\": 50,\n",
    "        \"input_type\": \"html\",\n",
    "        \"input_str\": document_html,\n",
    "        \"additional_metadata\": {\n",
    "            \"document_title\": document_title,\n",
    "            \"document_url\": document_url,\n",
    "            \"document_date\": document_date,\n",
    "            \"document_date_modified\": document_date_modified,\n",
    "        },\n",
    "    },\n",
    ")\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"Chunk #{i}\")\n",
    "    # note that word_count is a list which contains word components of all text components, including ones that are\n",
    "    # only code. These get removed from the final text though\n",
    "    print(f\"Word Count: {sum(wc for wc, only_code in zip(chunk['word_count'], chunk['only_code']) if not only_code)}\")\n",
    "    print(chunk[\"text\"])\n",
    "    print(\"==========\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Heading Sections"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many further tweaks we could consider, beyond just changing the minimum words per chunk or number of overlap words. \n",
    "\n",
    "For example, we could choose to chunk by adding paragraphs instead of sentences until we hit the minimum number of words. Perhaps it doesn't make sense to break apart paragraphs, since the author saw them as one logical unit. \n",
    "\n",
    "Let's additionally consider the implicit structure provided by the headings. We could chunk only off the headings, though this would likely result in a similar issue of awkward small chunks for heading sections that are irregular sizes (especially if we taking code sections out of the embeddable text). \n",
    "\n",
    "The chunking microservice lets us preserve which heading section a sentence/paragraph came from and insert that into the chunk text, helping the embedding model understand longer-range context for the chunk. This way, if a chunk starts in the middle of a particular section, it can still be interpreted alongside the title of that section. We just change `strategy` to `heading_section_sentence`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk #0\n",
      "Word Count: 253\n",
      "Bringing Generative AI to the Edge with NVIDIA Metropolis Microservices for Jetson\n",
      "NVIDIA Metropolis Microservices for Jetson provides a suite of easy-to-deploy services that enable you to quickly build production-quality vision AI applications while using the latest AI approaches. This post explains how to develop and deploy generative AI –powered applications with Metropolis Microservices on the NVIDIA Jetson edge AI platform by walking through a reference example that can be used as a general recipe for nearly any model. The reference example uses a stand-alone zero-shot detection NanoOwl application and integrates it with Metropolis Microservices for Jetson, so that you can quickly prototype and deploy it in production. \n",
      "Transform your applications with generative AI\n",
      "Generative AI is a new class of machine learning that enables models to understand the world in a more open way than previous methods. At the heart of most generative AI is a transformer-based model that has been trained on internet-scale data. These models have a much broader understanding across domains, enabling them to be used as a backbone for a variety of tasks. This flexibility enables models like CLIP, Owl, Llama, GPT, and Stable Diffusion to comprehend natural language inputs. They are capable of zero or few-shot learning. GIF shows the NanoOwl model detecting a person, face, hands, and shoes on request. Figure 1. Zero-shot detection using NanoOwl For more information about generative AI models for Jetson, see the NVIDIA Jetson Generative AI Lab and Bringing Generative AI to Life with NVIDIA Jetson. \n",
      "Metropolis Microservices for Jetson\n",
      "Metropolis Microservices can be used to rapidly build production-ready AI applications on Jetson.\n",
      "==========\n",
      "Chunk #1\n",
      "Word Count: 315\n",
      "Transform your applications with generative AI\n",
      "GIF shows the NanoOwl model detecting a person, face, hands, and shoes on request. Figure 1. Zero-shot detection using NanoOwl For more information about generative AI models for Jetson, see the NVIDIA Jetson Generative AI Lab and Bringing Generative AI to Life with NVIDIA Jetson. \n",
      "Metropolis Microservices for Jetson\n",
      "Metropolis Microservices can be used to rapidly build production-ready AI applications on Jetson. Metropolis Microservices are a set of modular and easily deployable Docker containers for camera management, system monitoring, IoT device integration, networking, storage, and more. These can be brought together to create powerful applications. Figure 2 shows the available microservices. The diagram shows reference AI workflows, app microservices, an AI stack, and platform software. Figure 2. Metropolis Microservices for Jetson stack For more information, see the Metropolis Microservices for Jetson whitepaper. \n",
      "Integrating generative AI apps with Metropolis Microservices\n",
      "Metropolis Microservices and generative AI can be combined to take advantage of models that require little to no training. Figure 3 shows a diagram of the NanoOwl reference example that can be used as a general recipe to build generative AI–powered applications with Metropolis Microservices on Jetson. The system diagram shows the user interfaces, live camera streams, and microservices on Jetson, including the generative AI application, Video Storage Toolkit, ingress, Redis, and monitoring. Figure 3. Generative AI reference application using Metropolis Microservices for Jetson \n",
      "Application customization with Metropolis Microservices\n",
      "There are many open-source generative AI models available on GitHub and some have been optimized to run specifically on Jetson. You can find several of these models in the Jetson Generative AI Lab. Most of these models have a lot in common. As inputs, they can typically accept text and an image. These models must first be loaded into memory with any configuration options. Then, the model can be called with an inference function where the image and text are passed in to produce an output. In the Python reference example, we used NanoOwl as the generative AI model.\n",
      "==========\n",
      "Chunk #2\n",
      "Word Count: 321\n",
      "Application customization with Metropolis Microservices\n",
      "As inputs, they can typically accept text and an image. These models must first be loaded into memory with any configuration options. Then, the model can be called with an inference function where the image and text are passed in to produce an output. In the Python reference example, we used NanoOwl as the generative AI model. However, the general recipe of the reference example can be applied to nearly any generative AI model. The diagram shows how a generative AI application can take in an RTSP stream and output detection information to RTSP and Redis. Figure 4. Overview of a generative AI application using Metropolis Microservices To run any generative AI model with Metropolis Microservices, you must first align the input and output from other microservices (Figure 4). For streaming video, the input and output uses the RTSP protocol. RTSP is streamed from Video Storage Toolkit (VST), a video ingestion and management microservice. The output is streamed over RTSP with the overlaid inference output. The output metadata is sent to a Redis stream where other applications can read the data. For more information, see the Video Storage Toolkit with Metropolis Microservices demo videos. Second, as a generative AI application requires some external interface such as prompts, you need the application to take REST API requests. Lastly, the application must be containerized to integrate seamlessly with other microservices. Figure 5 shows an example of NanoOwl object detection and metadata output on Redis. GIF shows the generative AI application detecting various objects like boxes, pallets, and people. Figure 5. Generative AI application running \n",
      "Prepare the generative AI application\n",
      "This reference example uses NanoOwl. However, you can follow these steps for any model that has a load and inference function callable from Python. There are several Python code examples shown in this post to highlight the main ideas of how to combine generative AI with Metropolis Microservices but some code is omitted to focus on the general recipe.\n",
      "==========\n",
      "Chunk #3\n",
      "Word Count: 308\n",
      "Prepare the generative AI application\n",
      "However, you can follow these steps for any model that has a load and inference function callable from Python. There are several Python code examples shown in this post to highlight the main ideas of how to combine generative AI with Metropolis Microservices but some code is omitted to focus on the general recipe. For more information about the full implementation, see the reference example on the /NVIDIA-AI-IOT/mmj_genai GitHub project. To prepare a generative AI model for integration with Metropolis Microservices, take the following steps: Call the ```predict``` function for model inference Add RTSP I/O using the ```jetson-utils``` library. Add a REST endpoint for prompt updates with Flask. Use ```mmj_utils``` to generate overlays. Use ```mmj_utils``` to interact with VST to get streams. Use ```mmj_utils``` to output metadata to Redis. \n",
      "Call the predict function for model inference\n",
      "NanoOwl wraps the generative AI model in an ```OwlPredictor``` class. When this class is instantiated, it loads the model into memory. To make an inference on an image and text input, call the ```predict``` function to get the output. In this case, the output is a list of bounding boxes and labels for the detected objects. Most generative AI models have similar Python interfaces. There is image and text input, the model must be loaded, and then the model can infer from the prompt and image to get some output. To bring in your own generative AI model, you can wrap it in a class and implement an interface similar to the ```OwlPredictor``` class. \n",
      "Add RTSP I/O using the jetson-utils library\n",
      "You can add RTSP video stream input using the jetson-utils library.\n",
      "==========\n",
      "Chunk #4\n",
      "Word Count: 329\n",
      "Call the predict function for model inference\n",
      "There is image and text input, the model must be loaded, and then the model can infer from the prompt and image to get some output. To bring in your own generative AI model, you can wrap it in a class and implement an interface similar to the ```OwlPredictor``` class. \n",
      "Add RTSP I/O using the jetson-utils library\n",
      "You can add RTSP video stream input using the jetson-utils library. This library provides ```videoSource``` and ```videoOutput``` classes that can be used to capture frames from an RTSP stream and output frames on a new RTSP stream. This code example captures frames from an RTSP stream, which can then be passed to a model inference function. A new image is created from the model outputs and rendered to an output RTSP stream. \n",
      "Add a REST endpoint for prompt updates with Flask\n",
      "Many generative AI models accept some kind of prompt or text input. To enable a user or another service to update the prompt dynamically, add a REST endpoint using Flask that accepts prompt updates and passes them to the model. To make the Flask server integrate more easily with your model, create a wrapper class that can be called to launch a Flask server in its own thread. For more information, see the /NVIDIA-AI-IOT/mmj_genai GitHub project. Connect your main script and the Flask endpoint through a queue that holds any incoming prompt updates. When a GET request is sent to the REST endpoint, the Flask server places the updated prompt in a queue. Your main loop can then check the queue for new prompts and pass it to the model for inference on the updated classes. \n",
      "Use mmj_utils to generate overlays\n",
      "For computer vision tasks, it is nice to see a visual overlay of the model output (Figure 6). In the case of object detection models, you can overlay the bounding boxes and labels generated by the model on the input image to view where the model detected each object.\n",
      "==========\n",
      "Chunk #5\n",
      "Word Count: 326\n",
      "Add a REST endpoint for prompt updates with Flask\n",
      "Your main loop can then check the queue for new prompts and pass it to the model for inference on the updated classes. \n",
      "Use mmj_utils to generate overlays\n",
      "For computer vision tasks, it is nice to see a visual overlay of the model output (Figure 6). In the case of object detection models, you can overlay the bounding boxes and labels generated by the model on the input image to view where the model detected each object. Image shows a factory floor with people, pallets, and equipment delineated by labeled bounding boxes. Figure 6. Detection overlay generated with mmj_utils To do this, use the utility class called ```DetectionGenerationCUDA``` from the mmj_utils library. This library depends on ```jetson_utils```, which provides CUDA-accelerated functions used to generate the overlay. You can instantiate the ```DetectionGenerationCUDA``` object with several keyword arguments to adjust the text size, bounding box size, and colors to suit your needs. For more information about overlay generation with ```mmj_utils```, see /NVIDIA-AI-IOT/mmj_utils GitHub repo. To generate the overlay, call the object and pass the input image, list of labels, and bounding boxes generated by the model. It then draws the labels and bounding boxes on the input image and returns the modified image with the overlay. This modified image can then be rendered out on the RTSP stream. \n",
      "Use mmj_utils to interact with VST to get streams\n",
      "VST can help manage your RTSP streams and provide a nice web UI to view the input and output streams. To integrate with VST, use the VST REST API directly or the VST class from mmj_utils, which wraps around the VST REST API. Instead of hardcoding the RTSP input stream in the Python script, grab an RTSP stream link from VST. This link could come from IP cameras or other video stream sources managed through VST. This connects to VST and grabs the first valid RTSP link.\n",
      "==========\n",
      "Chunk #6\n",
      "Word Count: 317\n",
      "Use mmj_utils to interact with VST to get streams\n",
      "To integrate with VST, use the VST REST API directly or the VST class from mmj_utils, which wraps around the VST REST API. Instead of hardcoding the RTSP input stream in the Python script, grab an RTSP stream link from VST. This link could come from IP cameras or other video stream sources managed through VST. This connects to VST and grabs the first valid RTSP link. More complex logic could be added here to connect to a specific source or change the inputs dynamically. \n",
      "Use mmj_utils to output metadata to Redis\n",
      "Generative AI models generate‌ metadata that can be used downstream by other services for generating analytics and insights. In this case, NanoOwl outputs bounding boxes on detected objects. You can output this information in Metropolis Schema on a Redis stream, which can be captured by an analytic service. In the ```mmj_utils``` library, there is a class to help produce detection metadata on Redis. You can instantiate a ```SchemaGenerator``` object with information about the input camera stream and connect to Redis. The object can then be called by passing in text labels and bounding boxes produced by the model. The detection information gets converted to Metropolis Schema and output to Redis to be used by other microservices. The diagram shows components such as VST Stream Discovery, NanoOwl Object Detection, a Flask REST Endpoint, Overlay Generation, Metadata Generation, and the incoming RTSP stream. Figure 7. Generative AI application \n",
      "Application deployment\n",
      "To deploy the application, you can set up platform services like Ingress and Redis. Then, combine your custom generative AI container with the application services such as VST through ```docker compose```. With the main application prepared with all the necessary I/O and microservice integration (Figure 7), you can deploy the application and connect with Metropolis Microservices. Containerize the generative AI application. Set up the necessary platform services.\n",
      "==========\n",
      "Chunk #7\n",
      "Word Count: 318\n",
      "Application deployment\n",
      "Then, combine your custom generative AI container with the application services such as VST through ```docker compose```. With the main application prepared with all the necessary I/O and microservice integration (Figure 7), you can deploy the application and connect with Metropolis Microservices. Containerize the generative AI application. Set up the necessary platform services. Launch the application with ```docker compose```. View outputs in real time. \n",
      "Containerize the generative AI application\n",
      "The first step for deployment is to containerize the generative AI application using Docker. An easy way to do this is to use the jetson-containers project. This project provides an easy way to build Docker containers for Jetson to support machine learning applications including generative AI models. Use jetson-containers to make a container with the necessary dependencies and then customize the container further to include the application code and any other packages needed to run your generative AI model. For more information about how to build the container for the NanoOwl example, see the /src/readme file in the GitHub project. \n",
      "Set up the necessary platform services\n",
      "Next, set up the necessary platform services provided by Metropolis Microservices. These platform services provide many features needed to deploy an application with Metropolis Microservices. This reference generative AI application only requires the Ingress, Redis, and Monitoring platform services. Platform services can be quickly installed through APT and launched with ```systemctl```. For more information about how to install and launch the necessary platform services, see the Metropolis Microservices for Jetson Quickstart Guide. \n",
      "Launch the application with docker compose\n",
      "With the application containerized and the necessary platform services set up, you can launch your application along with other application services like VST or Analytics using ```docker compose```. To do this, create a ```docker-compose.yaml``` file that defines the containers to be launched along with any necessary launch options.\n",
      "==========\n",
      "Chunk #8\n",
      "Word Count: 261\n",
      "Launch the application with docker compose\n",
      "With the application containerized and the necessary platform services set up, you can launch your application along with other application services like VST or Analytics using ```docker compose```. To do this, create a ```docker-compose.yaml``` file that defines the containers to be launched along with any necessary launch options. After you define the ```docker compose``` file, you can start or stop your application using the ```docker compose up``` and ```docker compose down``` commands. For more information about docker deployment, see the /deploy/readme file in the GitHub project. \n",
      "View outputs in real time\n",
      "After the application is deployed, you can add an RTSP stream through VST and interact with the generative AI model through the REST API to send prompt updates and view the detections change in real time by watching the RTSP output. You can also see the metadata output on Redis. Video 1. Transform Edge AI Applications with Generative AI Using Metropolis Microservices for Jetson \n",
      "Conclusion\n",
      "This post explained how to take a generative AI model and integrate it with Metropolis Microservices for Jetson. With generative AI and Metropolis Microservices, you can rapidly build intelligent video analytic applications that are both flexible and accurate. For more information about the provided services, see the Metropolis Microservices for Jetson product page. To view the full reference application and more detailed steps about how to build and deploy it for yourself, see the /NVIDIA-AI-IOT/mmj_genai GitHub project.\n",
      "==========\n"
     ]
    }
   ],
   "source": [
    "item = example2\n",
    "\n",
    "document_title = item[\"title\"][\"rendered\"]\n",
    "document_url = item[\"link\"]\n",
    "document_html = item[\"content\"][\"rendered\"]\n",
    "document_date = item[\"date_gmt\"]\n",
    "document_date_modified = item[\"modified_gmt\"]\n",
    "\n",
    "\n",
    "chunks = chunk_request(\n",
    "    client,\n",
    "    {\n",
    "        \"strategy\": \"heading_section_sentence\",\n",
    "        \"code_behavior\": \"remove_code_sections\",\n",
    "        \"chunk_min_words\": 250,\n",
    "        \"chunk_overlap_words\": 50,\n",
    "        \"input_type\": \"html\",\n",
    "        \"input_str\": document_html,\n",
    "        \"additional_metadata\": {\n",
    "            \"document_title\": document_title,\n",
    "            \"document_url\": document_url,\n",
    "            \"document_date\": document_date,\n",
    "            \"document_date_modified\": document_date_modified,\n",
    "        },\n",
    "    },\n",
    ")\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"Chunk #{i}\")\n",
    "    # note that word_count is a list which contains word components of all text components, including ones that are\n",
    "    # only code. These get removed from the final text though\n",
    "    print(f\"Word Count: {sum(wc for wc, only_code in zip(chunk['word_count'], chunk['only_code']) if not only_code)}\")\n",
    "    print(chunk[\"text\"])\n",
    "    print(\"==========\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unstructured vs. Structured Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chunking structured text like HTML articles is different than chunking unstructured text like the transcript of a video. With structured text, we can take advantage of the implicit structure provided by the original authors. With unstructured text, we have to find other methods.\n",
    "\n",
    "For structured text, we could use a recursive strategy that looks at the largest logical chunks of the HTML articles first, breaking HTML by the largest heading sections (since heading levels suggest hierarchy). Then if necessary, break into smaller heading sections (for example h3 headings nested under h2 headings). And then progressively break into paragraphs and so on.\n",
    "\n",
    "Similarly, with unstructured text, we might assume that e.g. a presentation is comprised of topics. We could then embed small chunks, combining adjacent ones with close enough embeddings (i.e. ones likely on the same topic) into larger chunks.\n",
    "\n",
    "After this course is complete, you'll be well positioned to explore those techniques!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Another Chunking Strategy: Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we started exploring the data, we were considering potential use cases. The chunking strategy we've selected so far gives us direct access to the contents of the articles, and it's well-suited for a question-answering (QA) task where we need to extract fine-grained details from a document to answer a user's question.\n",
    "\n",
    "A good semantic search system enables an array of use cases, including a more general version of classic search: finding which articles are available on a given topic, without needing to know the relevant keywords.\n",
    "\n",
    "For this kind of asset discovery task, the system doesn't need to know all the detailed information in each article--the kind that we were preserving in chunks--so instead we could summarize each article and store that summary as a chunk.\n",
    "\n",
    "Plus, as long as we keep the original text in our database, we could simultaneously support full-article retrieval and question-answering.\n",
    "\n",
    "To accomplish this we will do the following:\n",
    "- First use the chunking service to split articles by heading and remove code sections\n",
    "- Next, concatenate the non-code sections and send to an LLM for summarization\n",
    "- Finally, concatenate all text (including the code sections) and store that as additional metadata."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split by Heading and Remove Code Sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk #0\n",
      "Word Count: 85\n",
      "Improving CUDA Initialization Times Using cgroups in Certain Scenarios\n",
      "Many CUDA applications running on multi-GPU platforms usually use a single GPU for their compute needs. In such scenarios, a performance penalty is paid by applications because CUDA has to enumerate/initialize all the GPUs on the system. If a CUDA application does not require other GPUs to be visible and accessible, you can launch such applications by isolating the unwanted GPUs from the CUDA process and eliminating unnecessary initialization steps. This post discusses the various methods to accomplish this and their performance benefits.\n",
      "==========\n",
      "Chunk #1\n",
      "Word Count: 77\n",
      "GPU isolation\n",
      "GPU isolation can be achieved on Linux systems by using Linux tools like ```cgroups```. In this section, we first discuss a lower-level approach and then a higher-level possible approach. Another method exposed by CUDA to isolate devices is the use of ```CUDA_VISIBLE_DEVICES```. Although functionally similar, this approach has limited initialization performance gains compared to the ```cgroups``` approach.\n",
      "==========\n",
      "Chunk #2\n",
      "Word Count: 313\n",
      "Isolating GPUs using cgroups V1\n",
      "Control groups provide a mechanism for aggregating or partitioning sets of tasks and all their future children into hierarchical groups with specialized behavior. You can use ```cgroups``` to control which GPUs are visible to a CUDA process. This ensures that only the GPUs that are needed by the CUDA process are made available to it. The following code provides a low-level example of how to employ ```cgroups``` and fully isolate a GPU to a single process. Be aware that you will likely have to run these commands in a root shell to work properly. We show a more convenient, higher-level utility later in this post. When you are done with the tasks, unmount the ```/cgroupV1Device``` folder with the umount command. To allow or deny the user access to any other GPUs on the system, write those GPU numbers to the appropriate file. Here’s an example of denying access to only GPU5 and GPU6 on a multi-GPU system. In the ```/gpugroup``` folder created earlier, write the PID of the shell from where the CUDA process is to be launched into the ```tasks``` file: Now add GPU5 and GPU6 to the denied list: At this point, the CUDA process can’t see or access the two GPUs. To enable only specific GPUs to a CUDA process, those GPUs should be added to the ```devices.allow``` file and the rest of the GPUs should be added to the ```devices.deny``` file. The access controls apply per process. Multiple processes can be added to the ```tasks``` file to propagate the same controls to more than one process.\n",
      "==========\n",
      "Chunk #3\n",
      "Word Count: 77\n",
      "Isolating GPUs using the bubblewrap utility\n",
      "The bubblewrap utility (bwrap) is a higher-level utility available for sandboxing and access control in Linux operating systems, which can be used to achieve the same effect as the solution presented earlier. You can use this to conveniently restrict or allow access to specific GPUs from a CUDA process: More than one GPU can be exposed to a CUDA process by extending the ```dev-bind``` option in the code example.\n",
      "==========\n",
      "Chunk #4\n",
      "Word Count: 131\n",
      "Performance benefits of GPU isolation\n",
      "In this section, we compare the performance of the CUDA driver initialization API (cuInit) with and without GPU isolation, measured over 256 iterations. The APIs are being run on an x86-based machine with four A100 class GPUs. Bar graph shows the performance of cuInit API running on an A104 system with and without GPU isolation using cgroups. The bar on the left shows the performance of cuInit when only a single GPU is exposed to the calling CUDA process via cgroups (~65 ms). The bar on the right shows the performance of cuInit when all four GPUs on the system are made available to the CUDA process (225 ms). Figure 1. CUDA initialisation performance comparison between a cgroup-constrained process and the default scenario on a four-GPU test system\n",
      "==========\n",
      "Chunk #5\n",
      "Word Count: 60\n",
      "Summary\n",
      "GPU isolation using ```cgroups``` offers you the option of improving CUDA initialization times in a limited number of use cases where all the GPUs on the system are not required to be used by a given CUDA process. For more information, see the following resources: Control Groups version 1 — The Linux Kernel documentation cuInit\n",
      "==========\n"
     ]
    }
   ],
   "source": [
    "item = example1\n",
    "\n",
    "document_title = item[\"title\"][\"rendered\"]\n",
    "document_url = item[\"link\"]\n",
    "document_html = item[\"content\"][\"rendered\"]\n",
    "document_date = item[\"date_gmt\"]\n",
    "document_date_modified = item[\"modified_gmt\"]\n",
    "\n",
    "\n",
    "chunks = chunk_request(\n",
    "    client,\n",
    "    {\n",
    "        \"strategy\": \"heading_section\",\n",
    "        \"code_behavior\": \"remove_code_sections\",\n",
    "        \"input_type\": \"html\",\n",
    "        \"input_str\": document_html,\n",
    "        \"additional_metadata\": {\n",
    "            \"document_title\": document_title,\n",
    "            \"document_url\": document_url,\n",
    "            \"document_date\": document_date,\n",
    "            \"document_date_modified\": document_date_modified,\n",
    "        },\n",
    "    },\n",
    ")\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"Chunk #{i}\")\n",
    "    # note that word_count is a list which contains word components of all text components, including ones that are\n",
    "    # only code. These get removed from the final text though\n",
    "    print(\n",
    "        f\"Word Count: {sum(wc for wc, only_code in zip(chunk['word_count'], chunk['only_code']) if not only_code)}\"\n",
    "    )\n",
    "    print(chunk[\"text\"])\n",
    "    print(\"==========\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenate Non-code Sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improving CUDA Initialization Times Using cgroups in Certain Scenarios\n",
      "Many CUDA applications running on multi-GPU platforms usually use a single GPU for their compute needs. In such scenarios, a performance penalty is paid by applications because CUDA has to enumerate/initialize all the GPUs on the system. If a CUDA application does not require other GPUs to be visible and accessible, you can launch such applications by isolating the unwanted GPUs from the CUDA process and eliminating unnecessary initialization steps. This post discusses the various methods to accomplish this and their performance benefits.\n",
      "GPU isolation\n",
      "GPU isolation can be achieved on Linux systems by using Linux tools like ```cgroups```. In this section, we first discuss a lower-level approach and then a higher-level possible approach. Another method exposed by CUDA to isolate devices is the use of ```CUDA_VISIBLE_DEVICES```. Although functionally similar, this approach has limited initialization performance gains compared to the ```cgroups``` approach.\n",
      "Isolating GPUs using cgroups V1\n",
      "Control groups provide a mechanism for aggregating or partitioning sets of tasks and all their future children into hierarchical groups with specialized behavior. You can use ```cgroups``` to control which GPUs are visible to a CUDA process. This ensures that only the GPUs that are needed by the CUDA process are made available to it. The following code provides a low-level example of how to employ ```cgroups``` and fully isolate a GPU to a single process. Be aware that you will likely have to run these commands in a root shell to work properly. We show a more convenient, higher-level utility later in this post. When you are done with the tasks, unmount the ```/cgroupV1Device``` folder with the umount command. To allow or deny the user access to any other GPUs on the system, write those GPU numbers to the appropriate file. Here’s an example of denying access to only GPU5 and GPU6 on a multi-GPU system. In the ```/gpugroup``` folder created earlier, write the PID of the shell from where the CUDA process is to be launched into the ```tasks``` file: Now add GPU5 and GPU6 to the denied list: At this point, the CUDA process can’t see or access the two GPUs. To enable only specific GPUs to a CUDA process, those GPUs should be added to the ```devices.allow``` file and the rest of the GPUs should be added to the ```devices.deny``` file. The access controls apply per process. Multiple processes can be added to the ```tasks``` file to propagate the same controls to more than one process.\n",
      "Isolating GPUs using the bubblewrap utility\n",
      "The bubblewrap utility (bwrap) is a higher-level utility available for sandboxing and access control in Linux operating systems, which can be used to achieve the same effect as the solution presented earlier. You can use this to conveniently restrict or allow access to specific GPUs from a CUDA process: More than one GPU can be exposed to a CUDA process by extending the ```dev-bind``` option in the code example.\n",
      "Performance benefits of GPU isolation\n",
      "In this section, we compare the performance of the CUDA driver initialization API (cuInit) with and without GPU isolation, measured over 256 iterations. The APIs are being run on an x86-based machine with four A100 class GPUs. Bar graph shows the performance of cuInit API running on an A104 system with and without GPU isolation using cgroups. The bar on the left shows the performance of cuInit when only a single GPU is exposed to the calling CUDA process via cgroups (~65 ms). The bar on the right shows the performance of cuInit when all four GPUs on the system are made available to the CUDA process (225 ms). Figure 1. CUDA initialisation performance comparison between a cgroup-constrained process and the default scenario on a four-GPU test system\n",
      "Summary\n",
      "GPU isolation using ```cgroups``` offers you the option of improving CUDA initialization times in a limited number of use cases where all the GPUs on the system are not required to be used by a given CUDA process. For more information, see the following resources: Control Groups version 1 — The Linux Kernel documentation cuInit\n"
     ]
    }
   ],
   "source": [
    "clean_text_no_code = \"\\n\".join([x[\"text\"] for x in chunks])\n",
    "print(clean_text_no_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenate Code and Non-code Sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improving CUDA Initialization Times Using cgroups in Certain Scenarios\n",
      "Many CUDA applications running on multi-GPU platforms usually use a single GPU for their compute needs. In such scenarios, a performance penalty is paid by applications because CUDA has to enumerate/initialize all the GPUs on the system. If a CUDA application does not require other GPUs to be visible and accessible, you can launch such applications by isolating the unwanted GPUs from the CUDA process and eliminating unnecessary initialization steps.\n",
      "This post discusses the various methods to accomplish this and their performance benefits.\n",
      "GPU isolation\n",
      "GPU isolation can be achieved on Linux systems by using Linux tools like ```cgroups```. In this section, we first discuss a lower-level approach and then a higher-level possible approach.\n",
      "Another method exposed by CUDA to isolate devices is the use of ```CUDA_VISIBLE_DEVICES```. Although functionally similar, this approach has limited initialization performance gains compared to the ```cgroups``` approach.\n",
      "Isolating GPUs using cgroups V1\n",
      "Control groups provide a mechanism for aggregating or partitioning sets of tasks and all their future children into hierarchical groups with specialized behavior. You can use ```cgroups``` to control which GPUs are visible to a CUDA process. This ensures that only the GPUs that are needed by the CUDA process are made available to it.\n",
      "The following code provides a low-level example of how to employ ```cgroups``` and fully isolate a GPU to a single process. Be aware that you will likely have to run these commands in a root shell to work properly. We show a more convenient, higher-level utility later in this post.\n",
      "```\n",
      "# Create a mountpoint for the cgroup hierarchy as root\n",
      "$> cd /mnt\n",
      "$> mkdir cgroupV1Device\n",
      "\n",
      "# Use mount command to mount the hierarchy and attach the device subsystem to it\n",
      "$> mount -t cgroup -o devices devices cgroupV1Device\n",
      "$> cd cgroupV1Device\n",
      "# Now create a gpu subgroup directory to restrict/allow GPU access\n",
      "$> mkdir gpugroup\n",
      "$> cd gpugroup\n",
      "# in the gpugroup, you will see many cgroupfs files, the ones that interest us are tasks, device.deny and device.allow\n",
      "$> ls gpugroup\n",
      "tasks      devices.deny     devices.allow\n",
      "\n",
      "# Launch a shell from where the CUDA process will be executed. Gets the shells PID\n",
      "$> echo $$\n",
      "\n",
      "# Write this PID into the tasks files in the gpugroups folder\n",
      "$> echo <PID> tasks\n",
      "\n",
      "# List the device numbers of nvidia devices with the ls command\n",
      "$> ls -l /dev/nvidia*\n",
      "crw-rw-rw- 1 root root 195,   0 Jul 11 14:28 /dev/nvidia0\n",
      "crw-rw-rw- 1 root root 195,   0 Jul 11 14:28 /dev/nvidia1\n",
      "\n",
      "# Assuming that you only want to allow the CUDA process to access GPU0, you deny the CUDA process access to GPU1 by writing the following command to devices.deny\n",
      "$> echo 'c 195:1 rmw' > devices.deny\n",
      "\n",
      "# Now GPU1 will not be visible to The CUDA process that you launch from the second shell.\n",
      "# To provide the CUDA process access to GPU1, we should write the following to devices.allow\n",
      "\n",
      "$> echo 'c 195:1 rmw' > devices.allow\n",
      "```\n",
      "When you are done with the tasks, unmount the ```/cgroupV1Device``` folder with the umount command.\n",
      "```\n",
      "umount /mnt/cgroupV1Device\n",
      "```\n",
      "To allow or deny the user access to any other GPUs on the system, write those GPU numbers to the appropriate file. Here’s an example of denying access to only GPU5 and GPU6 on a multi-GPU system.\n",
      "In the ```/gpugroup``` folder created earlier, write the PID of the shell from where the CUDA process is to be launched into the ```tasks``` file:\n",
      "```\n",
      "$> echo <PID> tasks\n",
      "```\n",
      "Now add GPU5 and GPU6 to the denied list:\n",
      "```\n",
      "$> echo 'c 195:5 rmw' > devices.deny\n",
      "$> echo 'c 195:6 rmw' > devices.deny\n",
      "```\n",
      "At this point, the CUDA process can’t see or access the two GPUs. To enable only specific GPUs to a CUDA process, those GPUs should be added to the ```devices.allow``` file and the rest of the GPUs should be added to the ```devices.deny``` file.\n",
      "The access controls apply per process. Multiple processes can be added to the ```tasks``` file to propagate the same controls to more than one process.\n",
      "Isolating GPUs using the bubblewrap utility\n",
      "The bubblewrap utility (bwrap) is a higher-level utility available for sandboxing and access control in Linux operating systems, which can be used to achieve the same effect as the solution presented earlier. You can use this to conveniently restrict or allow access to specific GPUs from a CUDA process:\n",
      "```\n",
      "# install bubblewrap utility on Debian-like systems\n",
      "$>sudo apt-get install -y bubblewrap\n",
      "\n",
      "# create a simple shell script that uses bubblewap for binding the required GPU to the launched process\n",
      "\n",
      "#!/bin/sh\n",
      "# bwrap.sh\n",
      "GPU=$1;shift   # 0, 1, 2, 3, ..\n",
      "if [ \"$GPU\" = \"\" ]; then echo \"missing arg: gpu id\"; exit 1; fi\n",
      "bwrap \\\n",
      "        --bind / / \\\n",
      "        --dev /dev --dev-bind /dev/nvidiactl /dev/nvidiactl --dev-bind /dev/nvidia-uvm /dev/nvidia-uvm  \\\n",
      "        --dev-bind /dev/nvidia$GPU /dev/nvidia$GPU \\\n",
      "        \"$@\"\n",
      "\n",
      "\n",
      "# Launch the CUDA process with the bubblewrap utility to only allow access to a specific GPU while running\n",
      "$> ./bwrap.sh 0 ./test_cuda_app <args>\n",
      "```\n",
      "More than one GPU can be exposed to a CUDA process by extending the ```dev-bind``` option in the code example.\n",
      "Performance benefits of GPU isolation\n",
      "In this section, we compare the performance of the CUDA driver initialization API (cuInit) with and without GPU isolation, measured over 256 iterations. The APIs are being run on an x86-based machine with four A100 class GPUs.\n",
      "Bar graph shows the performance of cuInit API running on an A104 system with and without GPU isolation using cgroups. The bar on the left shows the performance of cuInit when only a single GPU is exposed to the calling CUDA process via cgroups (~65 ms). The bar on the right shows the performance of cuInit when all four GPUs on the system are made available to the CUDA process (225 ms).\n",
      "Figure 1. CUDA initialisation performance comparison between a cgroup-constrained process and the default scenario on a four-GPU test system\n",
      "Summary\n",
      "GPU isolation using ```cgroups``` offers you the option of improving CUDA initialization times in a limited number of use cases where all the GPUs on the system are not required to be used by a given CUDA process.\n",
      "For more information, see the following resources:\n",
      "Control Groups version 1 — The Linux Kernel documentation\n",
      "cuInit\n"
     ]
    }
   ],
   "source": [
    "clean_text_with_code = \"\\n\".join([ x[\"heading_section_title\"][0] + \"\\n\" + \"\\n\".join(x[\"text_components\"]) for x in chunks])\n",
    "print(clean_text_with_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use LLM to Summarize Blog Posts (Non-code Sections)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to summarize, we'll rely on making calls to an LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NeMo Inference Microservice Mixtral 8x7B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " As a default for our LLM we will use a local instance of Mistral's Mixtral 8x7B instruct model served via NIM. NVIDIA NIM, part of NVIDIA AI Enterprise, is a set of easy-to-use microservices designed to accelerate deployment of generative AI across your enterprise. This versatile runtime supports a broad spectrum of AI models—from open-source community models to NVIDIA AI Foundation models, as well as custom AI models. Leveraging industry standard APIs, developers can quickly build enterprise-grade AI applications with just a few lines of code. Built on the robust foundations including inference engines like Triton Inference Server, TensorRT, TensorRT-LLM, and PyTorch, NIM is engineered to facilitate seamless AI inferencing at scale, ensuring that you can deploy AI applications anywhere with confidence. Whether on-premises or in the cloud, NIM is the fastest way to achieve accelerated generative AI inference at scale. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get started, users can experience the accelerated generative AI models on the API catalog. When ready to deploy, enterprises can export models with NVIDIA NIM which is included with the NVIDIA AI Enterprise license, and run anywhere, giving them ownership to their customizations and full control of their IP and AI application.     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we import a `ChatOpenAI` instance of our local NIM Mixtral 8x7B model configured and ready for use with LangChain from an [`llms` helper file](llms.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llms import llms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = llms.nim_mixtral_llm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional Remote LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optionally, instead of using our local model, you can also use either NVIDIA AI Foundation's Mixtral 8x7B model or OpenAI's gpt-3.5-turbo.\n",
    "\n",
    "For either of these 2 options you'll need an API key. For more details about NVIDIA AI Foundation and obtaining a free API key, see [the notebook *NVIDIA AI Foundation.ipynb*](./NVIDIA%20AI%20Foundation.ipynb).\n",
    "\n",
    "After obtaining an appropriate API key, uncomment the appropriate cell below, add your API key, and run the cell to set `llm` to the remote LLM you chose to work with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NVIDIA AI Foundation Mixtral 8x7B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from llms import set_api_key\n",
    "# set_api_key('NVIDIA_API_KEY', '<your_nvidia_api_key>')\n",
    "# llm = llms.nvai_mixtral_llm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OpenAI GPT-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from llms import set_api_key\n",
    "# set_api_key('OPENAI_API_KEY', '<your_openai_api_key>')\n",
    "# llm = llms.openai_gpt3_llm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we try out whichever model you've chosen to work with with a simple prompt, using LangChain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.messages import (\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "template = ChatPromptTemplate.from_messages(\n",
    "    [(\"user\", \"{user_input}\")]\n",
    ")\n",
    "\n",
    "messages = template.format_messages(\n",
    "    user_input=\"Tell me a story about fishing.\"\n",
    ")\n",
    "\n",
    "generation: AIMessage = llm.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Once upon a time, in a small coastal village, there lived a young boy named Sam. Sam's family had been fishermen for generations, and he was expected to follow in their footsteps. But Sam had always dreamed of something more. He wanted to be a great artist, and spent all his free time sketching and painting the beautiful landscapes and seascapes around him.\n",
      "\n",
      "One day, Sam's father took him out on the boat for a day of fishing. The sun was shining, the sea was calm, and Sam couldn't help but bring his sketchbook along. As they sailed out to sea, Sam's father noticed his son's lack of enthusiasm for the task at hand.\n",
      "\n",
      "\"Sam, my boy,\" he said, \"you don't seem to have your heart in this today. What's on your mind?\"\n",
      "\n",
      "Sam hesitated for a moment, then decided to be honest with his father. \"I just don't think fishing is for me, dad. I want to be an artist.\"\n",
      "\n",
      "His father was quiet for a moment, then he smiled. \"Well, Sam, I had a feeling you might say that. But I also know that fishing is a part of who we are, and it's important to honor our family's legacy. How about this: why don't you try painting a picture of the sea while we fish? Maybe that will help you find a way to combine your two passions.\"\n",
      "\n",
      "Sam was thrilled at the idea, and set to work with his sketchbook and paints. As he looked out at the sparkling water, he began to see it in a new way. He noticed the way the light danced on the waves, the way the seaweed swayed in the current, and the way the seagulls dove for fish. He realized that there was beauty all around him, and that he could capture it in his art.\n",
      "\n",
      "As the day went on, Sam's father noticed a change in his son. He was no longer distracted and disinterested; he was fully present, engaged in the world around him. And his art was stunning.\n",
      "\n",
      "From that day on, Sam continued to fish with his father, but he also made time for his art. He began to sell his paintings in the village, and soon his work was in demand all over the region. People loved the way he captured the beauty of the sea, and they were willing to pay good money for his creations.\n",
      "\n",
      "Sam's father was proud of his son, and he knew that he had made the right decision. By encouraging Sam to pursue his passion, he had helped him find a way to honor their family's legacy and create something truly beautiful.\n",
      "\n",
      "And so, Sam became not just a fisherman, but also a celebrated artist. He proved that it was possible to follow your dreams and still respect your roots, and he inspired others to do the same. And every time he looked out at the sea, he was reminded of the beauty that could be found in the most unexpected places.\n"
     ]
    }
   ],
   "source": [
    "print(generation.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "504"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rough word count\n",
    "len(generation.content.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use LLM to Summarize Blog Post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "GPU isolation is a technique to improve CUDA initialization times in certain scenarios where not all GPUs on a system are required by a CUDA process. Linux tools like cgroups can be used to isolate GPUs and limit which ones are visible to a CUDA process. This can be done at a lower level using cgroups V1 or at a higher level using the bubblewrap utility. The cgroups V1 approach involves creating a group, writing the PID of the shell from where the CUDA process is to be launched into the tasks file, and then allowing or denying access to specific GPUs. The bubblewrap utility is a higher-level option that can also be used to restrict or allow access to specific GPUs. GPU isolation can improve CUDA initialization times by reducing the number of GPUs that need to be initialized. In a test using an x86-based machine with four A100 class GPUs, the performance of the cuInit API was improved from 225 ms to 65 ms when only one GPU was exposed to the CUDA process.\n"
     ]
    }
   ],
   "source": [
    "template = ChatPromptTemplate.from_messages(\n",
    "    [(\"user\", \"Summarize the following article in 200 words or less:\\n{user_input}\")]\n",
    ")\n",
    "\n",
    "messages = template.format_messages(\n",
    "    user_input=clean_text_no_code\n",
    ")\n",
    "\n",
    "generation: AIMessage = llm.invoke(messages)\n",
    "\n",
    "print(generation.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "176"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rough word count\n",
    "len(generation.content.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have made it through Lesson 01!\n",
    "\n",
    "To recap, we've got two different chunking strategies to use for preparing data for our search system. One is chunking the actual text of the articles using a running word count, while the other is summarizing the article and using that summary as the chunk. Each of these strategies is suited to its own task: raw text chunking for question-answering, and full-article summarization for asset discovery. \n",
    "\n",
    "In Lesson 02, we're going to launch our database container and start importing and searching through this data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Lesson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Move to the next lesson by double-clicking *Lesson 02.ipynb* on the file-viewer on the left-hand side of your Jupyter Lab environment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
